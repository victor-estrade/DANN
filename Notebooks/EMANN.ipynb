{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import sys\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets\n",
    "\n",
    "- the datasets are loaded/built.\n",
    "- The batchsize is defined\n",
    "- half of the data name (the source part) is defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Loading of datasets](#Load-datasets)\n",
    "- [2. Transformation of datasets](#Transform-datasets)\n",
    "- [3. Clusters init](#Clusters-Init)\n",
    "- [4. Optimal transport init](#Optimal-Transport-Init)\n",
    "- [5. Align](#Align)\n",
    "- [6. Init of the NN](#Build-the-Neural-Network)\n",
    "- [7. Architecture](#Architecture)\n",
    "- [8. Compiling](#Compiling)\n",
    "- [9. Preprocessing for alignment](#Add-preprocessing-for-alignment)\n",
    "- [10. Playground](#Play-!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.toys import make_clouds, make_circles, make_X, make_moons\n",
    "from datasets.utils import make_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples = 200  # Number of sample per class\n",
    "n_classes = 3\n",
    "batchsize = 80\n",
    "_data_name = 'Clouds'\n",
    "X, y = make_clouds(n_samples=n_samples, n_classes=n_classes)\n",
    "source_data = make_dataset(X, y, batchsize=batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples = 500  # Number of sample per class\n",
    "n_classes = 5\n",
    "n_dim = 2\n",
    "batchsize = 60\n",
    "_data_name = 'Circles'\n",
    "X, y = make_circles(n_samples=n_samples, n_classes=n_classes, n_dim=n_dim)\n",
    "source_data = make_dataset(X, y, batchsize=batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples = 500  # Number of sample per class\n",
    "n_classes = 5\n",
    "batchsize = 60\n",
    "_data_name = 'X'\n",
    "X, y = make_X(n_samples=n_samples, n_classes=n_classes)\n",
    "source_data = make_dataset(X, y, batchsize=batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples = 500\n",
    "batchsize = 60\n",
    "_data_name = 'Moons'\n",
    "X, y = make_moons(n_samples=n_samples)\n",
    "source_data = make_dataset(X, y, batchsize=batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from datasets.mnist import load_mnist\n",
    "batchsize = 500\n",
    "_data_name = 'MNIST'\n",
    "X, y = load_mnist()\n",
    "# X = X[:, 14:21, 14:21]\n",
    "source_data = make_dataset(X, y, batchsize=batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform datasets\n",
    "\n",
    "- the transformed datasets are built.\n",
    "- last part of the data name (the target part) is defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Loading of datasets](#Load-datasets)\n",
    "- [2. Transformation of datasets](#Transform-datasets)\n",
    "- [3. Clusters init](#Clusters-Init)\n",
    "- [4. Optimal transport init](#Optimal-Transport-Init)\n",
    "- [5. Align](#Align)\n",
    "- [6. Init of the NN](#Build-the-Neural-Network)\n",
    "- [7. Architecture](#Architecture)\n",
    "- [8. Compiling](#Compiling)\n",
    "- [9. Preprocessing for alignment](#Add-preprocessing-for-alignment)\n",
    "- [10. Playground](#Play-!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.utils import make_domain_dataset, make_corrector_dataset\n",
    "import datasets.transform as transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Data rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_name = _data_name+'_Rotated'\n",
    "angle = 35\n",
    "\n",
    "X_t, y_t = transform.rotate(X, y, angle=angle)\n",
    "target_data = make_dataset(X_t, y_t, batchsize=batchsize)\n",
    "domain_data = make_domain_dataset([source_data, target_data])\n",
    "corrector_data = make_corrector_dataset(source_data, target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data . Random Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_name = _data_name+'_RMat'\n",
    "\n",
    "X_t, y_t = transform.random_mat(X, y, normalize=False, random_state=42)\n",
    "target_data = make_dataset(X_t, y_t, batchsize=batchsize)\n",
    "domain_data = make_domain_dataset([source_data, target_data])\n",
    "corrector_data = make_corrector_dataset(source_data, target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data . Diag Dominant matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_name = _data_name+'_Diag'\n",
    "\n",
    "X_t, y_t = transform.diag_dominant(X, y, normalize=True)\n",
    "target_data = make_dataset(X_t, y_t, batchsize=batchsize)\n",
    "domain_data = make_domain_dataset([source_data, target_data])\n",
    "corrector_data = make_corrector_dataset(source_data, target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Mirror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_name = _data_name+'_Mirror'\n",
    "\n",
    "X_t, y_t = transform.mirror(X, y)\n",
    "target_data = make_dataset(X_t, y_t, batchsize=batchsize)\n",
    "domain_data = make_domain_dataset([source_data, target_data])\n",
    "corrector_data = make_corrector_dataset(source_data, target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Random Permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_name = _data_name+'_Rperm'\n",
    "\n",
    "X_t, y_t = transform.random_permut(X, y)\n",
    "target_data = make_dataset(X_t, y_t, batchsize=batchsize)\n",
    "domain_data = make_domain_dataset([source_data, target_data])\n",
    "corrector_data = make_corrector_dataset(source_data, target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Bend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_name = _data_name+'_GridBend'\n",
    "nx = 4\n",
    "ny = 4\n",
    "grid_noise = 0.5\n",
    "\n",
    "X_t, y_t, grid = transform.grid_bend(X, y, nx=nx, ny=ny, noise=grid_noise)\n",
    "target_data = make_dataset(X_t, y_t, batchsize=batchsize)\n",
    "domain_data = make_domain_dataset([source_data, target_data])\n",
    "corrector_data = make_corrector_dataset(source_data, target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_src, y_src = make_clouds(n_samples=50, n_classes=6)\n",
    "X_tgt, y_tgt = make_moons(n_samples=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusters Init\n",
    "Here we initialize the clusters on the *source* and on the *target* domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Loading of datasets](#Load-datasets)\n",
    "- [2. Transformation of datasets](#Transform-datasets)\n",
    "- [3. Clusters init](#Clusters-Init)\n",
    "- [4. Optimal transport init](#Optimal-Transport-Init)\n",
    "- [5. Align](#Align)\n",
    "- [6. Init of the NN](#Build-the-Neural-Network)\n",
    "- [7. Architecture](#Architecture)\n",
    "- [8. Compiling](#Compiling)\n",
    "- [9. Preprocessing for alignment](#Add-preprocessing-for-alignment)\n",
    "- [10. Playground](#Play-!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k_src = 10\n",
    "k_tgt = 11\n",
    "k_means_src = KMeans(n_clusters=k_src).fit(X_src)\n",
    "k_means_tgt = KMeans(n_clusters=k_tgt).fit(X_tgt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mass(k_means):\n",
    "    \"\"\"\n",
    "    Params\n",
    "    ------\n",
    "        k_means: (sklearn.cluster.KMeans instance)\n",
    "    Return\n",
    "    ------\n",
    "        w: (numpy.array [n_clusters]) the mass of each clusters \n",
    "    \"\"\"\n",
    "    w = np.unique(k_means.labels_, return_counts=True)[1]\n",
    "    w /= np.sum(w)\n",
    "    return w\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_src = mass(k_means_src)\n",
    "# w_src = np.abs(np.sin(-w_src-np.arange(w_src.shape[0])))\n",
    "# w_src /= np.sum(w_src)\n",
    "\n",
    "w_tgt = mass(k_means_tgt)\n",
    "# w_tgt = np.abs(np.sin(-w_tgt-np.arange(w_tgt.shape[0])))\n",
    "# w_tgt = np.exp(-w_tgt-np.arange(w_tgt.shape[0]))\n",
    "# w_tgt /= np.sum(w_tgt)\n",
    "cost_mat = np.random.uniform(0,1, size=(w_src.shape[0], w_tgt.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import visual\n",
    "visual.mat(cost_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Transport Init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Loading of datasets](#Load-datasets)\n",
    "- [2. Transformation of datasets](#Transform-datasets)\n",
    "- [3. Clusters init](#Clusters-Init)\n",
    "- [4. Optimal transport init](#Optimal-Transport-Init)\n",
    "- [5. Align](#Align)\n",
    "- [6. Init of the NN](#Build-the-Neural-Network)\n",
    "- [7. Architecture](#Architecture)\n",
    "- [8. Compiling](#Compiling)\n",
    "- [9. Preprocessing for alignment](#Add-preprocessing-for-alignment)\n",
    "- [10. Playground](#Play-!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from opt_transport import opt_transp_sup, computeTransportSinkhorn, computeTransportSinkhornLabelsLpL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transp = opt_transp_sup(k_means_src.cluster_centers_, k_means_tgt.cluster_centers_)\n",
    "transp = computeTransportSinkhorn(w_src, w_tgt, cost_mat, reg=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visual.mat(transp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Align"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Loading of datasets](#Load-datasets)\n",
    "- [2. Transformation of datasets](#Transform-datasets)\n",
    "- [3. Clusters init](#Clusters-Init)\n",
    "- [4. Optimal transport init](#Optimal-Transport-Init)\n",
    "- [5. Align](#Align)\n",
    "- [6. Init of the NN](#Build-the-Neural-Network)\n",
    "- [7. Architecture](#Architecture)\n",
    "- [8. Compiling](#Compiling)\n",
    "- [9. Preprocessing for alignment](#Add-preprocessing-for-alignment)\n",
    "- [10. Playground](#Play-!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transp.shape, X_src.shape, k_means_src.labels_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from align_learn.preprocess import align\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check. The cluster distribution of the aligned data should be the same as the target distrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "align_idx, cluster_T = align(transp, k_means_src.labels_, k_means_tgt.labels_)\n",
    "# print(np.unique(res).shape, X_src.shape, X_tgt.shape)\n",
    "uniq, count = np.unique(cluster_T, return_counts=True)\n",
    "# plt.plot(np.sum(transp,0), label='transp.sum(0)')\n",
    "plt.plot(w_tgt, label='w_tgt')\n",
    "# plt.plot(np.sum(transp,1), label='transp.sum(1)')\n",
    "plt.plot(w_src, label='w_src')\n",
    "plt.plot(count/cluster_T.shape[0], label='mapping')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_S, y_S = X_src, y_src\n",
    "X_T, y_T = X_tgt[align_idx], y_tgt[align_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visual.target_2D(X_T, y_T)\n",
    "visual.target_2D(X_tgt, y_tgt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_T.shape, X_S.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the probabilities to be predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_dataset(X_S, X_T, k_means_src, k_means_tgt, transp, align_idx, cluster_T):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Build the probabilities to be predict\n",
    "    # For the source data\n",
    "    proba_src = np.zeros((X_S.shape[0], k_means_src.n_clusters))\n",
    "    proba_src[np.arange(X_S.shape[0]), k_means_src.labels_] = 1.\n",
    "    proba_tgt = transp[k_means_src.labels_]\n",
    "    Y_S = np.hstack([proba_src, proba_tgt])\n",
    "    \n",
    "    # Build the probabilities to be predict\n",
    "    # For the aligned target data\n",
    "    proba_tgt = np.zeros((X_T.shape[0], k_means_tgt.n_clusters))\n",
    "    proba_tgt[np.arange(X_T.shape[0]), cluster_T] = 1.\n",
    "    proba_src = transp[:, cluster_T].T\n",
    "    Y_T = np.hstack([proba_src, proba_tgt])\n",
    "    \n",
    "    Y = np.vstack([Y_S, Y_T])\n",
    "    X = np.vstack([X_S, X_T])\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, Y = train_dataset(X_S, X_T, k_means_src, k_means_tgt, transp, align_idx, cluster_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X1, Y1 = train_dataset(X_S, X_T, k_means_src, k_means_tgt, transp, align_idx, cluster_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.all(X1==X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y[0], Y[350]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Loading of datasets](#Load-datasets)\n",
    "- [2. Transformation of datasets](#Transform-datasets)\n",
    "- [3. Clusters init](#Clusters-Init)\n",
    "- [4. Optimal transport init](#Optimal-Transport-Init)\n",
    "- [5. Align](#Align)\n",
    "- [6. Init of the NN](#Build-the-Neural-Network)\n",
    "- [7. Architecture](#Architecture)\n",
    "- [8. Compiling](#Compiling)\n",
    "- [9. Preprocessing for alignment](#Add-preprocessing-for-alignment)\n",
    "- [10. Playground](#Play-!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network building\n",
    "Start with the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nn.rgl import ReverseGradientLayer\n",
    "from nn.compilers import crossentropy_sgd_mom, squared_error_sgd_mom, adversarial\n",
    "from nn.training import Trainner, training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "- Learning rates\n",
    "- Hyper params\n",
    "- Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from logs import log_fname, new_logger, empty_logger\n",
    "\n",
    "hp_lambda = 0.\n",
    "\n",
    "# Learning rates and momentums\n",
    "label_rate = 0.1\n",
    "label_mom = 0.9\n",
    "\n",
    "domain_rate = 0.1\n",
    "domain_mom = 0.9\n",
    "\n",
    "# Get a logger\n",
    "logger = new_logger()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Theano variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_shape = np.shape(source_data['X_train'])\n",
    "n_dim = len(_shape)\n",
    "n_features = np.prod(_shape[1:])\n",
    "\n",
    "# Prepare Theano variables for inputs and targets\n",
    "if n_dim == 2:\n",
    "    input_var = T.matrix('inputs')\n",
    "    src_var = T.matrix('src')\n",
    "    target_var = T.matrix('targets')\n",
    "    shape = (batchsize,) + _shape[1:]\n",
    "elif n_dim == 3:\n",
    "    input_var = T.tensor3('inputs')\n",
    "    src_var = T.tensor3('src')\n",
    "    target_var = T.tensor3('targets')\n",
    "    shape = (batchsize,) + _shape[1:]\n",
    "elif n_dim == 4:\n",
    "    input_var = T.tensor4('inputs')\n",
    "    src_var = T.tensor4('src')\n",
    "    target_var = T.tensor4('targets')\n",
    "    shape = (batchsize,) + _shape[1:]\n",
    "\n",
    "# Logs\n",
    "logger.info('Building the input and output variables for : {}'.format(data_name))\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Loading of datasets](#Load-datasets)\n",
    "- [2. Transformation of datasets](#Transform-datasets)\n",
    "- [3. Clusters init](#Clusters-Init)\n",
    "- [4. Optimal transport init](#Optimal-Transport-Init)\n",
    "- [5. Align](#Align)\n",
    "- [6. Init of the NN](#Build-the-Neural-Network)\n",
    "- [7. Architecture](#Architecture)\n",
    "- [8. Compiling](#Compiling)\n",
    "- [9. Preprocessing for alignment](#Add-preprocessing-for-alignment)\n",
    "- [10. Playground](#Play-!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the layers\n",
    "input_layer = lasagne.layers.InputLayer(shape=shape, input_var=input_var)\n",
    "src_layer = lasagne.layers.InputLayer(shape=shape, input_var=src_var)\n",
    "dense_1 = lasagne.layers.DenseLayer(\n",
    "                input_layer,\n",
    "                num_units=25,\n",
    "                nonlinearity=lasagne.nonlinearities.rectify,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n",
    "# dropout_1 = lasagne.layers.DropoutLayer(dense_1, p=0.5)\n",
    "dense_2 = lasagne.layers.DenseLayer(\n",
    "                dense_1,\n",
    "                num_units=25,\n",
    "                nonlinearity=lasagne.nonlinearities.rectify,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n",
    "feature = lasagne.layers.DenseLayer(\n",
    "                input_layer,\n",
    "                num_units=np.prod(shape[1:]),  # Should have same number as the input dimension\n",
    "                nonlinearity=None,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n",
    "reshaper = lasagne.layers.ReshapeLayer(feature, (-1,) + shape[1:])\n",
    "output_layer = reshaper\n",
    "\n",
    "# Logs\n",
    "logger.info('Building the neural network architecture for : {}'.format(data_name))\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Compiling \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Loading of datasets](#Load-datasets)\n",
    "- [2. Transformation of datasets](#Transform-datasets)\n",
    "- [3. Clusters init](#Clusters-Init)\n",
    "- [4. Optimal transport init](#Optimal-Transport-Init)\n",
    "- [5. Align](#Align)\n",
    "- [6. Init of the NN](#Build-the-Neural-Network)\n",
    "- [7. Architecture](#Architecture)\n",
    "- [8. Compiling](#Compiling)\n",
    "- [9. Preprocessing for alignment](#Add-preprocessing-for-alignment)\n",
    "- [10. Playground](#Play-!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Logs\n",
    "logger.info('Compiling the neural network for : {}'.format(data_name))\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n",
    "\n",
    "# Compilation\n",
    "corrector_trainner = Trainner(squared_error_sgd_mom(output_layer, lr=label_rate, mom=0, \n",
    "                                                    target_var=target_var,\n",
    "                                                   regularization=None, reg_param=0.1), \n",
    "                             'corrector',)\n",
    "\n",
    "domain_trainner = Trainner(adversarial([src_layer, output_layer], hp_lambda=hp_lambda,\n",
    "                                      lr=domain_rate, mom=domain_mom),\n",
    "                           'domain')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add preprocessing for alignment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Loading of datasets](#Load-datasets)\n",
    "- [2. Transformation of datasets](#Transform-datasets)\n",
    "- [3. Clusters init](#Clusters-Init)\n",
    "- [4. Optimal transport init](#Optimal-Transport-Init)\n",
    "- [5. Align](#Align)\n",
    "- [6. Init of the NN](#Build-the-Neural-Network)\n",
    "- [7. Architecture](#Architecture)\n",
    "- [8. Compiling](#Compiling)\n",
    "- [9. Preprocessing for alignment](#Add-preprocessing-for-alignment)\n",
    "- [10. Playground](#Play-!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from align_learn.preprocess import classwise_shuffle, exhaustive_clostest, cluster_preprocess, build_clusters\n",
    "\n",
    "# Choose preprocessing :\n",
    "# corrector_trainner.preprocess = classwise_shuffle\n",
    "# corrector_trainner.preprocess = exhaustive_clostest\n",
    "corrector_trainner.preprocess = cluster_preprocess\n",
    "\n",
    "model_name = ''\n",
    "if corrector_trainner.preprocess is classwise_shuffle:\n",
    "    model_name = 'Classwise_Corrector'\n",
    "    corrector_data['labels'] = source_data['y_train']\n",
    "elif corrector_trainner.preprocess is exhaustive_clostest:\n",
    "    model_name = 'K-closest_Corrector'\n",
    "    corrector_data['labels'] = source_data['y_train']\n",
    "elif corrector_trainner.preprocess is cluster_preprocess:\n",
    "    model_name = 'Cluster_Corrector'\n",
    "    n_clusters = 6\n",
    "    corrector_data['k'] = -1\n",
    "    _y = source_data['y_train']\n",
    "    classes = np.unique(_y)\n",
    "\n",
    "    # Build the clusters for target data\n",
    "    centers_array, clusters_label, centers_labels = build_clusters(corrector_data['X_train'],\n",
    "                                                                   _y, n_clusters=n_clusters)\n",
    "    corrector_data['X_train_centers'] = centers_array\n",
    "    corrector_data['X_train_clusters'] = clusters_label\n",
    "    corrector_data['centers_labels'] = centers_labels\n",
    "    \n",
    "    # Build the clusters for source data\n",
    "    centers_array, clusters_label, centers_labels = build_clusters(corrector_data['y_train'],\n",
    "                                                                   _y, n_clusters=n_clusters)\n",
    "    corrector_data['y_train_centers'] = centers_array\n",
    "    corrector_data['y_train_clusters'] = clusters_label\n",
    "\n",
    "else:\n",
    "    model_name = 'Pairwise_Corrector'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train the neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Loading of datasets](#Load-datasets)\n",
    "- [2. Transformation of datasets](#Transform-datasets)\n",
    "- [3. Init of the NN](#Build-the-Neural-Network)\n",
    "- [4. Architecture](#Architecture)\n",
    "- [5. Compiling](#Compiling)\n",
    "- [6. Preprocessing for alignment](#Add-preprocessing-for-alignment)\n",
    "- [7. Playground](#Play-!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the counter and the stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logger.warn('Reset the epoch counter and saved statistics')\n",
    "epoch_counter = 0\n",
    "final_stats = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_n_epoch(n_epoch):\n",
    "    global epoch_counter, logger, final_stats, hp_lambda\n",
    "    global corrector_data, domain_data, corrector_trainner, domain_trainner\n",
    "    epoch_counter += n_epoch\n",
    "    trainers = [corrector_trainner,]\n",
    "    datas = [corrector_data,]\n",
    "    #  If hp_lambda == 0 no need to train adversarial (faster computation)\n",
    "    if hp_lambda != 0.0:\n",
    "        trainers.append(domain_trainner)\n",
    "        datas.append(domain_data)\n",
    "    # Now do the trainning part !\n",
    "    logger.info('Trainning the neural network for {} additional epochs ({} total)'.format(n_epoch, epoch_counter))\n",
    "    stats = training(trainers, datas, num_epochs=n_epoch, logger=None)\n",
    "    final_stats = {k: (final_stats[k]+v if k in final_stats else v) for k, v in stats.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Plot results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Loading of datasets](#Load-datasets)\n",
    "- [2. Transformation of datasets](#Transform-datasets)\n",
    "- [3. Init of the NN](#Build-the-Neural-Network)\n",
    "- [4. Architecture](#Architecture)\n",
    "- [5. Compiling](#Compiling)\n",
    "- [6. Preprocessing for alignment](#Add-preprocessing-for-alignment)\n",
    "- [7. Playground](#Play-!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Data plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import visual\n",
    "def grid_view(X, y, trainer):\n",
    "    \"\"\"\n",
    "    Plot the corrected grid\n",
    "    \"\"\"\n",
    "    nx = ny = 20\n",
    "    x_min, x_max = np.min(X[:, 0])*2, np.max(X[:, 0])*2\n",
    "    y_min, y_max = np.min(X[:, 1])*2, np.max(X[:, 1])*2\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, num=nx),\n",
    "                         np.linspace(y_min, y_max, num=ny))\n",
    "    X_grid = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "    y_grid = np.hstack([np.ones(nx)*i for i in range(ny)])\n",
    "\n",
    "#    Plot original grid\n",
    "    ax1.plot(xx, yy, '--')\n",
    "    ax1.plot(xx.T, yy.T, '--')\n",
    "    ax2.plot(xx, yy, '--')\n",
    "    ax2.plot(xx.T, yy.T, '--')\n",
    "    \n",
    "#     Transform:\n",
    "#     X_tgt, y_tgt = transform.rotate(X_grid, y_grid, angle=angle)\n",
    "    X_tgt, y_tgt = transform.random_mat(X_grid, y_grid, random_state=42)\n",
    "#     X_tgt, y_tgt, _ = transform.grid_bend(X_grid, y_grid, nx=nx, ny=ny, grid=grid)\n",
    "#     X_tgt, y_tgt = TT(X_grid, y_grid)\n",
    "    \n",
    "    X_corr = np.array(trainer.output(X_tgt)).reshape((-1, 2))\n",
    "    xx = X_corr[:, 0].reshape(ny, nx)\n",
    "    yy = X_corr[:, 1].reshape(ny, nx)\n",
    "    \n",
    "\n",
    "    # Plot data test points (source + corrected) on left fig\n",
    "    visual.corrected_2D(X, y, ax=ax1)\n",
    "    ax1.plot(xx, yy)\n",
    "    ax1.plot(xx.T, yy.T)\n",
    "\n",
    "    xx = X_tgt[:, 0].reshape(ny, nx)\n",
    "    yy = X_tgt[:, 1].reshape(ny, nx)\n",
    "    visual.corrected_2D(X, y, ax=ax2)\n",
    "    ax2.plot(xx, yy)\n",
    "    ax2.plot(xx.T, yy.T)\n",
    "    return fig, (ax1, ax2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig_dir = '/home/victor/Workspace/Stage/DANN/fig/'\n",
    "fig_title = fig_dir+data_name+model_name\n",
    "from datasets.utils import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Play !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Loading of datasets](#Load-datasets)\n",
    "- [2. Transformation of datasets](#Transform-datasets)\n",
    "- [3. Clusters init](#Clusters-Init)\n",
    "- [4. Optimal transport init](#Optimal-Transport-Init)\n",
    "- [5. Align](#Align)\n",
    "- [6. Init of the NN](#Build-the-Neural-Network)\n",
    "- [7. Architecture](#Architecture)\n",
    "- [8. Compiling](#Compiling)\n",
    "- [9. Preprocessing for alignment](#Add-preprocessing-for-alignment)\n",
    "- [10. Playground](#Play-!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(model_name, data_name)\n",
    "do_n_epoch(50)\n",
    "\n",
    "# ================\n",
    "# Compute the correction on test data\n",
    "# ================\n",
    "corrected_data = Dataset(\n",
    "    X_test=np.array(corrector_trainner.output(corrector_data['X_test'])).reshape((-1, 2)),\n",
    "    y_test=source_data.y_test\n",
    ")\n",
    "\n",
    "\n",
    "# ================\n",
    "# Data visualisation\n",
    "# ================\n",
    "if not n_dim > 2:\n",
    "    # Init figure and axes\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "    # Plot data test points (source + target) on right fig\n",
    "    visual.target_2D(source_data.X_test, source_data.y_test, ax=ax1);\n",
    "    visual.source_2D(target_data.X_test, target_data.y_test, ax=ax1);\n",
    "    visual.add_legend(ax1, title=data_name+model_name)\n",
    "    \n",
    "    # Plot data test points (source + corrected) on left fig\n",
    "    visual.target_2D(source_data.X_test, source_data.y_test, ax=ax2);\n",
    "    visual.corrected_2D(corrected_data.X_test, source_data.y_test, ax=ax2);\n",
    "    visual.add_legend(ax2, title=data_name+model_name)\n",
    "    \n",
    "    # Plot cluster centers and cluster mapping \n",
    "#     if 'preprocess' in corrector_data and 'X_train_centers' in corrector_data:\n",
    "#         corrected_data['X_train_centers'] = np.array(\n",
    "#             corrector_trainner.output(corrector_data['X_train_centers'])).reshape((-1, 2))\n",
    "#         idx = corrector_data['preprocess']\n",
    "#         X = np.array(corrector_trainner.output(corrector_data['X_train_centers'])).reshape((-1, 2))\n",
    "#         Y = corrector_data['y_train_centers']\n",
    "#         Y = Y[idx]\n",
    "#         visual.centers_source(Y, ax=ax2)\n",
    "#         visual.centers_corrected(X, ax=ax2)\n",
    "#         visual.mapping(X, Y, ax=ax2)\n",
    "#         visual.centers_source(Y, ax=ax1)\n",
    "#         visual.centers_target(corrector_data['X_train_centers'], ax=ax1)\n",
    "#     SAVE\n",
    "    fig.savefig(fig_title+'-DATA.png')\n",
    "    fig.show()\n",
    "\n",
    "# ================\n",
    "# Learning curve\n",
    "# ================\n",
    "fig, ax = visual.learning_curve(final_stats, regex='corrector .* loss')\n",
    "#     SAVE\n",
    "fig.tight_layout()\n",
    "fig.savefig(fig_title+'-Learning_curve.png',bbox_inches='tight')\n",
    "fig.show()\n",
    "visual.learning_curve(final_stats, regex='domain.* acc');\n",
    "\n",
    "# ================\n",
    "# Grid check\n",
    "# ================\n",
    "# fig, (ax1, ax2) = grid_view(corrected_data.X_test, corrected_data.y_test, corrector_trainner)\n",
    "# fig.savefig(fig_title+'-GridCheck.png')\n",
    "# fig.show()\n",
    "\n",
    "# ================\n",
    "# Image samples\n",
    "# ================\n",
    "if data_name.startswith('MNIST'):\n",
    "    visual.img_samples([source_data, target_data, \n",
    "                     {'X_test': np.array(corrector_trainner.output(\n",
    "                    corrector_data['X_test'])).reshape((-1,)+corrector_data['X_test'].shape[1:])}])\n",
    "\n",
    "# ================\n",
    "# Weights visualisation\n",
    "# ================\n",
    "layers = lasagne.layers.get_all_layers(feature)\n",
    "fig, axes = plt.subplots(len(layers)//2, 2, figsize=(20, 6))\n",
    "axes = axes.ravel()\n",
    "for i, l in enumerate(layers[1:]):\n",
    "    if hasattr(l, 'W'):\n",
    "        visual.mat(l.W.get_value(), ax = axes[i])\n",
    "        visual.add_legend(axes[i], title='Layer {} Weights'.format(i))\n",
    "#     SAVE\n",
    "fig.savefig(fig_title+'-W.png')\n",
    "fig.show()\n",
    "\n",
    "# ================\n",
    "# Sanity check\n",
    "# ================\n",
    "print('Loss : IdentitÃ©e / Correction Finale')\n",
    "print(np.mean((source_data['X_test']-target_data['X_test'])**2))\n",
    "print(np.mean((source_data['X_test']-corrected_data['X_test'])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'Done'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "visual.img_samples([source_data, target_data, \n",
    "                 {'X_test': np.array(corrector_trainner.output(\n",
    "                corrector_data['X_test'])).reshape((-1,)+corrector_data['X_test'].shape[1:])}]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corrector_data.batchsize, corrector_data.X_val.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "fig, ax = plt.subplots()\n",
    "visual.target_2D(source_data.X_test, source_data.y_test, ax=ax);\n",
    "visual.add_legend(ax, title=data_name+model_name)\n",
    "# fig.tight_layout()\n",
    "fig.savefig(fig_dir+'C-Moons-SEUL'+'.png',bbox_inches='tight')\n",
    "fig.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "visual.target_2D(source_data.X_test, source_data.y_test, ax=ax);\n",
    "visual.corrected_2D(corrected_data.X_test, source_data.y_test, ax=ax);\n",
    "visual.add_legend(ax, title=data_name+model_name)\n",
    "# fig.tight_layout()\n",
    "fig.savefig(fig_dir+'C-Moons-APRES'+'.png',bbox_inches='tight')\n",
    "fig.show()\n",
    "\n",
    "# Plot data test points (source + target) on right fig\n",
    "fig, ax = plt.subplots()\n",
    "visual.target_2D(source_data.X_test, source_data.y_test, ax=ax);\n",
    "visual.source_2D(target_data.X_test, target_data.y_test, ax=ax);\n",
    "visual.add_legend(ax, title=data_name+model_name)\n",
    "# fig.tight_layout()\n",
    "fig.savefig(fig_dir+'C-Moons-AVANT'+'.png',bbox_inches='tight')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "corrected_data['X_train_centers'] = np.array(\n",
    "    corrector_trainner.output(corrector_data['X_train_centers'])).reshape((-1, 2))\n",
    "idx = corrector_data['preprocess']\n",
    "X = np.array(corrector_trainner.output(corrector_data['X_train_centers'])).reshape((-1, 2))\n",
    "Y = corrector_data['y_train_centers']\n",
    "Y = Y[idx]\n",
    "visual.target_2D(source_data.X_test, source_data.y_test, ax=ax);\n",
    "visual.corrected_2D(corrected_data.X_test, source_data.y_test, ax=ax);\n",
    "visual.centers_source(Y, ax=ax)\n",
    "visual.centers_corrected(X, ax=ax)\n",
    "visual.mapping(X, Y, ax=ax)\n",
    "visual.add_legend(ax, title=data_name+model_name)\n",
    "fig.savefig(fig_dir+'C-Moons-150'+'.png',bbox_inches='tight')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSNE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "fig, ax = visual.tsne(source_data['X_test'], target_data['X_test'], source_data['y_test'])\n",
    "visual.add_legend(ax, title='Source+target')\n",
    "ax.set_xlim(-150, 150)\n",
    "ax.set_ylim(-150, 150)\n",
    "\n",
    "fig, ax = visual.tsne(source_data['X_test'], \n",
    "            np.array(corrector_trainner.output(corrector_data['X_test'])).reshape((-1, 2)),\n",
    "            source_data['y_test'])\n",
    "visual.add_legend(ax, title='Source+Corrected')\n",
    "ax.set_xlim(-150, 150)\n",
    "ax.set_ylim(-150, 150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
