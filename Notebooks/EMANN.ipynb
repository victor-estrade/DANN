{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Loading of datasets](#Load-datasets)\n",
    "- [2. Transformation of datasets](#Transform-datasets)\n",
    "- [3. Helper functions](#Helper-functions)\n",
    "- [4. Manual EMANN](#Manual-EMANN)\n",
    "    - [4.1 EM Starts Here !](#EM-Starts-Here-!)\n",
    "- [5. ReGenerate data](#ReGenerate-data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import sys\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "import time\n",
    "import visual\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, pairwise_distances\n",
    "\n",
    "from nn.helper import CNN, NN\n",
    "from nn import block as nnb\n",
    "from nn import compilers as nnc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets\n",
    "\n",
    "- the datasets are loaded/built.\n",
    "- The batchsize is defined\n",
    "- half of the data name (the source part) is defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.toys import make_clouds, make_circles, make_X, make_moons\n",
    "from datasets.utils import make_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform datasets\n",
    "\n",
    "- the transformed datasets are built.\n",
    "- last part of the data name (the target part) is defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.utils import make_domain_dataset, make_corrector_dataset\n",
    "import datasets.transform as transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the solvers functions\n",
    "from opt_transport import opt_transp_sup, computeTransportSinkhorn, computeTransportSinkhornLabelsLpL1\n",
    "# Import loggers\n",
    "from logs import new_logger, empty_logger\n",
    "logger = new_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mass(k_means):\n",
    "    \"\"\"\n",
    "    Params\n",
    "    ------\n",
    "        k_means: (sklearn.cluster.KMeans instance)\n",
    "    Return\n",
    "    ------\n",
    "        w: (numpy.array [n_clusters]) the mass of each clusters \n",
    "    \"\"\"\n",
    "    w = np.unique(k_means.labels_, return_counts=True)[1]\n",
    "    w = w/np.sum(w)\n",
    "    return w\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from align_learn.preprocess import align\n",
    "\n",
    "def train_dataset(X_src, X_tgt, k_means_src, k_means_tgt, transp):\n",
    "    \"\"\"\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "        X_src:\n",
    "        X_tgt:\n",
    "        k_means_src:\n",
    "        k_means_tgt:\n",
    "        transp:\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "        X:\n",
    "        Y:\n",
    "    \"\"\"\n",
    "    align_idx, cluster_T = align(transp, k_means_src.labels_, k_means_tgt.labels_)\n",
    "    X_S, y_S = X_src, y_src\n",
    "    X_T, y_T = X_tgt[align_idx], y_tgt[align_idx]\n",
    "    # Build the probabilities to be predict\n",
    "    # For the source data\n",
    "    proba_src = np.zeros((X_S.shape[0], k_means_src.n_clusters))\n",
    "    proba_src[np.arange(X_S.shape[0]), k_means_src.labels_] = 1.\n",
    "    proba_tgt = transp[k_means_src.labels_]\n",
    "    proba_tgt = proba_tgt / np.sum(proba_tgt, 1).reshape(-1, 1)\n",
    "    Y_S = np.hstack([proba_src, proba_tgt])\n",
    "    \n",
    "    # Build the probabilities to be predict\n",
    "    # For the aligned target data\n",
    "    proba_tgt = np.zeros((X_T.shape[0], k_means_tgt.n_clusters))\n",
    "    proba_tgt[np.arange(X_T.shape[0]), cluster_T] = 1.\n",
    "    proba_src = transp[:, cluster_T].T\n",
    "    proba_src = proba_src / np.sum(proba_src, 1).reshape(-1, 1)\n",
    "    Y_T = np.hstack([proba_src, proba_tgt])\n",
    "    \n",
    "    Y = np.vstack([Y_S, Y_T])\n",
    "    X = np.vstack([X_S, X_T])\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual EMANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EM_ITER = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_classes_1 = 4\n",
    "n_classes_2 = 4\n",
    "n_samples = 1000\n",
    "X_src, y_src = make_clouds(n_samples=n_samples, n_classes=n_classes_1)\n",
    "\n",
    "# X_tgt, y_tgt = make_clouds(n_samples=n_samples, n_classes=n_classes_1)\n",
    "X_tgt, y_tgt = make_circles(n_samples=n_samples,  n_classes=n_classes_2)\n",
    "\n",
    "data_name='Clouds -> Same'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k_src = 10\n",
    "k_tgt = 10\n",
    "# We do not need to have the same number of cluster in the source and target data.\n",
    "k_means_src = KMeans(n_clusters=k_src).fit(X_src)\n",
    "k_means_tgt = KMeans(n_clusters=k_tgt).fit(X_tgt)\n",
    "# Mass and cost matrix\n",
    "w_src = mass(k_means_src)\n",
    "w_tgt = mass(k_means_tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random cost Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost_mat = np.random.uniform(0,1, size=(w_src.shape[0], w_tgt.shape[0]))\n",
    "visual.mat(cost_mat)\n",
    "plt.title(\"Cost matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pairwise euclidean distance cost matrix**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cost_mat = pairwise_distances(centers_src, centers_tgt)\n",
    "cost_mat = cost_mat**2\n",
    "visual.mat(cost_mat)\n",
    "plt.title(\"Cost matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Optimal Transport**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transp = computeTransportSinkhorn(w_src, w_tgt, cost_mat, reg=10)\n",
    "visual.mat(transp)\n",
    "plt.title(\"Transport matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Dual Proba dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y = train_dataset(X_src, X_tgt, k_means_src, k_means_tgt, transp)\n",
    "data = make_dataset(X, Y, batchsize=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Network Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get general information :\n",
    "# =========================\n",
    "batchsize = data.batchsize\n",
    "_shape = np.shape(data.X_train)\n",
    "n_dim = len(_shape)\n",
    "n_features = np.prod(_shape[1:])\n",
    "\n",
    "shape = (batchsize,) + _shape[1:]\n",
    "target_var = T.ivector('targets')\n",
    "\n",
    "# Logs\n",
    "logger.info('Building the input and output variables for : {}'.format(data_name))\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n",
    "\n",
    "# WARNING :: Une seule couche de proba. On prédit les ligne pas les colonnes !\n",
    "# Build the layers :\n",
    "# ==================\n",
    "def build():\n",
    "    input_layer = lasagne.layers.InputLayer(shape=shape)\n",
    "\n",
    "    dense_1 = lasagne.layers.DenseLayer(input_layer, 3, nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "    dense_2 = lasagne.layers.DenseLayer(dense_1, 3, nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "\n",
    "    # With concat :\n",
    "    cluster_src = lasagne.layers.DenseLayer(dense_2, k_src, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    cluster_tgt = lasagne.layers.DenseLayer(dense_2, k_tgt, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    concat_layer = lasagne.layers.ConcatLayer([cluster_src, cluster_tgt], axis=1)\n",
    "\n",
    "    end_layer = concat_layer\n",
    "    return end_layer, dense_2\n",
    "end_layer, repr_layer = build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile the NN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instanciate the NN :\n",
    "# ====================\n",
    "# nn = NN(end_layer, name='EMANN test')\n",
    "nn = CNN(name='EMANN test')\n",
    "nn.add_output('proba', end_layer)\n",
    "nn.add_output('repr', repr_layer)\n",
    "\n",
    "# Compile :\n",
    "# =========\n",
    "nn.compile('proba', nnc.crossentropy_sgd_mom, lr=0.1, mom=0.9)\n",
    "nn.compile('proba', nnc.crossentropy_validation)\n",
    "nn.compile('proba', nnc.output)\n",
    "nn.compile('repr', nnc.output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the NN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the nn :\n",
    "# ==============\n",
    "# nn.train(data, num_epochs=100);\n",
    "nn.train([data], ['proba'], num_epochs=100);\n",
    "\n",
    "# ================\n",
    "# Learning curve\n",
    "# ================\n",
    "fig, ax = visual.learning_curve(nn.global_stats, regex='loss')\n",
    "#     SAVE\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(fig_title+'-Learning_curve.png',bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = nn.parts['proba'].output(data.X_test)[0]\n",
    "i = np.random.randint(0, data.X_test.shape[0])\n",
    "# print('\\n'.join('{:1.5f}--{:1.5f}'.format(pred, truth) for pred, truth in zip(y_pred[i], data.y_test[i])))\n",
    "width=0.4\n",
    "plt.bar(np.arange(k_src+k_tgt), y_pred[i], width, color='r')\n",
    "plt.bar(np.arange(k_src+k_tgt)+width, data.y_test[i], width, color='b')\n",
    "plt.title(\"One point distrib\")\n",
    "plt.legend(bbox_to_anchor=(1.2,1.))\n",
    "# plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **EM Starts Here !**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "centers_src = nn.parts['repr'].output(k_means_src.cluster_centers_)[0]\n",
    "centers_tgt = nn.parts['repr'].output(k_means_tgt.cluster_centers_)[0]\n",
    "cost_mat = pairwise_distances(centers_src, centers_tgt)\n",
    "cost_mat = cost_mat**2\n",
    "visual.mat(cost_mat)\n",
    "plt.title(\"Cost matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimal Transport**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transp = computeTransportSinkhorn(w_src, w_tgt, cost_mat, reg=5)\n",
    "visual.mat(transp)\n",
    "plt.title(\"Transport matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dual Proba dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y = train_dataset(X_src, X_tgt, k_means_src, k_means_tgt, transp)\n",
    "data = make_dataset(X, Y, batchsize=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural network** (re-initialization)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Build the layers :\n",
    "# ==================\n",
    "end_layer, repr_layer = build()\n",
    "\n",
    "# Instanciate the NN :\n",
    "# ====================\n",
    "# nn = NN(end_layer, name='EMANN test')\n",
    "# nn = CNN(name='EMANN test')\n",
    "nn['proba'] = end_layer\n",
    "nn['repr'] = repr_layer\n",
    "\n",
    "# Compile :\n",
    "# =========\n",
    "# nn.compile(nnc.crossentropy_sgd_mom, lr=0.1, mom=0.9)\n",
    "# nn.compile(nnc.crossentropy_validation)\n",
    "# nn.compile(nnc.output)\n",
    "\n",
    "nn.compile('proba', nnc.crossentropy_sgd_mom, lr=0.01, mom=0.9)\n",
    "nn.compile('proba', nnc.crossentropy_validation)\n",
    "nn.compile('proba', nnc.output)\n",
    "nn.compile('repr', nnc.output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the NN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the nn :\n",
    "# ==============\n",
    "# nn.train(data, num_epochs=100);\n",
    "nn.train([data], ['proba'], num_epochs=100);\n",
    "# ================\n",
    "# Learning curve\n",
    "# ================\n",
    "fig, ax = visual.learning_curve(nn.global_stats, regex='loss')\n",
    "#     SAVE\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(fig_title+'-Learning_curve.png',bbox_inches='tight')\n",
    "fig.show()\n",
    "\n",
    "EM_ITER += 1\n",
    "print('Iteration n*', EM_ITER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = nn.parts['proba'].output(data.X_test)[0]\n",
    "i = np.random.randint(0, data.X_test.shape[0])\n",
    "# print('\\n'.join('{:1.5f}--{:1.5f}'.format(pred, truth) for pred, truth in zip(y_pred[i], data.y_test[i])))\n",
    "width = 0.4\n",
    "plt.bar(np.arange(k_src+k_tgt), y_pred[i], width, color='r', label='pred')\n",
    "plt.bar(np.arange(k_src+k_tgt)+width, data.y_test[i], width, color='b', label='truth')\n",
    "# plt.yscale('log')\n",
    "plt.title(\"One point distrib (loss = \"+str(-np.sum(data.y_test[i]*np.log(y_pred[i])))+\")\")\n",
    "plt.legend(bbox_to_anchor=(1.2,1.))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**[EM LOOP]**](#EM-Starts-Here-!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ReGenerate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
