{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Ici je vais écrire la version finale de la méthode EM pour le *cas 3 : Contrainte de pureté*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Loading of datasets](#Load-datasets)\n",
    "2. [Transformation of datasets](#Transform-datasets)\n",
    "3. [Helper functions](#Helper-functions)\n",
    "4. [Manual EMANN](#Manual-EMANN)\n",
    "    1. [EM Starts Here !](#EM-Starts-Here-!)\n",
    "5. [Test the result](#Test-the-result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import sys\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "import time\n",
    "import visual\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, pairwise_distances\n",
    "\n",
    "from nn.helper import CNN, NN\n",
    "from nn import block as nnb\n",
    "from nn import compilers as nnc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets\n",
    "\n",
    "- the datasets are loaded/built.\n",
    "- The batchsize is defined\n",
    "- half of the data name (the source part) is defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.toys import make_clouds, make_circles, make_X, make_moons\n",
    "from datasets.utils import make_dataset, make_domain_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform datasets\n",
    "\n",
    "- the transformed datasets are built.\n",
    "- last part of the data name (the target part) is defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.utils import make_domain_dataset, make_corrector_dataset\n",
    "import datasets.transform as transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import loggers\n",
    "from logs import new_logger, empty_logger\n",
    "logger = new_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from align_learn.probability import mass, align, proba_src_P, proba_tgt_P, renorm, softmax_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax_alpha computes :\n",
    "\n",
    "$$res_{ij} = \\frac{e^{\\alpha x_{ij}}}{\\sum_j e^{\\alpha x_{ij}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual EMANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EM_ITER = 0\n",
    "proba_P = proba_src_P\n",
    "# proba_P = proba_tgt_P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data\n",
    "\n",
    "Première étape : générer les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_classes_1 = 4\n",
    "n_classes_2 = 4\n",
    "n_samples = 1000\n",
    "X_src, y_src = make_clouds(n_samples=n_samples, n_classes=n_classes_1)\n",
    "\n",
    "X_tgt, y_tgt = make_clouds(n_samples=n_samples, n_classes=n_classes_1)\n",
    "# X_tgt, y_tgt = make_circles(n_samples=n_samples,  n_classes=n_classes_2)\n",
    "\n",
    "data_name='Clouds -> Same'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Clusters\n",
    "\n",
    "Choisir/construire les partitions $C_{1i}$ et $C_{2j}$. \n",
    "\n",
    "Avoir des labels pour chaque points, placés dans $l_{src}$ et $l_{tgt}$. On garde $y_{src}$ et $y_{tgt}$ pour les véritables labels de classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k_src = 10\n",
    "k_tgt = 12\n",
    "# We do not need to have the same number of cluster in the source and target data.\n",
    "k_means_src = KMeans(n_clusters=k_src).fit(X_src)\n",
    "k_means_tgt = KMeans(n_clusters=k_tgt).fit(X_tgt)\n",
    "# labels\n",
    "l_src, l_tgt = k_means_src.labels_, k_means_tgt.labels_\n",
    "# l_src, l_tgt = np.asarray(y_src, dtype=int), np.asarray(y_tgt, dtype=int),\n",
    "\n",
    "# Mass\n",
    "w_src = mass(l_src)\n",
    "w_tgt = mass(l_tgt)\n",
    "\n",
    "# Params\n",
    "n_class_tgt = len(np.unique(l_tgt))\n",
    "n_class_src = len(np.unique(l_src))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation de la matrice de proba du plongement.\n",
    "\n",
    "On met dans $P_{ij}$ la probabilité de plongement d'élément de la partition $C_{1i}$ dans $C_{2j}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "P = np.random.uniform(0,1, size=(n_class_src, n_class_tgt))\n",
    "P = renorm(P)\n",
    "visual.mat(P)\n",
    "plt.title(\"Proba matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "M = renorm(P, last='column')\n",
    "print(M.sum(0))\n",
    "print(M.sum(1))\n",
    "print('--------')\n",
    "M = renorm(P, last='line')\n",
    "print(M.sum(0))\n",
    "print(M.sum(1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "sa = softmax_alpha(M, 15)\n",
    "print(sa.max()/sa.min(), M.max()/M.min())\n",
    "visual.mat(sa);\n",
    "visual.mat(M);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training dataset\n",
    "\n",
    "Build the training datasets.\n",
    "\n",
    "The data from the source and the target distribution ordered so $x_s$ should correspond to $x_t$.\n",
    "\n",
    "The target is the probability that $x$ belong to the label $y$ in the source space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the alignment indexes according to the given probability matrix\n",
    "align_idx = align(P, l_src, l_tgt)\n",
    "# Align the data\n",
    "X_S, y_S = X_src, l_src\n",
    "X_T, y_T = X_tgt[align_idx], l_tgt[align_idx]\n",
    "# Get the probability to be predicted for each couple of data point.\n",
    "p_src, p_tgt = proba_P(P, l_src, l_tgt)\n",
    "n_class = n_class_tgt if proba_P is proba_tgt_P else n_class_src\n",
    "\n",
    "# Shuffle it all to prevent the index to be correclated to the labels\n",
    "indices = np.arange(X_S.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X_S, X_T, p_src, p_tgt = X_S[indices], X_T[indices], p_src[indices], p_tgt[indices]\n",
    "l_src, l_tgt = l_src[indices], l_tgt[indices]\n",
    "# Build split dataset (train, valid, test)\n",
    "src_data = make_dataset(X_S, p_src, batchsize=100)\n",
    "tgt_data = make_dataset(X_T, p_tgt, batchsize=100)\n",
    "adversarial_data = make_domain_dataset([src_data, tgt_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "\n",
    "2 entries : \n",
    "- one for the source data. The source data goes throught 2 NN parts $\\varphi$ (projection to target space) and $\\rho$ (classifier)\n",
    "- one for the target data. The target data goes throught 1 NN parts $\\rho$ (classifier)\n",
    "\n",
    "$\\rho(\\varphi (x_s)) = P(x_s\\in C_{1i})$\n",
    "\n",
    "$\\rho(x_t) = P(x_t\\in C_{1i} || x_t\\in C_{2j})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get general information :\n",
    "# =========================\n",
    "batchsize = src_data.batchsize\n",
    "_shape = np.shape(src_data.X_train)\n",
    "n_dim = len(_shape)\n",
    "n_features = np.prod(_shape[1:])\n",
    "\n",
    "shape = (batchsize,) + _shape[1:]\n",
    "target_var = T.ivector('targets')\n",
    "\n",
    "# Logs\n",
    "logger.info('Building the input and output variables for : {}'.format(data_name))\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n",
    "\n",
    "# WARNING :: Une seule couche de proba. On prédit les lignes pas les colonnes !\n",
    "# Build the layers :\n",
    "# ==================\n",
    "# Inputs layers\n",
    "# -------------\n",
    "input_layer_src = lasagne.layers.InputLayer(shape=shape)\n",
    "input_layer_tgt = lasagne.layers.InputLayer(shape=shape)\n",
    "\n",
    "# Representaion layers for the source data\n",
    "# ----------------------------------------\n",
    "dense_1 = lasagne.layers.DenseLayer(input_layer_src, 3, nonlinearity=lasagne.nonlinearities.rectify)\n",
    "dense_2 = lasagne.layers.DenseLayer(dense_1, shape[1], nonlinearity=None)\n",
    "repr_layer = dense_2\n",
    "\n",
    "# \"Classification\" layers for the source data\n",
    "# -------------------------------------------\n",
    "# WARNING :: Une seule couche de proba. On prédit les lignes pas les colonnes !\n",
    "# last = lasagne.layers.NonlinearityLayer(dense_2, nonlinearity=lasagne.nonlinearities.rectify)\n",
    "dense_3 = lasagne.layers.DenseLayer(repr_layer, 2, nonlinearity=lasagne.nonlinearities.rectify)\n",
    "cluster_src = lasagne.layers.DenseLayer(dense_3, n_class, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "# \"Classification\" layers for the target data\n",
    "# -------------------------------------------\n",
    "# WARNING :: Une seule couche de proba. On prédit les lignes pas les colonnes !\n",
    "dense_3_bis = lasagne.layers.DenseLayer(input_layer_tgt, 2, nonlinearity=lasagne.nonlinearities.rectify)\n",
    "cluster_tgt = lasagne.layers.DenseLayer(dense_3_bis, n_class, nonlinearity=lasagne.nonlinearities.softmax,\n",
    "                                         W=cluster_src.W, b=cluster_src.b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the NN\n",
    "\n",
    "Compile the functions:\n",
    "- training, validation, proba output for the source path\n",
    "- training, validation, proba output for the target path\n",
    "- raw output for the representation\n",
    "- training, validation, proba output for the adverssarial path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instanciate the NN :\n",
    "# ====================\n",
    "nn = CNN(name='EMANN test')\n",
    "nn.add_output('proba_src', cluster_src)\n",
    "nn.add_output('proba_tgt', cluster_tgt)\n",
    "nn.add_output('repr', repr_layer)\n",
    "# Ok for the adversarial the code is not intuitive. [Further work]\n",
    "nn.add_output('adversarial', [repr_layer, input_layer_tgt])\n",
    "\n",
    "# Compile :\n",
    "# =========\n",
    "nn.compile('proba_src', nnc.crossentropy_sgd_mom, lr=0.1, mom=0.9)\n",
    "nn.compile('proba_src', nnc.crossentropy_validation)\n",
    "nn.compile('proba_src', nnc.output)\n",
    "nn.compile('proba_tgt', nnc.crossentropy_sgd_mom, lr=0.1, mom=0.9)\n",
    "nn.compile('proba_tgt', nnc.crossentropy_validation)\n",
    "nn.compile('proba_tgt', nnc.output)\n",
    "nn.compile('repr', nnc.output)\n",
    "nn.compile('adversarial', nnc.adversarial, hp_lambda=0.1, lr=0.1, mom=0.9)\n",
    "\n",
    "logger.info(\"Compilation Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the NN\n",
    "\n",
    "Now is the training session.\n",
    "\n",
    "It altarnatively (mini-batch after mini-batch) train (forwward-backward propagation) each part of the neural network.\n",
    "\n",
    "- Source data $\\to$ Predict the label of the source data in the source space\n",
    "- Target data $\\to$ Predict the probability of being in the partition of the target data in the source space\n",
    "- Adversarial $\\to$ Predict from wich distribution the data comes from (Source or Target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the nn :\n",
    "# ==============\n",
    "# nn.train(data, num_epochs=100);\n",
    "nn.train([src_data, tgt_data, adversarial_data], ['proba_src', 'proba_tgt', 'adversarial'], num_epochs=5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ================\n",
    "# Learning curve\n",
    "# ================\n",
    "# Usefull regex : 'proba.* loss', 'loss', 'acc'\n",
    "fig, ax = visual.learning_curve(nn.global_stats, regex='loss')\n",
    "#     SAVE\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(fig_title+'-Learning_curve.png',bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check some results\n",
    "\n",
    "Check the output of the NN:\n",
    "\n",
    "The predicted probability of being in a partition **vs** the true value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = nn.parts['proba_src'].output(src_data.X_test)[0]\n",
    "i = np.random.randint(0, src_data.X_test.shape[0])\n",
    "# print('\\n'.join('{:1.5f}--{:1.5f}'.format(pred, truth) for pred, truth in zip(y_pred[i], data.y_test[i])))\n",
    "width=0.4\n",
    "plt.bar(np.arange(n_class), y_pred[i], width, color='r', label='prediction')\n",
    "plt.bar(np.arange(n_class)+width, src_data.y_test[i], width, color='b', label='true value')\n",
    "plt.title(\"One point distrib\")\n",
    "plt.legend(bbox_to_anchor=(1.25,1.))\n",
    "# plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = nn.parts['proba_tgt'].output(tgt_data.X_test)[0]\n",
    "i = np.random.randint(0, tgt_data.X_test.shape[0])\n",
    "# print('\\n'.join('{:1.5f}--{:1.5f}'.format(pred, truth) for pred, truth in zip(y_pred[i], data.y_test[i])))\n",
    "width=0.4\n",
    "plt.bar(np.arange(n_class), y_pred[i], width, color='r', label='prediction')\n",
    "plt.bar(np.arange(n_class)+width, tgt_data.y_test[i], width, color='b', label='true value')\n",
    "plt.title(\"One point distrib\")\n",
    "plt.legend(bbox_to_anchor=(1.25,1.))\n",
    "# plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = nn.parts['repr'].output(X_src)[0]\n",
    "fig, ax = visual.target_2D(X_tgt, y_tgt);\n",
    "visual.corrected_2D(X, y_src, ax=ax);\n",
    "visual.add_legend(ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **EM Starts Here !**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rebuild P**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get some outputs for the lines of P\n",
    "# -----------------------------------\n",
    "n_samples = X_T.shape[0]\n",
    "n_train = int(0.6*n_samples)\n",
    "n_val = int(0.15*n_samples)+n_train\n",
    "\n",
    "# for each label\n",
    "if proba_P is proba_tgt_P:\n",
    "    for l in np.unique(l_src):\n",
    "        # get some points\n",
    "        a = tgt_data.X_train[np.where(l_src[:n_train])]\n",
    "        x = a[np.random.choice(a.shape[0], size=10, replace=False)]\n",
    "        # get the output of the NN\n",
    "        p = nn.parts['proba_tgt'].output(x)\n",
    "        # Agregate lines\n",
    "        P[l, :] = np.median(p, axis=1)\n",
    "    # Update P\n",
    "else:\n",
    "    for l in np.unique(l_tgt):\n",
    "        # get some points\n",
    "        a = tgt_data.X_train[np.where(l_tgt[:n_train])]\n",
    "        x = a[np.random.choice(a.shape[0], size=10, replace=False)]\n",
    "        # get the output of the NN\n",
    "        p = nn.parts['proba_tgt'].output(x)\n",
    "        # Agregate lines\n",
    "        P[:, l] = np.median(p, axis=1)\n",
    "    # Update P\n",
    "    \n",
    "# P = softmax_alpha(P, alpha=15)\n",
    "\n",
    "fig, ax = visual.mat(P)\n",
    "plt.title(\"Proba matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dual Proba dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the alignment indexes according to the given probability matrix\n",
    "align_idx = align(P, l_src, l_tgt)\n",
    "# Align the data\n",
    "X_S, y_S = X_src, l_src\n",
    "X_T, y_T = X_tgt[align_idx], l_tgt[align_idx]\n",
    "# Get the probability to be predicted for each couple of data point.\n",
    "p_src, p_tgt = proba_P(P, l_src, l_tgt)\n",
    "n_class = n_class_tgt if proba_P is proba_tgt_P else n_class_src\n",
    "\n",
    "# Shuffle it all to prevent the index to be correclated to the labels\n",
    "indices = np.arange(X_S.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X_S, X_T, p_src, p_tgt = X_S[indices], X_T[indices], p_src[indices], p_tgt[indices]\n",
    "# Build split dataset (train, valid, test)\n",
    "src_data = make_dataset(X_S, p_src, batchsize=100)\n",
    "tgt_data = make_dataset(X_T, p_tgt, batchsize=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural network** (re-initialization)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Build the layers :\n",
    "# ==================\n",
    "# Inputs layers\n",
    "# -------------\n",
    "input_layer_src = lasagne.layers.InputLayer(shape=shape)\n",
    "input_layer_tgt = lasagne.layers.InputLayer(shape=shape)\n",
    "\n",
    "# Representaion layers for the source data\n",
    "# ----------------------------------------\n",
    "dense_1 = lasagne.layers.DenseLayer(input_layer_src, 3, nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "dense_2 = lasagne.layers.DenseLayer(dense_1, shape[1], nonlinearity=None)\n",
    "repr_layer = dense_2\n",
    "\n",
    "# \"Classification\" layers for the source data\n",
    "# -------------------------------------------\n",
    "# WARNING :: Une seule couche de proba. On prédit les lignes pas les colonnes !\n",
    "# last = lasagne.layers.NonlinearityLayer(dense_2, nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "# dense_3 = lasagne.layers.DenseLayer(last, 2, nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "cluster_src = lasagne.layers.DenseLayer(repr_layer, n_class_src, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "# \"Classification\" layers for the target data\n",
    "# -------------------------------------------\n",
    "# WARNING :: Une seule couche de proba. On prédit les lignes pas les colonnes !\n",
    "cluster_tgt = lasagne.layers.DenseLayer(input_layer_tgt, n_class_src, nonlinearity=lasagne.nonlinearities.softmax,\n",
    "                                         W=cluster_src.W, b=cluster_src.b)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Instanciate the NN :\n",
    "# ====================\n",
    "nn = CNN(name='EMANN test')\n",
    "nn.add_output('proba_src', cluster_src)\n",
    "nn.add_output('proba_tgt', cluster_tgt)\n",
    "nn.add_output('repr', repr_layer)\n",
    "# Ok for the adversarial the code is not intuitive. [Further work]\n",
    "nn.add_output('adversarial', [repr_layer, input_layer_tgt])\n",
    "\n",
    "# Compile :\n",
    "# =========\n",
    "nn.compile('proba_src', nnc.crossentropy_sgd_mom, lr=0.1, mom=0.9)\n",
    "nn.compile('proba_src', nnc.crossentropy_validation)\n",
    "nn.compile('proba_src', nnc.output)\n",
    "nn.compile('proba_tgt', nnc.crossentropy_sgd_mom, lr=0.1, mom=0.9)\n",
    "nn.compile('proba_tgt', nnc.crossentropy_validation)\n",
    "nn.compile('proba_tgt', nnc.output)\n",
    "nn.compile('repr', nnc.output)\n",
    "nn.compile('adversarial', nnc.adversarial, hp_lambda=0, lr=1, mom=0.9)\n",
    "\n",
    "logger.info(\"Compilation Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the NN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the nn :\n",
    "# ==============\n",
    "# nn.train(data, num_epochs=100);\n",
    "nn.train([src_data, tgt_data, adversarial_data], ['proba_src', 'proba_tgt', 'adversarial'], num_epochs=5);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EM_ITER += 1\n",
    "print('Iteration n*', EM_ITER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ================\n",
    "# Learning curve\n",
    "# ================\n",
    "# Usefull regex : 'proba.* loss', 'loss', 'acc'\n",
    "fig, ax = visual.learning_curve(nn.global_stats, regex='proba.* loss')\n",
    "#     SAVE\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(fig_title+'-Learning_curve.png',bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check some results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = nn.parts['proba_src'].output(src_data.X_test)[0]\n",
    "i = np.random.randint(0, src_data.X_test.shape[0])\n",
    "# print('\\n'.join('{:1.5f}--{:1.5f}'.format(pred, truth) for pred, truth in zip(y_pred[i], data.y_test[i])))\n",
    "width=0.4\n",
    "plt.bar(np.arange(n_class), y_pred[i], width, color='r', label='prediction')\n",
    "plt.bar(np.arange(n_class)+width, src_data.y_test[i], width, color='b', label='true value')\n",
    "plt.title(\"One point distrib\")\n",
    "plt.legend(bbox_to_anchor=(1.25,1.))\n",
    "# plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = nn.parts['proba_tgt'].output(tgt_data.X_test)[0]\n",
    "i = np.random.randint(0, tgt_data.X_test.shape[0])\n",
    "# print('\\n'.join('{:1.5f}--{:1.5f}'.format(pred, truth) for pred, truth in zip(y_pred[i], data.y_test[i])))\n",
    "width=0.4\n",
    "plt.bar(np.arange(n_class), y_pred[i], width, color='r', label='prediction')\n",
    "plt.bar(np.arange(n_class)+width, tgt_data.y_test[i], width, color='b', label='true value')\n",
    "plt.title(\"One point distrib\")\n",
    "plt.legend(bbox_to_anchor=(1.25,1.))\n",
    "# plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**[EM LOOP]**](#EM-Starts-Here-!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Test the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = nn.parts['repr'].output(X_src)[0]\n",
    "fig, ax = visual.target_2D(X_tgt, y_tgt);\n",
    "visual.corrected_2D(X, y_src, ax=ax);\n",
    "visual.add_legend(ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Remaining work\n",
    "\n",
    "- Have more pertinent graphics and results monitoring\n",
    "\n",
    "- **Build 2 similar Notebooks for case 1 and case 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
