{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Ici je vais écrire la version finale de la méthode EM pour le *cas 3 : Contrainte de pureté*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Loading of datasets](#Load-datasets)\n",
    "2. [Transformation of datasets](#Transform-datasets)\n",
    "3. [Helper functions](#Helper-functions)\n",
    "4. [Manual EMANN](#Manual-EMANN)\n",
    "    1. [EM Starts Here !](#EM-Starts-Here-!)\n",
    "5. [Test the result](#Test-the-result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import sys\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "import time\n",
    "import visual\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, pairwise_distances\n",
    "\n",
    "from nn.helper import CNN, NN\n",
    "from nn import block as nnb\n",
    "from nn import compilers as nnc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets\n",
    "\n",
    "- the datasets are loaded/built.\n",
    "- The batchsize is defined\n",
    "- half of the data name (the source part) is defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.toys import make_clouds, make_circles, make_X, make_moons\n",
    "from datasets.utils import make_dataset, make_domain_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform datasets\n",
    "\n",
    "- the transformed datasets are built.\n",
    "- last part of the data name (the target part) is defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.utils import make_domain_dataset, make_corrector_dataset\n",
    "import datasets.transform as transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import loggers\n",
    "from logs import new_logger, empty_logger\n",
    "logger = new_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mass(labels):\n",
    "    \"\"\"\n",
    "    Params\n",
    "    ------\n",
    "        labels: labels for each data point giving the partition it belongs to.\n",
    "    Return\n",
    "    ------\n",
    "        w: (numpy.array [n_clusters]) the mass of each clusters/partitions. \n",
    "    \"\"\"\n",
    "    w = np.unique(labels, return_counts=True)[1]\n",
    "    w = w/np.sum(w)\n",
    "    return w\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from align_learn.preprocess import align\n",
    "\n",
    "def train_dataset(X_src, X_tgt, k_means_src, k_means_tgt, transp, proba='tgt'):\n",
    "    \"\"\"\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "        X_src:\n",
    "        X_tgt:\n",
    "        k_means_src:\n",
    "        k_means_tgt:\n",
    "        transp:\n",
    "        proba: 'tgt', 'target', 'src', 'source', 'stacked'\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "        X:\n",
    "        Y:\n",
    "    \"\"\"\n",
    "    align_idx, cluster_T = align(transp, k_means_src.labels_, k_means_tgt.labels_)\n",
    "    X_S, y_S = X_src, y_src\n",
    "    X_T, y_T = X_tgt[align_idx], y_tgt[align_idx]\n",
    "    # Build the probabilities to be predict\n",
    "    # For the source data\n",
    "    proba_src = np.zeros((X_S.shape[0], k_means_src.n_clusters))\n",
    "    proba_src[np.arange(X_S.shape[0]), k_means_src.labels_] = 1.\n",
    "    proba_tgt = transp[k_means_src.labels_]\n",
    "    proba_tgt = proba_tgt / np.sum(proba_tgt, 1).reshape(-1, 1)\n",
    "    if proba=='stacked':\n",
    "        Y_S = np.hstack([proba_src, proba_tgt])\n",
    "    elif proba=='tgt' or proba=='target':\n",
    "        Y_S = proba_tgt\n",
    "    elif proba=='src' or proba=='source':\n",
    "        Y_S = proba_src\n",
    "    else:\n",
    "        raise ValueError(\"proba option should be 'tgt', 'target', 'src', 'source' or 'stacked'.\"\n",
    "                        \"{} found\".format(proba))\n",
    "\n",
    "    # Build the probabilities to be predict\n",
    "    # For the aligned target data\n",
    "    proba_tgt = np.zeros((X_T.shape[0], k_means_tgt.n_clusters))\n",
    "    proba_tgt[np.arange(X_T.shape[0]), cluster_T] = 1.\n",
    "    proba_src = transp[:, cluster_T].T\n",
    "    proba_src = proba_src / np.sum(proba_src, 1).reshape(-1, 1)\n",
    "    if proba=='stacked':\n",
    "        Y_T = np.hstack([proba_src, proba_tgt])\n",
    "    elif proba=='tgt' or proba=='target':\n",
    "        Y_T = proba_tgt\n",
    "    elif proba=='src' or proba=='source':\n",
    "        Y_T = proba_src\n",
    "    else:\n",
    "        raise ValueError(\"proba option should be 'tgt', 'target', 'src', 'source' or 'stacked'.\"\n",
    "                        \"{} found\".format(proba))\n",
    "    \n",
    "    Y = np.vstack([Y_S, Y_T])\n",
    "    X = np.vstack([X_S, X_T])\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def align(P, y_src, y_tgt):\n",
    "    \"\"\"\n",
    "    Return indexes of the chosen target data.\n",
    "    then\n",
    "    >>> X_S, y_S = X_src, y_src\n",
    "    >>> X_T, y_T = X_tgt[align_idx], y_tgt[align_idx]\n",
    "    coresponding transport lines and columns = k_means_src.labels_, k_means_tgt.labels_[idx_tgt]\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "        P: the transport/probability matrix\n",
    "        y_src: the source's data labels\n",
    "        y_tgt: the target's data labels\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "        align_idx, cluster_T\n",
    "    \"\"\"\n",
    "    labels_src = np.unique(y_src)\n",
    "    labels_tgt = np.unique(y_tgt)\n",
    "    src_slices = [np.where(y_src == l_src)[0] for l_src in labels_src]\n",
    "    tgt_slices = [np.where(y_tgt == l_tgt)[0] for l_tgt in labels_tgt]\n",
    "    # src member of cluster i go to some tgt cluster j \n",
    "    # with the probability in the i-th row of transport matrix\n",
    "    cluster_choice = [np.random.choice(len(tgt_slices), size=src_idx.shape, p=P[i]/np.sum(P[i])) \n",
    "           for i, src_idx in enumerate(src_slices)]\n",
    "    # Stack it into array. And use the former indexes to match X_src with X_tgt[res]\n",
    "    cluster_choice_array = np.hstack(cluster_choice)[np.hstack(src_slices)]\n",
    "    align_idx = np.array([np.random.choice(tgt_slices[i]) for i in cluster_choice_array])\n",
    "    return align_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# align_idx = align(P, y_src, y_tgt)\n",
    "# X_S, y_S = X_src, y_src\n",
    "# X_T, y_T = X_tgt[align_idx], y_tgt[align_idx]\n",
    "# p_src, p_tgt = proba_src_P(P, y_src, y_tgt)\n",
    "\n",
    "\n",
    "def proba_src_P(P, y_src, y_tgt):\n",
    "    \"\"\"\n",
    "    return the probability of the given \n",
    "    \"\"\"\n",
    "    # Build the probabilities to be predict\n",
    "    # For the source data in the source distribution/partition\n",
    "    n_label = len(np.unique(y_src))\n",
    "    n_sample = y_src.shape[0]\n",
    "    proba_src = np.zeros((n_sample, n_label))\n",
    "    proba_src[np.arange(n_sample), y_src] = 1.\n",
    "    \n",
    "    # For the target data in the source distribution/partition\n",
    "    proba_tgt = P[:, y_tgt].T\n",
    "    proba_tgt = proba_tgt / np.sum(proba_tgt, 1)[:, np.newaxis]\n",
    "    \n",
    "    return proba_src, proba_tgt\n",
    "\n",
    "def proba_tgt_P(P, y_src, y_tgt):\n",
    "    \"\"\"\n",
    "    return the probability of the given \n",
    "    \"\"\"\n",
    "    # Build the probabilities to be predict\n",
    "    # For the target data in the target distribution/partition\n",
    "    n_label = len(np.unique(y_tgt))\n",
    "    n_sample = y_tgt.shape[0]\n",
    "    proba_tgt = np.zeros((n_sample, n_label))\n",
    "    proba_tgt[np.arange(n_sample), y_tgt] = 1.\n",
    "    \n",
    "    # For the source data in the target distribution/partition\n",
    "    proba_src = P[y_src]\n",
    "    proba_src = proba_src / np.sum(proba_src, 1)[:, np.newaxis]\n",
    "    \n",
    "    return proba_src, proba_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def renorm(M, w_S=None, w_T=None, max_iter=200, epsilon=1e-6, last='line'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # init data\n",
    "    # ---------\n",
    "    if w_S is None:\n",
    "        w_S = np.ones(np.shape(M)[0])\n",
    "    if w_T is None:\n",
    "        w_T = np.ones(np.shape(M)[1])\n",
    "    Nini = len(w_S)\n",
    "    Nfin = len(w_T)\n",
    "    \n",
    "    # we assume that no distances are null except those of the diagonal of distances\n",
    "    u = np.ones(Nini)/Nini\n",
    "    v = np.ones(Nfin)/Nfin\n",
    "    uprev = np.zeros(Nini)\n",
    "    cpt = 0\n",
    "    err = 1\n",
    "    # Main loop\n",
    "    # ---------\n",
    "    while (err > epsilon and cpt < max_iter):\n",
    "        cpt = cpt +1\n",
    "        # First we do a sanity check\n",
    "        if np.logical_or(np.any(np.dot(M.T,u)==0),np.isnan(np.sum(u))):\n",
    "            # we have reached the machine precision\n",
    "            # come back to previous solution and quit loop\n",
    "            print('Infinity')\n",
    "            if cpt!=0:\n",
    "                u = uprev\n",
    "            break\n",
    "        uprev = u  # Save the previous results in case of divide by 0\n",
    "        # now the real algo part : update vectors u and v\n",
    "        v = w_T/np.dot(M.T,u)\n",
    "        u = w_S/np.dot(M,v)\n",
    "        # Computing the new error value\n",
    "        if cpt%10==0:\n",
    "            # we can speed up the process by checking for the error only all the n-th iterations\n",
    "            final = u[:, np.newaxis]*M*v[:, np.newaxis].T\n",
    "            err = np.linalg.norm((np.sum(final, axis=0)-w_T))**2\n",
    "    # End of Main loop\n",
    "    # Return the transpotation matrix\n",
    "    if last=='column':\n",
    "        v = w_T/np.dot(M.T,u)\n",
    "    return u[:, np.newaxis]*M*v[:, np.newaxis].T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax_alpha computes :\n",
    "\n",
    "$$res_{ij} = \\frac{e^{\\alpha x_{ij}}}{\\sum_j e^{\\alpha x_{ij}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_alpha(x, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Compute softmax values for each sets of scores in x.\n",
    "    \n",
    "    $res_{ij} = \\frac{e^{\\alpha x_{ij}}}{\\sum_j e^{\\alpha x_{ij}}}$\n",
    "    \"\"\"\n",
    "    e_x = np.exp(alpha*(x - np.max(x)))\n",
    "    return e_x / e_x.sum(1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual EMANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EM_ITER = 0\n",
    "# proba_P = proba_src_P\n",
    "proba_P = proba_tgt_P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data\n",
    "\n",
    "Première étape : générer les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_classes_1 = 4\n",
    "n_classes_2 = 4\n",
    "n_samples = 1000\n",
    "X_src, y_src = make_clouds(n_samples=n_samples, n_classes=n_classes_1)\n",
    "\n",
    "X_tgt, y_tgt = make_clouds(n_samples=n_samples, n_classes=n_classes_1)\n",
    "# X_tgt, y_tgt = make_circles(n_samples=n_samples,  n_classes=n_classes_2)\n",
    "\n",
    "data_name='Clouds -> Same'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Clusters\n",
    "\n",
    "Choisir/construire les partitions $C_{1i}$ et $C_{2j}$. \n",
    "\n",
    "Avoir des labels pour chaque points, placés dans $l_{src}$ et $l_{tgt}$. On garde $y_{src}$ et $y_{tgt}$ pour les véritables labels de classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k_src = 10\n",
    "k_tgt = 12\n",
    "# We do not need to have the same number of cluster in the source and target data.\n",
    "k_means_src = KMeans(n_clusters=k_src).fit(X_src)\n",
    "k_means_tgt = KMeans(n_clusters=k_tgt).fit(X_tgt)\n",
    "# labels\n",
    "l_src, l_tgt = k_means_src.labels_, k_means_tgt.labels_\n",
    "# l_src, l_tgt = np.asarray(y_src, dtype=int), np.asarray(y_tgt, dtype=int),\n",
    "\n",
    "# Mass\n",
    "w_src = mass(l_src)\n",
    "w_tgt = mass(l_tgt)\n",
    "\n",
    "# Params\n",
    "n_class_tgt = len(np.unique(l_tgt))\n",
    "n_class_src = len(np.unique(l_src))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation de la matrice de proba du plongement.\n",
    "\n",
    "On met dans $P_{ij}$ la probabilité de plongement d'élément de la partition $C_{1i}$ dans $C_{2j}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "P = np.random.uniform(0,1, size=(n_class_src, n_class_tgt))\n",
    "P = renorm(P)\n",
    "visual.mat(P)\n",
    "plt.title(\"Proba matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "M = renorm(P, last='column')\n",
    "print(M.sum(0))\n",
    "print(M.sum(1))\n",
    "print('--------')\n",
    "M = renorm(P, last='line')\n",
    "print(M.sum(0))\n",
    "print(M.sum(1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "sa = softmax_alpha(M, 15)\n",
    "print(sa.max()/sa.min(), M.max()/M.min())\n",
    "visual.mat(sa);\n",
    "visual.mat(M);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training dataset\n",
    "\n",
    "Build the training datasets.\n",
    "\n",
    "The data from the source and the target distribution ordered so $x_s$ should correspond to $x_t$.\n",
    "\n",
    "The target is the probability that $x$ belong to the label $y$ in the source space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the alignment indexes according to the given probability matrix\n",
    "align_idx = align(P, l_src, l_tgt)\n",
    "# Align the data\n",
    "X_S, y_S = X_src, l_src\n",
    "X_T, y_T = X_tgt[align_idx], l_tgt[align_idx]\n",
    "# Get the probability to be predicted for each couple of data point.\n",
    "p_src, p_tgt = proba_P(P, l_src, l_tgt)\n",
    "n_class = n_class_tgt if proba_P is proba_tgt_P else n_class_src\n",
    "\n",
    "# Shuffle it all to prevent the index to be correclated to the labels\n",
    "indices = np.arange(X_S.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X_S, X_T, p_src, p_tgt = X_S[indices], X_T[indices], p_src[indices], p_tgt[indices]\n",
    "l_src, l_tgt = l_src[indices], l_tgt[indices]\n",
    "# Build split dataset (train, valid, test)\n",
    "src_data = make_dataset(X_S, p_src, batchsize=100)\n",
    "tgt_data = make_dataset(X_T, p_tgt, batchsize=100)\n",
    "adversarial_data = make_domain_dataset([src_data, tgt_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "\n",
    "2 entries : \n",
    "- one for the source data. The source data goes throught 2 NN parts $\\varphi$ (projection to target space) and $\\rho$ (classifier)\n",
    "- one for the target data. The target data goes throught 1 NN parts $\\rho$ (classifier)\n",
    "\n",
    "$\\rho(\\varphi (x_s)) = P(x_s\\in C_{1i})$\n",
    "\n",
    "$\\rho(x_t) = P(x_t\\in C_{1i} || x_t\\in C_{2j})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get general information :\n",
    "# =========================\n",
    "batchsize = src_data.batchsize\n",
    "_shape = np.shape(src_data.X_train)\n",
    "n_dim = len(_shape)\n",
    "n_features = np.prod(_shape[1:])\n",
    "\n",
    "shape = (batchsize,) + _shape[1:]\n",
    "target_var = T.ivector('targets')\n",
    "\n",
    "# Logs\n",
    "logger.info('Building the input and output variables for : {}'.format(data_name))\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n",
    "\n",
    "# WARNING :: Une seule couche de proba. On prédit les lignes pas les colonnes !\n",
    "# Build the layers :\n",
    "# ==================\n",
    "# Inputs layers\n",
    "# -------------\n",
    "input_layer_src = lasagne.layers.InputLayer(shape=shape)\n",
    "input_layer_tgt = lasagne.layers.InputLayer(shape=shape)\n",
    "\n",
    "# Representaion layers for the source data\n",
    "# ----------------------------------------\n",
    "dense_1 = lasagne.layers.DenseLayer(input_layer_src, 3, nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "dense_2 = lasagne.layers.DenseLayer(dense_1, shape[1], nonlinearity=None)\n",
    "repr_layer = dense_2\n",
    "\n",
    "# \"Classification\" layers for the source data\n",
    "# -------------------------------------------\n",
    "# WARNING :: Une seule couche de proba. On prédit les lignes pas les colonnes !\n",
    "# last = lasagne.layers.NonlinearityLayer(dense_2, nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "# dense_3 = lasagne.layers.DenseLayer(last, 2, nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "cluster_src = lasagne.layers.DenseLayer(repr_layer, n_class, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "# \"Classification\" layers for the target data\n",
    "# -------------------------------------------\n",
    "# WARNING :: Une seule couche de proba. On prédit les lignes pas les colonnes !\n",
    "cluster_tgt = lasagne.layers.DenseLayer(input_layer_tgt, n_class, nonlinearity=lasagne.nonlinearities.softmax,\n",
    "                                         W=cluster_src.W, b=cluster_src.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the NN\n",
    "\n",
    "Compile the functions:\n",
    "- training, validation, proba output for the source path\n",
    "- training, validation, proba output for the target path\n",
    "- raw output for the representation\n",
    "- training, validation, proba output for the adverssarial path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instanciate the NN :\n",
    "# ====================\n",
    "nn = CNN(name='EMANN test')\n",
    "nn.add_output('proba_src', cluster_src)\n",
    "nn.add_output('proba_tgt', cluster_tgt)\n",
    "nn.add_output('repr', repr_layer)\n",
    "# Ok for the adversarial the code is not intuitive. [Further work]\n",
    "nn.add_output('adversarial', [repr_layer, input_layer_tgt])\n",
    "\n",
    "# Compile :\n",
    "# =========\n",
    "nn.compile('proba_src', nnc.crossentropy_sgd_mom, lr=0.1, mom=0.9)\n",
    "nn.compile('proba_src', nnc.crossentropy_validation)\n",
    "nn.compile('proba_src', nnc.output)\n",
    "nn.compile('proba_tgt', nnc.crossentropy_sgd_mom, lr=0.1, mom=0.9)\n",
    "nn.compile('proba_tgt', nnc.crossentropy_validation)\n",
    "nn.compile('proba_tgt', nnc.output)\n",
    "nn.compile('repr', nnc.output)\n",
    "nn.compile('adversarial', nnc.adversarial, hp_lambda=0, lr=1, mom=0.9)\n",
    "\n",
    "logger.info(\"Compilation Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the NN\n",
    "\n",
    "Now is the training session.\n",
    "\n",
    "It altarnatively (mini-batch after mini-batch) train (forwward-backward propagation) each part of the neural network.\n",
    "\n",
    "- Source data $\\to$ Predict the label of the source data in the source space\n",
    "- Target data $\\to$ Predict the probability of being in the partition of the target data in the source space\n",
    "- Adversarial $\\to$ Predict from wich distribution the data comes from (Source or Target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the nn :\n",
    "# ==============\n",
    "# nn.train(data, num_epochs=100);\n",
    "nn.train([src_data, tgt_data, adversarial_data], ['proba_src', 'proba_tgt', 'adversarial'], num_epochs=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ================\n",
    "# Learning curve\n",
    "# ================\n",
    "# Usefull regex : 'proba.* loss', 'loss', 'acc'\n",
    "fig, ax = visual.learning_curve(nn.global_stats, regex='proba.* loss')\n",
    "#     SAVE\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(fig_title+'-Learning_curve.png',bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check some results\n",
    "\n",
    "Check the output of the NN:\n",
    "\n",
    "The predicted probability of being in a partition **vs** the true value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = nn.parts['proba_src'].output(src_data.X_test)[0]\n",
    "i = np.random.randint(0, src_data.X_test.shape[0])\n",
    "# print('\\n'.join('{:1.5f}--{:1.5f}'.format(pred, truth) for pred, truth in zip(y_pred[i], data.y_test[i])))\n",
    "width=0.4\n",
    "plt.bar(np.arange(n_class), y_pred[i], width, color='r', label='prediction')\n",
    "plt.bar(np.arange(n_class)+width, src_data.y_test[i], width, color='b', label='true value')\n",
    "plt.title(\"One point distrib\")\n",
    "plt.legend(bbox_to_anchor=(1.25,1.))\n",
    "# plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = nn.parts['proba_tgt'].output(tgt_data.X_test)[0]\n",
    "i = np.random.randint(0, tgt_data.X_test.shape[0])\n",
    "# print('\\n'.join('{:1.5f}--{:1.5f}'.format(pred, truth) for pred, truth in zip(y_pred[i], data.y_test[i])))\n",
    "width=0.4\n",
    "plt.bar(np.arange(n_class), y_pred[i], width, color='r', label='prediction')\n",
    "plt.bar(np.arange(n_class)+width, tgt_data.y_test[i], width, color='b', label='true value')\n",
    "plt.title(\"One point distrib\")\n",
    "plt.legend(bbox_to_anchor=(1.25,1.))\n",
    "# plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **EM Starts Here !**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rebuild P**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get some outputs for the lines of P\n",
    "# -----------------------------------\n",
    "n_samples = X_T.shape[0]\n",
    "n_train = int(0.6*n_samples)\n",
    "n_val = int(0.15*n_samples)+n_train\n",
    "\n",
    "# for each label\n",
    "if proba_P is proba_tgt_P:\n",
    "    for l in np.unique(l_src):\n",
    "        # get some points\n",
    "        a = tgt_data.X_train[np.where(l_src[:n_train])]\n",
    "        x = a[np.random.choice(a.shape[0], size=10, replace=False)]\n",
    "        # get the output of the NN\n",
    "        p = nn.parts['proba_tgt'].output(x)\n",
    "        # Agregate lines\n",
    "        P[l, :] = np.median(p, axis=1)\n",
    "    # Update P\n",
    "else:\n",
    "    for l in np.unique(l_tgt):\n",
    "        # get some points\n",
    "        a = tgt_data.X_train[np.where(l_tgt[:n_train])]\n",
    "        x = a[np.random.choice(a.shape[0], size=10, replace=False)]\n",
    "        # get the output of the NN\n",
    "        p = nn.parts['proba_tgt'].output(x)\n",
    "        # Agregate lines\n",
    "        P[:, l] = np.median(p, axis=1)\n",
    "    # Update P\n",
    "    \n",
    "P = softmax_alpha(P, alpha=15)\n",
    "\n",
    "fig, ax = visual.mat(P)\n",
    "plt.title(\"Proba matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dual Proba dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the alignment indexes according to the given probability matrix\n",
    "align_idx = align(P, l_src, l_tgt)\n",
    "# Align the data\n",
    "X_S, y_S = X_src, l_src\n",
    "X_T, y_T = X_tgt[align_idx], l_tgt[align_idx]\n",
    "# Get the probability to be predicted for each couple of data point.\n",
    "p_src, p_tgt = proba_P(P, l_src, l_tgt)\n",
    "n_class = n_class_tgt if proba_P is proba_tgt_P else n_class_src\n",
    "\n",
    "# Shuffle it all to prevent the index to be correclated to the labels\n",
    "indices = np.arange(X_S.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X_S, X_T, p_src, p_tgt = X_S[indices], X_T[indices], p_src[indices], p_tgt[indices]\n",
    "# Build split dataset (train, valid, test)\n",
    "src_data = make_dataset(X_S, p_src, batchsize=100)\n",
    "tgt_data = make_dataset(X_T, p_tgt, batchsize=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural network** (re-initialization)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Build the layers :\n",
    "# ==================\n",
    "# Inputs layers\n",
    "# -------------\n",
    "input_layer_src = lasagne.layers.InputLayer(shape=shape)\n",
    "input_layer_tgt = lasagne.layers.InputLayer(shape=shape)\n",
    "\n",
    "# Representaion layers for the source data\n",
    "# ----------------------------------------\n",
    "dense_1 = lasagne.layers.DenseLayer(input_layer_src, 3, nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "dense_2 = lasagne.layers.DenseLayer(dense_1, shape[1], nonlinearity=None)\n",
    "repr_layer = dense_2\n",
    "\n",
    "# \"Classification\" layers for the source data\n",
    "# -------------------------------------------\n",
    "# WARNING :: Une seule couche de proba. On prédit les lignes pas les colonnes !\n",
    "# last = lasagne.layers.NonlinearityLayer(dense_2, nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "# dense_3 = lasagne.layers.DenseLayer(last, 2, nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "cluster_src = lasagne.layers.DenseLayer(repr_layer, n_class_src, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "# \"Classification\" layers for the target data\n",
    "# -------------------------------------------\n",
    "# WARNING :: Une seule couche de proba. On prédit les lignes pas les colonnes !\n",
    "cluster_tgt = lasagne.layers.DenseLayer(input_layer_tgt, n_class_src, nonlinearity=lasagne.nonlinearities.softmax,\n",
    "                                         W=cluster_src.W, b=cluster_src.b)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Instanciate the NN :\n",
    "# ====================\n",
    "nn = CNN(name='EMANN test')\n",
    "nn.add_output('proba_src', cluster_src)\n",
    "nn.add_output('proba_tgt', cluster_tgt)\n",
    "nn.add_output('repr', repr_layer)\n",
    "# Ok for the adversarial the code is not intuitive. [Further work]\n",
    "nn.add_output('adversarial', [repr_layer, input_layer_tgt])\n",
    "\n",
    "# Compile :\n",
    "# =========\n",
    "nn.compile('proba_src', nnc.crossentropy_sgd_mom, lr=0.1, mom=0.9)\n",
    "nn.compile('proba_src', nnc.crossentropy_validation)\n",
    "nn.compile('proba_src', nnc.output)\n",
    "nn.compile('proba_tgt', nnc.crossentropy_sgd_mom, lr=0.1, mom=0.9)\n",
    "nn.compile('proba_tgt', nnc.crossentropy_validation)\n",
    "nn.compile('proba_tgt', nnc.output)\n",
    "nn.compile('repr', nnc.output)\n",
    "nn.compile('adversarial', nnc.adversarial, hp_lambda=0, lr=1, mom=0.9)\n",
    "\n",
    "logger.info(\"Compilation Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the NN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the nn :\n",
    "# ==============\n",
    "# nn.train(data, num_epochs=100);\n",
    "nn.train([src_data, tgt_data, adversarial_data], ['proba_src', 'proba_tgt', 'adversarial'], num_epochs=50);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EM_ITER += 1\n",
    "print('Iteration n*', EM_ITER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ================\n",
    "# Learning curve\n",
    "# ================\n",
    "# Usefull regex : 'proba.* loss', 'loss', 'acc'\n",
    "fig, ax = visual.learning_curve(nn.global_stats, regex='proba.* loss')\n",
    "#     SAVE\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(fig_title+'-Learning_curve.png',bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check some results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = nn.parts['proba_src'].output(src_data.X_test)[0]\n",
    "i = np.random.randint(0, src_data.X_test.shape[0])\n",
    "# print('\\n'.join('{:1.5f}--{:1.5f}'.format(pred, truth) for pred, truth in zip(y_pred[i], data.y_test[i])))\n",
    "width=0.4\n",
    "plt.bar(np.arange(n_class), y_pred[i], width, color='r', label='prediction')\n",
    "plt.bar(np.arange(n_class)+width, src_data.y_test[i], width, color='b', label='true value')\n",
    "plt.title(\"One point distrib\")\n",
    "plt.legend(bbox_to_anchor=(1.25,1.))\n",
    "# plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = nn.parts['proba_tgt'].output(tgt_data.X_test)[0]\n",
    "i = np.random.randint(0, tgt_data.X_test.shape[0])\n",
    "# print('\\n'.join('{:1.5f}--{:1.5f}'.format(pred, truth) for pred, truth in zip(y_pred[i], data.y_test[i])))\n",
    "width=0.4\n",
    "plt.bar(np.arange(n_class), y_pred[i], width, color='r', label='prediction')\n",
    "plt.bar(np.arange(n_class)+width, tgt_data.y_test[i], width, color='b', label='true value')\n",
    "plt.title(\"One point distrib\")\n",
    "plt.legend(bbox_to_anchor=(1.25,1.))\n",
    "# plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**[EM LOOP]**](#EM-Starts-Here-!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Test the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = nn.parts['repr'].output(X_src)[0]\n",
    "fig, ax = visual.corrected_2D(X, y_src);\n",
    "# visual.target_2D(X_tgt, y_tgt, ax=ax);\n",
    "visual.add_legend(ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Remaining work\n",
    "\n",
    "- Agregate the output of the NN to update P\n",
    "- Have pertinent graphics and results monitoring\n",
    "\n",
    "- **Build 2 similar Notebooks for case 1 and case 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
