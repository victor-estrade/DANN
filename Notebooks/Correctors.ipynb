{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "from __future__ import division, print_function\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from logs import log_fname, new_logger\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets\n",
    "\n",
    "Here the datasets are loaded/built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets.toys import load_circles\n",
    "n_samples = 80  # Number of sample per class\n",
    "n_classes = 10\n",
    "batchsize = 80\n",
    "_data_name = 'Circles'\n",
    "source_data = load_circles(n_samples=n_samples, n_classes=n_classes, batchsize=batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.toys import load_clouds\n",
    "n_samples = 30  # Number of sample per class\n",
    "n_classes = 3\n",
    "batchsize = 80\n",
    "_data_name = 'Clouds'\n",
    "source_data = load_clouds(n_samples=n_samples, n_classes=n_classes, batchsize=batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.toys import load_moons\n",
    "n_samples = 800\n",
    "batchsize = 80\n",
    "_data_name = 'Moons'\n",
    "source_data = load_moons(n_samples=n_samples, batchsize=batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.mnist import load_mnist\n",
    "batchsize = 500\n",
    "_data_name = 'MNIST'\n",
    "source_data = load_mnist(batchsize=batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform datasets\n",
    "\n",
    "Here the transformed datasets are built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets.utils import make_domain_dataset, make_corrector_dataset\n",
    "from datasets.transform import rotate_dataset\n",
    "\n",
    "data_name = _data_name+'_Rotated'\n",
    "angle = 80\n",
    "\n",
    "target_data = rotate_dataset(source_data)\n",
    "domain_data = make_domain_dataset([source_data, target_data])\n",
    "corrector_data = make_corrector_dataset(source_data, target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data . Random Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.utils import make_domain_dataset, make_corrector_dataset\n",
    "from datasets.transform import random_mat_dataset\n",
    "\n",
    "data_name = _data_name+'_RMat'\n",
    "\n",
    "target_data = random_mat_dataset(source_data, normalize=False)\n",
    "domain_data = make_domain_dataset([source_data, target_data])\n",
    "corrector_data = make_corrector_dataset(source_data, target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data . Diag Dominant matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.utils import make_domain_dataset, make_corrector_dataset\n",
    "from datasets.transform import diag_dataset\n",
    "\n",
    "data_name = _data_name+'_Diag'\n",
    "\n",
    "target_data = diag_dataset(source_data, normalize=True)\n",
    "domain_data = make_domain_dataset([source_data, target_data])\n",
    "corrector_data = make_corrector_dataset(source_data, target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Mirror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.utils import make_domain_dataset, make_corrector_dataset\n",
    "from datasets.transform import mirror_dataset\n",
    "\n",
    "data_name = _data_name+'_Mirror'\n",
    "\n",
    "target_data = mirror_dataset(source_data)\n",
    "domain_data = make_domain_dataset([source_data, target_data])\n",
    "corrector_data = make_corrector_dataset(source_data, target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epoch Preprocessing\n",
    "\n",
    "The preprocessing function that will run at the begining of each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Neural Network\n",
    "\n",
    "^^[Back to the Loading of datasets](#Load-datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nn.rgl import ReverseGradientLayer\n",
    "from nn.block import adversarial\n",
    "from nn.compilers import crossentropy_sgd_mom, squared_error_sgd_mom\n",
    "from nn.training import Trainner, training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network building\n",
    "Start with the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hp_lambda = 0.\n",
    "\n",
    "label_rate = 1\n",
    "label_mom = 0.9\n",
    "\n",
    "domain_rate = 1\n",
    "domain_mom = 0.9\n",
    "\n",
    "# Get a logger\n",
    "logger = new_logger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare Theano variables for inputs and targets\n",
    "if data_name.startswith('MNIST'):\n",
    "    input_var = T.tensor3('inputs')\n",
    "    src_var = T.tensor3('src')\n",
    "    target_var = T.tensor3('targets')\n",
    "    shape = (batchsize, 28, 28)\n",
    "elif data_name.startswith('Moon') or data_name.startswith('Clouds') or data_name.startswith('Circles'):\n",
    "    input_var = T.matrix('inputs')\n",
    "    src_var = T.matrix('src')\n",
    "    target_var = T.matrix('targets')\n",
    "    shape = (batchsize, 2)\n",
    "\n",
    "# Logs\n",
    "logger.info('Building the input and output variables for |{}|'.format(data_name))\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "^^[Back to the Loading of datasets](#Load-datasets)\n",
    "\n",
    "^[Back to the init of the NN](#Build-the-Neural-Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the layers\n",
    "input_layer = lasagne.layers.InputLayer(shape=shape, input_var=input_var)\n",
    "src_layer = lasagne.layers.InputLayer(shape=shape, input_var=src_var)\n",
    "# feature = lasagne.layers.DenseLayer(\n",
    "#                 input_layer,\n",
    "#                 num_units=np.prod(shape[1:]),\n",
    "#                 nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#                 # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "#                 )\n",
    "feature = lasagne.layers.DenseLayer(\n",
    "                input_layer,\n",
    "                num_units=np.prod(shape[1:]),  # Should have same number as the input dimension\n",
    "                nonlinearity=None,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n",
    "reshaper = lasagne.layers.ReshapeLayer(feature, (-1,) + shape[1:])\n",
    "output_layer = reshaper\n",
    "\n",
    "# Logs\n",
    "logger.info('Building the neural network architecture for |{}|'.format(data_name))\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling \n",
    "\n",
    "^^[Back to the Loading of datasets](#Load-datasets)\n",
    "\n",
    "^[Back to the init of the NN](#Build-the-Neural-Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Logs\n",
    "logger.info('Compiling the neural network for |{}|'.format(data_name))\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n",
    "\n",
    "# Compilation\n",
    "corrector_trainner = Trainner(squared_error_sgd_mom(output_layer, lr=label_rate, mom=0, target_var=target_var), \n",
    "                             'corrector',)\n",
    "\n",
    "if hp_lambda != 0.0:\n",
    "    print('hp_lambda != 0 : Compliling the adversarial part of the networks')\n",
    "    domain_trainner = Trainner(adversarial([src_layer, output_layer], hp_lambda=hp_lambda,\n",
    "                                          lr=domain_rate, mom=domain_mom),\n",
    "                               'domain')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add preprocessing (for alignment)\n",
    "\n",
    "^^[Back to the Loading of datasets](#Load-datasets)\n",
    "\n",
    "^[Back to the init of the NN](#Build-the-Neural-Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from align_learn.preprocess import classwise_shuffle, exhaustive_clostest, cluster_preprocess, build_clusters\n",
    "\n",
    "# Choose preprocessing :\n",
    "#corrector_trainner.preprocess = classwise_shuffle\n",
    "#corrector_trainner.preprocess = exhaustive_clostest\n",
    "corrector_trainner.preprocess = cluster_preprocess\n",
    "\n",
    "model_name = ''\n",
    "if corrector_trainner.preprocess is classwise_shuffle:\n",
    "    model_name = 'Classwise_Corrector'\n",
    "    corrector_data['labels'] = source_data['y_train']\n",
    "elif corrector_trainner.preprocess is exhaustive_clostest:\n",
    "    model_name = 'K-closest_Corrector'\n",
    "    corrector_data['labels'] = source_data['y_train']\n",
    "elif corrector_trainner.preprocess is cluster_preprocess:\n",
    "    model_name = 'Cluster_Corrector'\n",
    "    n_clusters = 6\n",
    "    corrector_data['k'] = -1\n",
    "    y = source_data['y_train']\n",
    "    classes = np.unique(y)\n",
    "\n",
    "    # Build the clusters for target data\n",
    "    centers_array, clusters_label, centers_labels = build_clusters(corrector_data['X_train'],\n",
    "                                                                   y, n_clusters=n_clusters)\n",
    "    corrector_data['X_train_centers'] = centers_array\n",
    "    corrector_data['X_train_clusters'] = clusters_label\n",
    "    corrector_data['centers_labels'] = centers_labels\n",
    "    \n",
    "    # Build the clusters for source data\n",
    "    centers_array, clusters_label, centers_labels = build_clusters(corrector_data['y_train'],\n",
    "                                                                   y, n_clusters=n_clusters)\n",
    "    corrector_data['y_train_centers'] = centers_array\n",
    "    corrector_data['y_train_clusters'] = clusters_label\n",
    "\n",
    "else:\n",
    "    model_name = 'Pairwise_Corrector'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train the neural network\n",
    "\n",
    "^^[Back to the Loading of datasets](#Load-datasets)\n",
    "\n",
    "^[Back to the init of the NN](#Build-the-Neural-Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the counter and the stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.warn('Reset the epoch counter and saved statistics')\n",
    "epoch_counter = 0\n",
    "final_stats = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_n_epoch(n_epoch):\n",
    "    global epoch_counter, logger, final_stats\n",
    "    global corrector_data, domain_data, corrector_trainner, domain_trainner\n",
    "    epoch_counter += n_epoch\n",
    "    logger.info('Trainning the neural network for {} additional epochs ({} total)'.format(n_epoch, epoch_counter))\n",
    "    if hp_lambda != 0.0:\n",
    "        stats = training([corrector_trainner, domain_trainner], [corrector_data, domain_data],\n",
    "                         num_epochs=n_epoch, logger=logger)\n",
    "    else:\n",
    "        stats = training([corrector_trainner,], [corrector_data,],\n",
    "                     num_epochs=n_epoch, logger=logger)\n",
    "\n",
    "    final_stats = {k: (final_stats[k]+v if k in final_stats else v) for k, v in stats.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Plot results\n",
    "\n",
    "^^[Back to the Loading of datasets](#Load-datasets)\n",
    "\n",
    "^[Back to the init of the NN](#Build-the-Neural-Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Learning curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Data plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import visual\n",
    "\n",
    "def my_2D_plot(source_data, target_data, corrector_data, trainer):\n",
    "    \"\"\"\n",
    "    Plot things\n",
    "    \"\"\"\n",
    "    # Compute the correction on test data\n",
    "    corrected_data = {\n",
    "        'X_test': np.array(corrector_trainner.output(corrector_data['X_test'])).reshape((-1, 2)),\n",
    "        'X_train_centers': np.array(\n",
    "            corrector_trainner.output(corrector_data['X_train_centers'])).reshape((-1, 2))\n",
    "    }\n",
    "    # Init figure and axes\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))\n",
    "    \n",
    "    # Plot data test points (source + corrected) on left fig\n",
    "    visual.source_2D(source_data['X_test'], source_data['y_test'], ax=ax1);\n",
    "    visual.corrected_2D(corrected_data['X_test'], source_data['y_test'], ax=ax1);\n",
    "\n",
    "    # Plot data test points (source + target) on right fig\n",
    "    visual.source_2D(source_data['X_test'], source_data['y_test'], ax=ax2);\n",
    "    visual.target_2D(target_data['X_test'], target_data['y_test'], ax=ax2);\n",
    "    \n",
    "    # Plot cluster centers and cluster mapping \n",
    "    if 'preprocess' in corrector_data and 'X_train_centers' in corrector_data:\n",
    "        idx = corrector_data['preprocess']\n",
    "        X = np.array(corrector_trainner.output(corrector_data['X_train_centers'])).reshape((-1, 2))\n",
    "        Y = corrector_data['y_train_centers']\n",
    "        Y = Y[idx]\n",
    "        visual.centers_source(Y, ax=ax1)\n",
    "        visual.centers_corrected(X, ax=ax1)\n",
    "        visual.mapping(X, Y, ax=ax1)\n",
    "        visual.centers_source(Y, ax=ax2)\n",
    "        visual.centers_target(corrector_data['X_train_centers'], ax=ax2)\n",
    "\n",
    "    # Legends\n",
    "    ax1.set_title('Source data vs Corrected data')\n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    ax1.legend(handles, labels, bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "    ax2.set_title('Source data vs Target data')\n",
    "    handles, labels = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(handles, labels, bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(model_name, data_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Play !\n",
    "\n",
    "^^[Back to the Loading of datasets](#Load-datasets)\n",
    "\n",
    "^[Back to the init of the NN](#Build-the-Neural-Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "do_n_epoch(1)\n",
    "my_2D_plot(source_data, target_data, corrector_data, corrector_trainner)\n",
    "visual.learning_curve(final_stats, regex='loss');\n",
    "fig, ax = visual.mat(feature.W.get_value())\n",
    "_ = fig.suptitle('Weights');\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from datasets.transform import _rot_mat\n",
    "visual.mat(_rot_mat(-80));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.unique(corrector_data['X_train_clusters']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
