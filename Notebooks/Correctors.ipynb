{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import sys\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets\n",
    "\n",
    "- the datasets are loaded/built.\n",
    "- The batchsize is defined\n",
    "- half of the data name (the source part) is defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Loading of datasets](#Load-datasets)\n",
    "- [2. Transformation of datasets](#Transform-datasets)\n",
    "- [3. Init of the NN](#Build-the-Neural-Network)\n",
    "- [4. Architecture](#Architecture)\n",
    "- [5. Compiling](#Compiling)\n",
    "- [6. Preprocessing for alignment](#Add-preprocessing-for-alignment)\n",
    "- [7. Playground](#Play-!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets.toys import make_circles\n",
    "from datasets.utils import make_dataset\n",
    "\n",
    "n_samples = 300  # Number of sample per class\n",
    "n_classes = 5\n",
    "n_dim = 2\n",
    "batchsize = 80\n",
    "_data_name = 'Circles'\n",
    "X, y = make_circles(n_samples=n_samples, n_classes=n_classes, n_dim=n_dim)\n",
    "source_data = make_dataset(X, y, batchsize=batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.toys import make_clouds\n",
    "from datasets.utils import make_dataset\n",
    "\n",
    "n_samples = 300  # Number of sample per class\n",
    "n_classes = 8\n",
    "batchsize = 80\n",
    "_data_name = 'Clouds'\n",
    "X, y = make_clouds(n_samples=n_samples, n_classes=n_classes)\n",
    "source_data = make_dataset(X, y, batchsize=batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets.toys import make_X\n",
    "from datasets.utils import make_dataset\n",
    "\n",
    "n_samples = 300  # Number of sample per class\n",
    "n_classes = 5\n",
    "batchsize = 80\n",
    "_data_name = 'X'\n",
    "X, y = make_X(n_samples=n_samples, n_classes=n_classes)\n",
    "source_data = make_dataset(X, y, batchsize=batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets.toys import make_moons\n",
    "from datasets.utils import make_dataset\n",
    "\n",
    "n_samples = 500\n",
    "batchsize = 80\n",
    "_data_name = 'Moons'\n",
    "X, y = make_moons(n_samples=n_samples)\n",
    "source_data = make_dataset(X, y, batchsize=batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.mnist import load_mnist\n",
    "from datasets.utils import make_dataset\n",
    "\n",
    "batchsize = 500\n",
    "_data_name = 'MNIST'\n",
    "X, y = load_mnist()\n",
    "source_data = make_dataset(X, y, batchsize=batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform datasets\n",
    "\n",
    "- the transformed datasets are built.\n",
    "- last part of the data name (the target part) is defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Loading of datasets](#Load-datasets)\n",
    "- [2. Transformation of datasets](#Transform-datasets)\n",
    "- [3. Init of the NN](#Build-the-Neural-Network)\n",
    "- [4. Architecture](#Architecture)\n",
    "- [5. Compiling](#Compiling)\n",
    "- [6. Preprocessing for alignment](#Add-preprocessing-for-alignment)\n",
    "- [7. Playground](#Play-!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets.utils import make_domain_dataset, make_corrector_dataset\n",
    "import datasets.transform as transform\n",
    "\n",
    "data_name = _data_name+'_Rotated'\n",
    "angle = 35\n",
    "\n",
    "X_t, y_t = transform.rotate(X, y)\n",
    "target_data = make_dataset(X_t, y_t, batchsize=batchsize)\n",
    "domain_data = make_domain_dataset([source_data, target_data])\n",
    "corrector_data = make_corrector_dataset(source_data, target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data . Random Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets.utils import make_domain_dataset, make_corrector_dataset\n",
    "import datasets.transform as transform\n",
    "\n",
    "data_name = _data_name+'_RMat'\n",
    "\n",
    "X_t, y_t = transform.random_mat(X, y, normalize=False)\n",
    "target_data = make_dataset(X_t, y_t, batchsize=batchsize)\n",
    "domain_data = make_domain_dataset([source_data, target_data])\n",
    "corrector_data = make_corrector_dataset(source_data, target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data . Diag Dominant matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.utils import make_domain_dataset, make_corrector_dataset\n",
    "import datasets.transform as transform\n",
    "\n",
    "data_name = _data_name+'_Diag'\n",
    "\n",
    "X_t, y_t = transform.diag_dominant(X, y, normalize=True)\n",
    "target_data = make_dataset(X_t, y_t, batchsize=batchsize)\n",
    "domain_data = make_domain_dataset([source_data, target_data])\n",
    "corrector_data = make_corrector_dataset(source_data, target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Mirror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets.utils import make_domain_dataset, make_corrector_dataset\n",
    "import datasets.transform as transform\n",
    "\n",
    "data_name = _data_name+'_Mirror'\n",
    "\n",
    "X_t, y_t = transform.mirror(X, y)\n",
    "target_data = make_dataset(X_t, y_t, batchsize=batchsize)\n",
    "domain_data = make_domain_dataset([source_data, target_data])\n",
    "corrector_data = make_corrector_dataset(source_data, target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Random Permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.utils import make_domain_dataset, make_corrector_dataset\n",
    "import datasets.transform as transform\n",
    "\n",
    "data_name = _data_name+'_Rperm'\n",
    "\n",
    "X_t, y_t = transform.random_permut(X, y)\n",
    "target_data = make_dataset(X_t, y_t, batchsize=batchsize)\n",
    "domain_data = make_domain_dataset([source_data, target_data])\n",
    "corrector_data = make_corrector_dataset(source_data, target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Bend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets.utils import make_domain_dataset, make_corrector_dataset\n",
    "import datasets.transform as transform\n",
    "\n",
    "data_name = _data_name+'_GridBend'\n",
    "nx = 3\n",
    "ny = 3\n",
    "grid_noise = 0.3\n",
    "\n",
    "X_t, y_t, grid = transform.grid_bend(X, y, nx=nx, ny=ny, noise=grid_noise)\n",
    "target_data = make_dataset(X_t, y_t, batchsize=batchsize)\n",
    "domain_data = make_domain_dataset([source_data, target_data])\n",
    "corrector_data = make_corrector_dataset(source_data, target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets.utils import make_domain_dataset, make_corrector_dataset\n",
    "import datasets.transform as transform\n",
    "\n",
    "data_name = _data_name+'_Apply'\n",
    "\n",
    "def _fun(X):\n",
    "    f = np.vectorize((lambda x: x if x<0 else x-x*x))\n",
    "    return f(X)\n",
    "#     return X-X**2\n",
    "#     return 2*X\n",
    "X_t = _fun(X)\n",
    "y_t = np.copy(y)\n",
    "target_data = make_dataset(X_t, y_t, batchsize=batchsize)\n",
    "domain_data = make_domain_dataset([source_data, target_data])\n",
    "corrector_data = make_corrector_dataset(source_data, target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import visual\n",
    "fig, ax = visual.source_2D(X, y)\n",
    "fig, ax = visual.target_2D(X_t, y_t, ax=ax)\n",
    "ax.plot(grid.xxx, grid.yyy, '--');\n",
    "ax.plot(grid.xxx.T, grid.yyy.T, '--');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Loading of datasets](#Load-datasets)\n",
    "- [2. Transformation of datasets](#Transform-datasets)\n",
    "- [3. Init of the NN](#Build-the-Neural-Network)\n",
    "- [4. Architecture](#Architecture)\n",
    "- [5. Compiling](#Compiling)\n",
    "- [6. Preprocessing for alignment](#Add-preprocessing-for-alignment)\n",
    "- [7. Playground](#Play-!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network building\n",
    "Start with the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nn.rgl import ReverseGradientLayer\n",
    "from nn.compilers import crossentropy_sgd_mom, squared_error_sgd_mom, adversarial\n",
    "from nn.training import Trainner, training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "- Learning rates\n",
    "- Hyper params\n",
    "- Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from logs import log_fname, new_logger, empty_logger\n",
    "\n",
    "hp_lambda = 0.\n",
    "\n",
    "label_rate = 0.1\n",
    "label_mom = 0.9\n",
    "\n",
    "domain_rate = 0.1\n",
    "domain_mom = 0.9\n",
    "\n",
    "# Get a logger\n",
    "logger = new_logger()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theano variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_shape = np.shape(source_data['X_train'])\n",
    "n_dim = len(_shape)\n",
    "n_features = np.prod(_shape[1:])\n",
    "\n",
    "# Prepare Theano variables for inputs and targets\n",
    "if n_dim == 2:\n",
    "    input_var = T.matrix('inputs')\n",
    "    src_var = T.matrix('src')\n",
    "    target_var = T.matrix('targets')\n",
    "    shape = (batchsize,) + _shape[1:]\n",
    "elif n_dim == 3:\n",
    "    input_var = T.tensor3('inputs')\n",
    "    src_var = T.tensor3('src')\n",
    "    target_var = T.tensor3('targets')\n",
    "    shape = (batchsize,) + _shape[1:]\n",
    "elif n_dim == 4:\n",
    "    input_var = T.tensor4('inputs')\n",
    "    src_var = T.tensor4('src')\n",
    "    target_var = T.tensor4('targets')\n",
    "    shape = (batchsize,) + _shape[1:]\n",
    "\n",
    "# Logs\n",
    "logger.info('Building the input and output variables for : {}'.format(data_name))\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Loading of datasets](#Load-datasets)\n",
    "- [2. Transformation of datasets](#Transform-datasets)\n",
    "- [3. Init of the NN](#Build-the-Neural-Network)\n",
    "- [4. Architecture](#Architecture)\n",
    "- [5. Compiling](#Compiling)\n",
    "- [6. Preprocessing for alignment](#Add-preprocessing-for-alignment)\n",
    "- [7. Playground](#Play-!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the layers\n",
    "input_layer = lasagne.layers.InputLayer(shape=shape, input_var=input_var)\n",
    "src_layer = lasagne.layers.InputLayer(shape=shape, input_var=src_var)\n",
    "dense_1 = lasagne.layers.DenseLayer(\n",
    "                input_layer,\n",
    "                num_units=5,\n",
    "                nonlinearity=lasagne.nonlinearities.tanh,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n",
    "# dropout_1 = lasagne.layers.DropoutLayer(dense_1, p=0.5)\n",
    "dense_2 = lasagne.layers.DenseLayer(\n",
    "                dense_1,\n",
    "                num_units=5,\n",
    "                nonlinearity=lasagne.nonlinearities.tanh,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n",
    "feature = lasagne.layers.DenseLayer(\n",
    "                input_layer,\n",
    "                num_units=np.prod(shape[1:]),  # Should have same number as the input dimension\n",
    "                nonlinearity=None,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n",
    "reshaper = lasagne.layers.ReshapeLayer(feature, (-1,) + shape[1:])\n",
    "output_layer = reshaper\n",
    "\n",
    "# Logs\n",
    "logger.info('Building the neural network architecture for : {}'.format(data_name))\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Loading of datasets](#Load-datasets)\n",
    "- [2. Transformation of datasets](#Transform-datasets)\n",
    "- [3. Init of the NN](#Build-the-Neural-Network)\n",
    "- [4. Architecture](#Architecture)\n",
    "- [5. Compiling](#Compiling)\n",
    "- [6. Preprocessing for alignment](#Add-preprocessing-for-alignment)\n",
    "- [7. Playground](#Play-!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Logs\n",
    "logger.info('Compiling the neural network for : {}'.format(data_name))\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n",
    "\n",
    "# Compilation\n",
    "corrector_trainner = Trainner(squared_error_sgd_mom(output_layer, lr=label_rate, mom=0, \n",
    "                                                    target_var=target_var,\n",
    "                                                   regularization=None, reg_param=0.1), \n",
    "                             'corrector',)\n",
    "\n",
    "domain_trainner = Trainner(adversarial([src_layer, output_layer], hp_lambda=hp_lambda,\n",
    "                                      lr=domain_rate, mom=domain_mom),\n",
    "                           'domain')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add preprocessing for alignment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Loading of datasets](#Load-datasets)\n",
    "- [2. Transformation of datasets](#Transform-datasets)\n",
    "- [3. Init of the NN](#Build-the-Neural-Network)\n",
    "- [4. Architecture](#Architecture)\n",
    "- [5. Compiling](#Compiling)\n",
    "- [6. Preprocessing for alignment](#Add-preprocessing-for-alignment)\n",
    "- [7. Playground](#Play-!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from align_learn.preprocess import classwise_shuffle, exhaustive_clostest, cluster_preprocess, build_clusters\n",
    "\n",
    "# Choose preprocessing :\n",
    "# corrector_trainner.preprocess = classwise_shuffle\n",
    "# corrector_trainner.preprocess = exhaustive_clostest\n",
    "corrector_trainner.preprocess = cluster_preprocess\n",
    "\n",
    "model_name = ''\n",
    "if corrector_trainner.preprocess is classwise_shuffle:\n",
    "    model_name = 'Classwise_Corrector'\n",
    "    corrector_data['labels'] = source_data['y_train']\n",
    "elif corrector_trainner.preprocess is exhaustive_clostest:\n",
    "    model_name = 'K-closest_Corrector'\n",
    "    corrector_data['labels'] = source_data['y_train']\n",
    "elif corrector_trainner.preprocess is cluster_preprocess:\n",
    "    model_name = 'Cluster_Corrector'\n",
    "    n_clusters = 6\n",
    "    corrector_data['k'] = -1\n",
    "    y = source_data['y_train']\n",
    "    classes = np.unique(y)\n",
    "\n",
    "    # Build the clusters for target data\n",
    "    centers_array, clusters_label, centers_labels = build_clusters(corrector_data['X_train'],\n",
    "                                                                   y, n_clusters=n_clusters)\n",
    "    corrector_data['X_train_centers'] = centers_array\n",
    "    corrector_data['X_train_clusters'] = clusters_label\n",
    "    corrector_data['centers_labels'] = centers_labels\n",
    "    \n",
    "    # Build the clusters for source data\n",
    "    centers_array, clusters_label, centers_labels = build_clusters(corrector_data['y_train'],\n",
    "                                                                   y, n_clusters=n_clusters)\n",
    "    corrector_data['y_train_centers'] = centers_array\n",
    "    corrector_data['y_train_clusters'] = clusters_label\n",
    "\n",
    "else:\n",
    "    model_name = 'Pairwise_Corrector'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train the neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Loading of datasets](#Load-datasets)\n",
    "- [2. Transformation of datasets](#Transform-datasets)\n",
    "- [3. Init of the NN](#Build-the-Neural-Network)\n",
    "- [4. Architecture](#Architecture)\n",
    "- [5. Compiling](#Compiling)\n",
    "- [6. Preprocessing for alignment](#Add-preprocessing-for-alignment)\n",
    "- [7. Playground](#Play-!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the counter and the stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.warn('Reset the epoch counter and saved statistics')\n",
    "epoch_counter = 0\n",
    "final_stats = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_n_epoch(n_epoch):\n",
    "    global epoch_counter, logger, final_stats, hp_lambda\n",
    "    global corrector_data, domain_data, corrector_trainner, domain_trainner\n",
    "    epoch_counter += n_epoch\n",
    "    trainers = [corrector_trainner,]\n",
    "    datas = [corrector_data,]\n",
    "    #  If hp_lambda == 0 no need to train adversarial (faster computation)\n",
    "    if hp_lambda != 0.0:\n",
    "        trainers.append(domain_trainner)\n",
    "        datas.append(domain_data)\n",
    "    # Now do the trainning part !\n",
    "    logger.info('Trainning the neural network for {} additional epochs ({} total)'.format(n_epoch, epoch_counter))\n",
    "    stats = training(trainers, datas, num_epochs=n_epoch, logger=None)\n",
    "    final_stats = {k: (final_stats[k]+v if k in final_stats else v) for k, v in stats.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Plot results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Loading of datasets](#Load-datasets)\n",
    "- [2. Transformation of datasets](#Transform-datasets)\n",
    "- [3. Init of the NN](#Build-the-Neural-Network)\n",
    "- [4. Architecture](#Architecture)\n",
    "- [5. Compiling](#Compiling)\n",
    "- [6. Preprocessing for alignment](#Add-preprocessing-for-alignment)\n",
    "- [7. Playground](#Play-!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Data plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import visual\n",
    "\n",
    "def my_2D_plot(source_data, target_data, corrector_data, trainer):\n",
    "    \"\"\"\n",
    "    Plot things\n",
    "    \"\"\"\n",
    "    # Compute the correction on test data\n",
    "    corrected_data = {\n",
    "        'X_test': np.array(corrector_trainner.output(corrector_data['X_test'])).reshape((-1, 2)),\n",
    "    }\n",
    "    # Init figure and axes\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))\n",
    "    \n",
    "    # Plot data test points (source + corrected) on left fig\n",
    "    visual.source_2D(source_data['X_test'], source_data['y_test'], ax=ax1);\n",
    "    visual.corrected_2D(corrected_data['X_test'], source_data['y_test'], ax=ax1);\n",
    "\n",
    "    # Plot data test points (source + target) on right fig\n",
    "    visual.source_2D(source_data['X_test'], source_data['y_test'], ax=ax2);\n",
    "    visual.target_2D(target_data['X_test'], target_data['y_test'], ax=ax2);\n",
    "    \n",
    "    # Plot cluster centers and cluster mapping \n",
    "#     if 'preprocess' in corrector_data and 'X_train_centers' in corrector_data:\n",
    "#         corrected_data['X_train_centers'] = np.array(\n",
    "#             corrector_trainner.output(corrector_data['X_train_centers'])).reshape((-1, 2))\n",
    "#         idx = corrector_data['preprocess']\n",
    "#         X = np.array(corrector_trainner.output(corrector_data['X_train_centers'])).reshape((-1, 2))\n",
    "#         Y = corrector_data['y_train_centers']\n",
    "#         Y = Y[idx]\n",
    "#         visual.centers_source(Y, ax=ax1)\n",
    "#         visual.centers_corrected(X, ax=ax1)\n",
    "#         visual.mapping(X, Y, ax=ax1)\n",
    "#         visual.centers_source(Y, ax=ax2)\n",
    "#         visual.centers_target(corrector_data['X_train_centers'], ax=ax2)\n",
    "    return fig, [ax1, ax2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def grid_view(X, y, trainer):\n",
    "    \"\"\"\n",
    "    Plot the corrected grid\n",
    "    \"\"\"\n",
    "    nx = ny = 20\n",
    "    x_min, x_max = np.min(X[:, 0])*2, np.max(X[:, 0])*2\n",
    "    y_min, y_max = np.min(X[:, 1])*2, np.max(X[:, 1])*2\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, num=nx),\n",
    "                         np.linspace(y_min, y_max, num=ny))\n",
    "    X_grid = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "    y_grid = np.hstack([np.ones(nx)*i for i in range(ny)])\n",
    "    \n",
    "#     Transform:\n",
    "    X_tgt, y_tgt = transform.rotate(X_grid, y_grid, angle=angle)\n",
    "    \n",
    "    X_corr = np.array(trainer.output(X_tgt)).reshape((-1, 2))\n",
    "    xx = X_corr[:, 0].reshape(ny, nx)\n",
    "    yy = X_corr[:, 1].reshape(ny, nx)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "    # Plot data test points (source + corrected) on left fig\n",
    "    visual.corrected_2D(X, y, ax=ax1)\n",
    "    ax1.plot(xx, yy)\n",
    "    ax1.plot(xx.T, yy.T)\n",
    "\n",
    "    xx = X_tgt[:, 0].reshape(ny, nx)\n",
    "    yy = X_tgt[:, 1].reshape(ny, nx)\n",
    "    visual.corrected_2D(X, y, ax=ax2)\n",
    "    ax2.plot(xx, yy)\n",
    "    ax2.plot(xx.T, yy.T)\n",
    "    return fig, (ax1, ax2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig_dir = '/home/victor/Workspace/Stage/DANN/fig/'\n",
    "fig_title = fig_dir+data_name+model_name\n",
    "from datasets.utils import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Play !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Loading of datasets](#Load-datasets)\n",
    "- [2. Transformation of datasets](#Transform-datasets)\n",
    "- [3. Init of the NN](#Build-the-Neural-Network)\n",
    "- [4. Architecture](#Architecture)\n",
    "- [5. Compiling](#Compiling)\n",
    "- [6. Preprocessing for alignment](#Add-preprocessing-for-alignment)\n",
    "- [7. Playground](#Play-!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(model_name, data_name)\n",
    "do_n_epoch(0)\n",
    "\n",
    "# ================\n",
    "# Compute the correction on test data\n",
    "# ================\n",
    "corrected_data = Dataset(\n",
    "    X_test=np.array(corrector_trainner.output(corrector_data['X_test'])).reshape((-1, 2)),\n",
    "    y_test=source_data.y_test\n",
    ")\n",
    "\n",
    "\n",
    "# ================\n",
    "# Data visualisation\n",
    "# ================\n",
    "if not n_dim > 2:\n",
    "    # Init figure and axes\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "    # Plot data test points (source + corrected) on left fig\n",
    "    visual.source_2D(source_data.X_test, source_data.y_test, ax=ax1);\n",
    "    visual.corrected_2D(corrected_data.X_test, source_data.y_test, ax=ax1);\n",
    "    visual.add_legend(ax1, title=data_name+model_name)\n",
    "\n",
    "    # Plot data test points (source + target) on right fig\n",
    "    visual.source_2D(source_data.X_test, source_data.y_test, ax=ax2);\n",
    "    visual.target_2D(target_data.X_test, target_data.y_test, ax=ax2);\n",
    "    visual.add_legend(ax2, title=data_name+model_name)\n",
    "    # Plot cluster centers and cluster mapping \n",
    "#     if 'preprocess' in corrector_data and 'X_train_centers' in corrector_data:\n",
    "#         corrected_data['X_train_centers'] = np.array(\n",
    "#             corrector_trainner.output(corrector_data['X_train_centers'])).reshape((-1, 2))\n",
    "#         idx = corrector_data['preprocess']\n",
    "#         X = np.array(corrector_trainner.output(corrector_data['X_train_centers'])).reshape((-1, 2))\n",
    "#         Y = corrector_data['y_train_centers']\n",
    "#         Y = Y[idx]\n",
    "#         visual.centers_source(Y, ax=ax1)\n",
    "#         visual.centers_corrected(X, ax=ax1)\n",
    "#         visual.mapping(X, Y, ax=ax1)\n",
    "#         visual.centers_source(Y, ax=ax2)\n",
    "#         visual.centers_target(corrector_data['X_train_centers'], ax=ax2)\n",
    "#     SAVE\n",
    "    fig.savefig(fig_title+'-DATA.png')\n",
    "    fig.show()\n",
    "\n",
    "# ================\n",
    "# Learning curve\n",
    "# ================\n",
    "fig, ax = visual.learning_curve(final_stats, regex='corrector .* loss')\n",
    "#     SAVE\n",
    "fig.tight_layout()\n",
    "fig.savefig(fig_title+'-Learning_curve.png',bbox_inches='tight')\n",
    "visual.learning_curve(final_stats, regex='domain.* acc');\n",
    "\n",
    "# ================\n",
    "# Grid check\n",
    "# ================\n",
    "fig, (ax1, ax2) = grid_view(corrected_data.X_test, corrected_data.y_test, corrector_trainner)\n",
    "fig.savefig(fig_title+'-GridCheck.png')\n",
    "\n",
    "# ================\n",
    "# Image samples\n",
    "# ================\n",
    "if data_name.startswith('MNIST'):\n",
    "    visual.img_samples([source_data, target_data, \n",
    "                     {'X_test': np.array(corrector_trainner.output(\n",
    "                    corrector_data['X_test'])).reshape((-1,)+corrector_data['X_test'].shape[1:])}])\n",
    "\n",
    "# ================\n",
    "# Weights visualisation\n",
    "# ================\n",
    "layers = lasagne.layers.get_all_layers(feature)\n",
    "fig, axes = plt.subplots(len(layers)//2, 2, figsize=(20, 6))\n",
    "axes = axes.ravel()\n",
    "for i, l in enumerate(layers[1:]):\n",
    "    if hasattr(l, 'W'):\n",
    "        visual.mat(l.W.get_value(), ax = axes[i])\n",
    "        visual.add_legend(axes[i], title='Layer {} Weights'.format(i))\n",
    "#     SAVE\n",
    "fig.savefig(fig_title+'-W.png')\n",
    "\n",
    "# ================\n",
    "# Sanity check\n",
    "# ================\n",
    "print('Loss : Identitée')\n",
    "print(np.mean((source_data['X_test']-target_data['X_test'])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'Done'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "visual.img_samples([source_data, target_data, \n",
    "                 {'X_test': np.array(corrector_trainner.output(\n",
    "                corrector_data['X_test'])).reshape((-1,)+corrector_data['X_test'].shape[1:])}]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSNE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "fig, ax = visual.tsne(source_data['X_test'], target_data['X_test'], source_data['y_test'])\n",
    "visual.add_legend(ax, title='Source+target')\n",
    "ax.set_xlim(-150, 150)\n",
    "ax.set_ylim(-150, 150)\n",
    "\n",
    "fig, ax = visual.tsne(source_data['X_test'], \n",
    "            np.array(corrector_trainner.output(corrector_data['X_test'])).reshape((-1, 2)),\n",
    "            source_data['y_test'])\n",
    "visual.add_legend(ax, title='Source+Corrected')\n",
    "ax.set_xlim(-150, 150)\n",
    "ax.set_ylim(-150, 150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
