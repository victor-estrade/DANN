{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 30 days\n",
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 30 days\n",
      "Using gpu device 0: GeForce GT 750M\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "from __future__ import division, print_function\n",
    "\n",
    "import sys, os\n",
    "sys.path.append('..')\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from logs import log_fname, new_logger\n",
    "from nn.rgl import ReverseGradientLayer\n",
    "from nn.block import Dense, Classifier, adversarial\n",
    "from nn.compilers import crossentropy_sgd_mom, squared_error_sgd_mom\n",
    "from nn.training import Trainner, training\n",
    "\n",
    "from utils import plot_bound, save_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets\n",
    "\n",
    "Here the datasets are loaded/built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to make the *corrector* dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.toys import load_clouds\n",
    "n_samples = 30  # Number of sample per class\n",
    "n_classes = 10\n",
    "batchsize = 80\n",
    "\n",
    "source_data = load_clouds(n_samples=n_samples, n_classes=n_classes, batchsize=batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clouds rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets.utils import make_domain_dataset, make_corrector_dataset\n",
    "from datasets.transform import rotate_dataset\n",
    "\n",
    "data_name = 'Clouds_Rotated'\n",
    "angle = 80\n",
    "\n",
    "target_data = rotate_dataset(source_data)\n",
    "domain_data = make_domain_dataset([source_data, target_data])\n",
    "corrector_data = make_corrector_dataset(source_data, target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clouds . Random Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets.utils import make_domain_dataset\n",
    "from datasets.transform import random_mat_dataset\n",
    "\n",
    "data_name = 'Clouds_RMat'\n",
    "\n",
    "target_data = random_mat_dataset(source_data)\n",
    "domain_data = make_domain_dataset([source_data, target_data])\n",
    "corrector_data = make_corrector_dataset(source_data, target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.toys import load_moons\n",
    "n_samples = 800\n",
    "batchsize = 80\n",
    "\n",
    "source_data = load_moons(n_samples=n_samples, batchsize=batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moon rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets.utils import make_domain_dataset\n",
    "from datasets.transform import rotate_dataset\n",
    "\n",
    "data_name = 'Moon_Rotated'\n",
    "angle = 30.\n",
    "\n",
    "target_data = rotate_dataset(source_data)\n",
    "domain_data = make_domain_dataset([source_data, target_data])\n",
    "corrector_data = make_corrector_dataset(source_data, target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moon . Random Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.utils import make_domain_dataset\n",
    "from datasets.transform import random_mat_dataset\n",
    "\n",
    "data_name = 'Moon_RMat'\n",
    "\n",
    "target_data = random_mat_dataset(source_data)\n",
    "domain_data = make_domain_dataset([source_data, target_data])\n",
    "corrector_data = make_corrector_dataset(source_data, target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.mnist import load_mnist\n",
    "batchsize = 500\n",
    "source_data = load_mnist(batchsize=batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST . Diag Dominant matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.transform import diag_dataset\n",
    "\n",
    "data_name = 'MNIST_Diag'\n",
    "\n",
    "target_data = diag_dataset(source_data, normalize=True)\n",
    "domain_data = make_domain_dataset([source_data, target_data])\n",
    "corrector_data = make_corrector_dataset(source_data, target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Mirror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets.transform import mirror_dataset\n",
    "\n",
    "data_name = 'MNIST_Mirror'\n",
    "\n",
    "target_data = mirror_dataset(source_data)\n",
    "domain_data = make_domain_dataset([source_data, target_data])\n",
    "corrector_data = make_corrector_dataset(source_data, target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST . Random Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets.transform import random_mat_dataset\n",
    "\n",
    "data_name = 'MNIST_Rmat'\n",
    "\n",
    "target_data = random_mat_dataset(source_data)\n",
    "domain_data = make_domain_dataset([source_data, target_data])\n",
    "corrector_data = make_corrector_dataset(source_data, target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epoch Preprocessing\n",
    "\n",
    "The preprocessing function that will run at the begining of each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/questions/25886374/pdist-for-theano-tensor\n",
    "# Tested and approved\n",
    "X = T.fmatrix('X')\n",
    "Y = T.fmatrix('Y')\n",
    "translation_vectors = X.reshape((X.shape[0], 1, -1)) - Y.reshape((1, Y.shape[0], -1))\n",
    "euclidiean_distances = (translation_vectors ** 2).sum(2)\n",
    "f_euclidean = theano.function([X, Y], euclidiean_distances, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kclosest(X, Y, k, batchsize=None):\n",
    "    \"\"\"\n",
    "    Computes for each sample from X the k-closest samples in Y and return \n",
    "    their index.\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "        X: (numpy array [n_sample, n_feature])\n",
    "        Y: (numpy array [n_sample, n_feature])\n",
    "        k: (int)\n",
    "    Return\n",
    "    ------\n",
    "        kclosest : (numpy array [n_sample, k]) the ordered index of \n",
    "            the k-closest instances from Y to X samples\n",
    "    \"\"\"\n",
    "    assert X.shape == Y.shape\n",
    "    N = X.shape[0]\n",
    "    if batchsize is None:\n",
    "        dist = f_euclidean(X.reshape(N, -1), Y.reshape(N, -1))\n",
    "    else:\n",
    "        dist = np.empty((N, N), dtype=theano.config.floatX)\n",
    "        batch = np.arange(0, N+batchsize, batchsize)\n",
    "        for excerpt_X in (slice(i0, i1) for i0, i1 in zip(batch[:-1], batch[1:])):\n",
    "            dist[excerpt_X] = f_euclidean(X[excerpt_X], Y)\n",
    "    kbest = np.argsort(dist, axis=1)[:, :k]\n",
    "    return kbest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def realign(X_out, X_trg, y, k=20, batchsize=None):\n",
    "    counter = np.zeros(X_out.shape[0], dtype=int)\n",
    "    idx = np.empty_like(y, dtype=int)\n",
    "    for label in np.unique(y):\n",
    "        # Get the examples of the right label\n",
    "        idx_label = np.where(y==label)[0]\n",
    "\n",
    "        # Get the k-closest index from the small part with the same labels\n",
    "        idx_label2 = kclosest(X_out[idx_label], X_trg[idx_label], k, batchsize=batchsize)\n",
    "        \n",
    "        for i1, i2 in zip(idx_label, idx_label2):\n",
    "            # i2 is an index array of shape (k,) with the sorted closest example index \n",
    "            # (of the sorted single class array)\n",
    "            # Then idx_label[i2] are the sorted original index of the k-closest examples\n",
    "            i = idx_label[i2[np.argmin(counter[idx_label[i2]])]]\n",
    "            # i contains the chosen one, in the (k-)clostest example, with the minimum counter\n",
    "            counter[i] = counter[i]+1\n",
    "            idx[i1] = i\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchpad(batchsize, output_shape, dtype=None):\n",
    "    \"\"\"Re-batching decorator\n",
    "    \"\"\"\n",
    "    def decoreted(func):\n",
    "        def wrapper(X, *args, **kwargs):\n",
    "            if dtype is None:\n",
    "                dtype2 = X.dtype\n",
    "            else:\n",
    "                dtype2 = dtype\n",
    "            \n",
    "            N = X.shape[0]\n",
    "            \n",
    "            if output_shape is None:\n",
    "                shape = X.shape\n",
    "            else:\n",
    "                shape = tuple( out_s if out_s is not None else X_s \n",
    "                              for out_s, X_s in zip(output_shape, X.shape))\n",
    "\n",
    "            result = np.empty(shape, dtype=dtype2)\n",
    "            batch = np.arange(0, N, batchsize)\n",
    "            for excerpt_X in (slice(i0, i1) for i0, i1 in zip(batch[:-1], batch[1:])):\n",
    "                result[excerpt_X] = func(X[excerpt_X], *args, **kwargs)\n",
    "            \n",
    "            last_excerpt = slice(batch[-1], N)\n",
    "            X = X[last_excerpt]\n",
    "            n_sample = X.shape[0]\n",
    "            X = np.vstack([X, np.zeros((batchsize-X.shape[0],)+X.shape[1:])])\n",
    "            X = func(X, *args, **kwargs)\n",
    "            result[last_excerpt] = X[:n_sample]\n",
    "            \n",
    "            return result\n",
    "        return wrapper\n",
    "    return decoreted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Méthode bourin. K-clostest on every data point\n",
    "def exhaustive_clostest(data, trainer, epoch, *args, **kwargs):\n",
    "    X = data['X_train']\n",
    "    k = data['k'] if 'k' in data else 5\n",
    "\n",
    "    @batchpad(data['batchsize'], X.shape, X.dtype)\n",
    "    def f_output(X, trainer):\n",
    "        return trainer.output(X)[0]\n",
    "    \n",
    "    X_out = f_output(X, trainer)\n",
    "    X_trg = data['y_train']\n",
    "    data['X_train'] = X[realign(X_out, X_trg, data['labels'], k=k, batchsize=None)]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classwise_shuffle(X, y):\n",
    "    \"\"\"\n",
    "    Shuffle X without changing the class positions\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "        X: the data (numpy array)\n",
    "        y: the labels \n",
    "    Return\n",
    "    ------\n",
    "        X_shuffled: Shuffled X without changing the class matching\n",
    "    \"\"\"\n",
    "    idx = np.empty_like(y, dtype=int)\n",
    "    for label in np.unique(y):\n",
    "        arr = np.where(y==label)[0]\n",
    "        arr2 = np.random.permutation(arr)\n",
    "        idx[arr] = arr2\n",
    "    return X[idx]\n",
    "\n",
    "\n",
    "def epoch_shuffle(data, trainer, epoch, *args, **kwargs):\n",
    "    data['X_train'] = classwise_shuffle(data['X_train'], data['labels'])\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster_preprocess(data, trainer, epoch, *args, **kwargs):\n",
    "    k = data['k'] if 'k' in data else 5\n",
    "    \n",
    "    @batchpad(data['batchsize'], None, X.dtype)\n",
    "    def f_output(X, trainer):\n",
    "        return trainer.output(X)[0]\n",
    "    \n",
    "    idx = realign(f_output(data['X_train_centers'], trainer), data['y_train_centers'], \n",
    "                  data['centers_labels'], k=k, batchsize=None)\n",
    "    # idx takes the indexes to relabel the closest clusters\n",
    "    # [2, 0, 1, 3]  means  0<-2, 1<-0, 2<-1, 3<-3\n",
    "    data['X_train_closest_cluster'] = idx[data['X_train_clusters'][:]]\n",
    "    # Now data['X_train_closest_cluster'] contains the nearest clusters label\n",
    "    # from each data in X_train to the clusters of y_train\n",
    "    \n",
    "    # Do a random realign from here\n",
    "    data['X_train'] = classwise_shuffle(data['X_train'], data['X_train_closest_cluster'])\n",
    "    \n",
    "    # Or a fully k-closest realignment\n",
    "    #idx = realign(f_output(data['X_train'], trainer), data['y_train'],\n",
    "    #                            data['X_train_closest_cluster'], k=k, batchsize=None)\n",
    "    #data['X_train'] = data['X_train'][idx]\n",
    "    data['preprocess'] = idx\n",
    "    return idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network building\n",
    "Start with the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hp_lambda = 0.\n",
    "\n",
    "label_rate = 1\n",
    "label_mom = 0.9\n",
    "\n",
    "domain_rate = 1\n",
    "domain_mom = 0.9\n",
    "\n",
    "# Get a logger\n",
    "logger = new_logger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12:18:32] INFO    : Building the input and output variables for |Clouds_Rotated|\n",
      "[12:18:32] INFO    : Input data expected shape : (80, 2)\n"
     ]
    }
   ],
   "source": [
    "# Prepare Theano variables for inputs and targets\n",
    "if data_name.startswith('MNIST'):\n",
    "    input_var = T.tensor3('inputs')\n",
    "    src_var = T.tensor3('src')\n",
    "    target_var = T.tensor3('targets')\n",
    "    shape = (batchsize, 28, 28)\n",
    "elif data_name.startswith('Moon') or data_name.startswith('Clouds'):\n",
    "    input_var = T.matrix('inputs')\n",
    "    src_var = T.matrix('src')\n",
    "    target_var = T.matrix('targets')\n",
    "    shape = (batchsize, 2)\n",
    "\n",
    "# Logs\n",
    "logger.info('Building the input and output variables for |{}|'.format(data_name))\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/anaconda/lib/python2.7/site-packages/Lasagne-0.1.dev0-py2.7.egg/lasagne/init.py:86: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.\n",
      "  warnings.warn(\"The uniform initializer no longer uses Glorot et al.'s \"\n",
      "[12:18:34] INFO    : Building the neural network architecture for |Clouds_Rotated|\n",
      "[12:18:34] INFO    : Input data expected shape : (80, 2)\n"
     ]
    }
   ],
   "source": [
    "# Build the layers\n",
    "input_layer = lasagne.layers.InputLayer(shape=shape, input_var=input_var)\n",
    "src_layer = lasagne.layers.InputLayer(shape=shape, input_var=src_var)\n",
    "# feature = lasagne.layers.DenseLayer(\n",
    "#                 input_layer,\n",
    "#                 num_units=np.prod(shape[1:]),\n",
    "#                 nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#                 # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "#                 )\n",
    "feature = lasagne.layers.DenseLayer(\n",
    "                input_layer,\n",
    "                num_units=np.prod(shape[1:]),\n",
    "                nonlinearity=None,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n",
    "reshaper = lasagne.layers.ReshapeLayer(feature, (-1,) + shape[1:])\n",
    "output_layer = reshaper\n",
    "\n",
    "# Logs\n",
    "logger.info('Building the neural network architecture for |{}|'.format(data_name))\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12:18:40] INFO    : Compiling the neural network for |Clouds_Rotated|\n",
      "[12:18:40] INFO    : Input data expected shape : (80, 2)\n",
      "/home/victor/anaconda/lib/python2.7/site-packages/Lasagne-0.1.dev0-py2.7.egg/lasagne/layers/helper.py:69: UserWarning: get_all_layers() has been changed to return layers in topological order. The former implementation is still available as get_all_layers_old(), but will be removed before the first release of Lasagne. To ignore this warning, use `warnings.filterwarnings('ignore', '.*topo.*')`.\n",
      "  warnings.warn(\"get_all_layers() has been changed to return layers in \"\n"
     ]
    }
   ],
   "source": [
    "# Logs\n",
    "logger.info('Compiling the neural network for |{}|'.format(data_name))\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n",
    "\n",
    "# Compilation\n",
    "corrector_trainner = Trainner(squared_error_sgd_mom(output_layer, lr=label_rate, mom=0, target_var=target_var), \n",
    "                             'corrector',)\n",
    "\n",
    "if hp_lambda != 0.0:\n",
    "    print('hp_lambda != 0 : Compliling the adversarial part of the networks')\n",
    "    domain_trainner = Trainner(adversarial([src_layer, output_layer], hp_lambda=hp_lambda,\n",
    "                                          lr=domain_rate, mom=domain_mom),\n",
    "                               'domain')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add preprocessing (for alignment)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Choose preprocessing :\n",
    "#corrector_trainner.preprocess = epoch_shuffle\n",
    "#corrector_trainner.preprocess = exhaustive_clostest\n",
    "corrector_trainner.preprocess = cluster_preprocess\n",
    "\n",
    "model_name = ''\n",
    "if corrector_trainner.preprocess is epoch_shuffle:\n",
    "    model_name = 'Classwise_Corrector'\n",
    "elif corrector_trainner.preprocess is exhaustive_clostest:\n",
    "    model_name = 'Exhaustive-closest_Corrector'\n",
    "elif corrector_trainner.preprocess is cluster_preprocess:\n",
    "    model_name = 'Cluster_Corrector'\n",
    "    \n",
    "    n_clusters = 12\n",
    "    km = KMeans(n_clusters=n_clusters)\n",
    "    km.fit(corrector_data['X_train'].reshape((corrector_data['X_train'].shape[0], -1)))\n",
    "    corrector_data['X_train_centers'] = km.cluster_centers_.reshape((n_clusters,)+shape[1:])\n",
    "    corrector_data['X_train_clusters'] = km.labels_\n",
    "    corrector_data['k'] = 12\n",
    "    km = KMeans(n_clusters=n_clusters)\n",
    "    km.fit(corrector_data['y_train'].reshape((corrector_data['y_train'].shape[0], -1)))\n",
    "    corrector_data['y_train_centers'] = km.cluster_centers_.reshape((n_clusters,)+shape[1:])\n",
    "    corrector_data['y_train_clusters'] = km.labels_\n",
    "    #corrector_data['labels'] = np.zeros(n_clusters)\n",
    "    corrector_data['centers_labels'] = np.zeros(n_clusters)\n",
    "else:\n",
    "    model_name = 'Pairwise_Corrector'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-ec8a282739cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mn_clusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrector_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mcorrector_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'k'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msource_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y_train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Choose preprocessing :\n",
    "#corrector_trainner.preprocess = epoch_shuffle\n",
    "#corrector_trainner.preprocess = exhaustive_clostest\n",
    "corrector_trainner.preprocess = cluster_preprocess\n",
    "\n",
    "model_name = ''\n",
    "if corrector_trainner.preprocess is epoch_shuffle:\n",
    "    model_name = 'Classwise_Corrector'\n",
    "    corrector_data['labels'] = source_data['y_train']\n",
    "elif corrector_trainner.preprocess is exhaustive_clostest:\n",
    "    model_name = 'K-closest_Corrector'\n",
    "    corrector_data['labels'] = source_data['y_train']\n",
    "elif corrector_trainner.preprocess is cluster_preprocess:\n",
    "    model_name = 'Cluster_Corrector'\n",
    "    n_clusters = 10\n",
    "    print(corrector_data)\n",
    "    corrector_data['k'] = 10\n",
    "    y = source_data['y_train']\n",
    "    classes = np.unique(y)\n",
    "\n",
    "    centers = []\n",
    "    clusters_label = np.empty(corrector_data['X_train'].shape[0], dtype=int)\n",
    "    labels = []\n",
    "    for label in classes:\n",
    "        km = KMeans(n_clusters=n_clusters)\n",
    "        y = source_data['y_train']\n",
    "        idx = np.where(y==label)[0]\n",
    "        X = corrector_data['X_train'][idx]\n",
    "        km.fit(X.reshape((X.shape[0], -1)))\n",
    "        centers.append(km.cluster_centers_.reshape((n_clusters,)+shape[1:]))\n",
    "        clusters_label[idx] = km.labels_+label*n_clusters\n",
    "        labels.append(np.ones(n_clusters, dtype=int)*label)\n",
    "    \n",
    "    corrector_data['X_train_centers'] = np.vstack(centers)\n",
    "    corrector_data['X_train_clusters'] = clusters_label\n",
    "    corrector_data['centers_labels'] = np.hstack(labels)\n",
    "    \n",
    "    \n",
    "    centers = []\n",
    "    labels = []\n",
    "    clusters_label = np.empty(corrector_data['y_train'].shape[0], dtype=int)\n",
    "    for label in classes:\n",
    "        km = KMeans(n_clusters=n_clusters)\n",
    "        y = target_data['y_train']\n",
    "        idx = np.where(y==label)[0]\n",
    "        X = corrector_data['y_train'][idx]\n",
    "        km.fit(X.reshape((X.shape[0], -1)))\n",
    "        centers.append(km.cluster_centers_.reshape((n_clusters,)+shape[1:]))\n",
    "        clusters_label[idx] = km.labels_+label*n_clusters\n",
    "    corrector_data['y_train_centers'] = np.vstack(centers)\n",
    "    corrector_data['y_train_clusters'] = clusters_label\n",
    "\n",
    "else:\n",
    "    model_name = 'Pairwise_Corrector'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the counter and the stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.warn('Reset the epoch counter and saved statistics')\n",
    "epoch_counter = 0\n",
    "final_stats = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_n_epoch(n_epoch):\n",
    "    global epoch_counter, logger, final_stats\n",
    "    global corrector_data, domain_data, corrector_trainner, domain_trainner\n",
    "    epoch_counter += n_epoch\n",
    "    logger.info('Trainning the neural network for {} additional epochs ({} total)'.format(n_epoch, epoch_counter))\n",
    "    if hp_lambda != 0.0:\n",
    "        stats = training([corrector_trainner, domain_trainner], [corrector_data, domain_data],\n",
    "                         num_epochs=n_epoch, logger=logger)\n",
    "    else:\n",
    "        stats = training([corrector_trainner,], [corrector_data,],\n",
    "                     num_epochs=n_epoch, logger=logger)\n",
    "\n",
    "    final_stats = {k: (final_stats[k]+v if k in final_stats else v) for k, v in stats.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Learning curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve():\n",
    "    global final_stats, data_name, model_name, hp_lambda\n",
    "    title = '++'.join([data_name, model_name, 'lambda={:.3e}'.format(hp_lambda)])\n",
    "    # Plot learning accuracy curve\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(final_stats['corrector valid loss'], label='source')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.set_title(title)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, labels, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    #fig.savefig('fig/'+title+'.png', bbox_inches='tight')\n",
    "    #fig.clf() # Clear plot window\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Data plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_2D_data(save=False):\n",
    "    global data_name, model_name, hp_lambda\n",
    "    title = '++'.join([data_name, model_name, 'lambda={:.3e}'.format(hp_lambda)])\n",
    "    global source_data, target_data, corrector_data, corrector_trainner\n",
    "    \n",
    "    if data_name.startswith('MNIST'):\n",
    "        return\n",
    "    \n",
    "    from matplotlib.colors import ListedColormap\n",
    "    import matplotlib.cm as cm\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    if data_name.startswith('Moon'):\n",
    "        color = cm.ScalarMappable(cmap=cm_bright)\n",
    "    else:\n",
    "        color = cm.ScalarMappable(cmap='Paired')\n",
    "\n",
    "    if data_name.startswith('Moon') or data_name.startswith('Clouds'):\n",
    "        # Plot the test data\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))\n",
    "        X = source_data['X_test']\n",
    "        y = source_data['y_test']\n",
    "        ax1.scatter(X[:, 0], X[:, 1], label='source', marker='o', s=80, edgecolors=color.to_rgba(y), facecolors='none')\n",
    "        ax2.scatter(X[:, 0], X[:, 1], label='source', marker='o', s=80, edgecolors=color.to_rgba(y), facecolors='none')\n",
    "\n",
    "        X = np.array(corrector_trainner.output(target_data['X_test'])).reshape((-1, 2))\n",
    "        y = target_data['y_test']\n",
    "        ax1.scatter(X[:, 0], X[:, 1], label='corrected', marker='x', s=80, edgecolors=color.to_rgba(y), facecolors=color.to_rgba(y))\n",
    "    \n",
    "        if 'X_train_centers' in corrector_data:\n",
    "            X = corrector_data['X_train_centers']\n",
    "            #ax1.scatter(X[:, 0], X[:, 1], label='target centers', marker='D', s=100, edgecolors='green', facecolors='green')\n",
    "            ax2.scatter(X[:, 0], X[:, 1], label='target centers', marker='D', s=100, edgecolors='green', facecolors='green')\n",
    "        \n",
    "            X = np.array(corrector_trainner.output(corrector_data['X_train_centers'])).reshape((-1, 2))\n",
    "            ax1.scatter(X[:, 0], X[:, 1], label='corrected centers', marker='D', s=100, edgecolors='k', facecolors='k')\n",
    "\n",
    "        if 'y_train_centers' in corrector_data:\n",
    "            X = corrector_data['y_train_centers']\n",
    "            ax1.scatter(X[:, 0], X[:, 1], label='source centers', marker='D', s=100, edgecolors='purple', facecolors='purple')\n",
    "            ax2.scatter(X[:, 0], X[:, 1], label='source centers', marker='D', s=100, edgecolors='purple', facecolors='purple')\n",
    "\n",
    "        if 'preprocess' in corrector_data and 'X_train_centers' in corrector_data and 'y_train_centers' in corrector_data:\n",
    "            idx = corrector_data['X_train_closest_cluster'][::10]\n",
    "            centers_corrected = np.array(corrector_trainner.output(corrector_data['X_train'][::10])).reshape((-1, 2))\n",
    "            centers_source = corrector_data['y_train_centers']\n",
    "            centers_source = centers_source[idx]\n",
    "            ax1.quiver(centers_corrected[:,0],centers_corrected[:,1],\n",
    "                      centers_source[:,0]-centers_corrected[:,0], centers_source[:,1]-centers_corrected[:,1],\n",
    "                      scale_units='xy', angles='xy', scale=1, facecolors='b', width=0.001)\n",
    "\n",
    "            centers_corrected = np.array(corrector_trainner.output(corrector_data['X_train'][::8])).reshape((-1, 2))\n",
    "            centers_source = corrector_data['y_train'][::8]\n",
    "            ax1.quiver(centers_corrected[:,0],centers_corrected[:,1],\n",
    "                      centers_source[:,0]-centers_corrected[:,0], centers_source[:,1]-centers_corrected[:,1],\n",
    "                      scale_units='xy', angles='xy', scale=1, facecolors='r', width=0.001)\n",
    "\n",
    "            idx = corrector_data['preprocess']\n",
    "            centers_corrected = np.array(corrector_trainner.output(corrector_data['X_train_centers'])).reshape((-1, 2))\n",
    "            centers_source = corrector_data['y_train_centers']\n",
    "            centers_source = centers_source[idx]\n",
    "            ax1.quiver(centers_corrected[:,0],centers_corrected[:,1],\n",
    "                      centers_source[:,0]-centers_corrected[:,0], centers_source[:,1]-centers_corrected[:,1],\n",
    "                      scale_units='xy', angles='xy', scale=1, facecolors='g')\n",
    "\n",
    "        ax1.set_title(title)\n",
    "        handles, labels = ax1.get_legend_handles_labels()\n",
    "        ax1.legend(handles, labels, bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.)\n",
    "        \n",
    "        X = target_data['X_test']\n",
    "        y = target_data['y_test']\n",
    "        ax2.scatter(X[:, 0], X[:, 1], label='target', marker='x', s=80, edgecolors=color.to_rgba(y), facecolors=color.to_rgba(y))\n",
    "        ax2.set_title(title)\n",
    "        handles, labels = ax2.get_legend_handles_labels()\n",
    "        ax2.legend(handles, labels, bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.)\n",
    "        if save:\n",
    "            fig.savefig('../fig/'+title+'-corrected_data.png', bbox_inches='tight')\n",
    "            #plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        #logger.info('Data plot {}'.format(X.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_img_samples():\n",
    "    global data_name, model_name, hp_lambda\n",
    "    title = '++'.join([data_name, model_name, 'lambda={:.3e}'.format(hp_lambda)])\n",
    "    global source_data, target_data, corrector_trainner\n",
    "    \n",
    "    if not data_name.startswith('MNIST'):\n",
    "        return\n",
    "\n",
    "    # Plot some sample images:\n",
    "    fig = plt.figure()\n",
    "    n_sample = 4\n",
    "    rand = np.random.RandomState()\n",
    "    for n in range(n_sample):\n",
    "        i = rand.randint(source_data['X_test'].shape[0])\n",
    "        sample_src = source_data['X_test'][i]\n",
    "        sample_trg = target_data['X_test'][i]\n",
    "        sample_corrected = corrector_trainner.output(target_data['X_test'][i][np.newaxis])\n",
    "        sample_corrected = np.array(sample_corrected).reshape((28,28))\n",
    "        ax = fig.add_subplot(n_sample, 3, n*3+1)\n",
    "        ax.axis('off')\n",
    "        ax.imshow(sample_src, cmap='Greys_r')\n",
    "        ax.set_title('Source image')\n",
    "        ax = fig.add_subplot(n_sample, 3, n*3+2)\n",
    "        ax.axis('off')\n",
    "        ax.imshow(sample_trg, cmap='Greys_r')\n",
    "        ax.set_title('Target image')\n",
    "        ax = fig.add_subplot(n_sample, 3, n*3+3)\n",
    "        ax.axis('off')\n",
    "        ax.imshow(sample_corrected, cmap='Greys_r')\n",
    "        ax.set_title('Corrected image')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_W():\n",
    "    global data_name, model_name, hp_lambda\n",
    "    title = '++'.join([data_name, model_name, 'lambda={:.3e}'.format(hp_lambda)])\n",
    "    global feature\n",
    "    # Plot the weights of the corrector\n",
    "    W = feature.W.get_value()\n",
    "    plt.imshow(W, interpolation='nearest', cmap=plt.cm.coolwarm)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(model_name, data_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Play !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "do_n_epoch(1)\n",
    "plot_2D_data(save=True)\n",
    "plot_img_samples()\n",
    "plot_learning_curve()\n",
    "plot_W()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
