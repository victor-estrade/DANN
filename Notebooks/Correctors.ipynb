{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "from __future__ import division, print_function\n",
    "\n",
    "import sys, os\n",
    "sys.path.append('..')\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from logs import log_fname, new_logger\n",
    "from nn.rgl import ReverseGradientLayer\n",
    "from nn.block import Dense, Classifier, adversarial\n",
    "from nn.compilers import crossentropy_sgd_mom, squared_error_sgd_mom\n",
    "from nn.training import Trainner, training\n",
    "\n",
    "from utils import plot_bound, save_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets\n",
    "\n",
    "Here the datasets are loaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the *corrector* and *domain* dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dom_corr_data():\n",
    "    global domain_data, corrector_data\n",
    "    domain_data = {\n",
    "                'X_train': [source_data['X_train'], target_data['X_train']],\n",
    "                'X_val': [source_data['X_val'], target_data['X_val']],\n",
    "                'X_test': [source_data['X_test'], target_data['X_test']],\n",
    "                'y_train': None,\n",
    "                'y_val': None,\n",
    "                'y_test': None,\n",
    "                'batchsize': batchsize,\n",
    "                }    \n",
    "\n",
    "    corrector_data = dict(target_data)\n",
    "    corrector_data.update({\n",
    "        'y_train': source_data['X_train'],\n",
    "        'y_val': source_data['X_val'],\n",
    "        'y_test': source_data['X_test'],\n",
    "        'labels': source_data['y_train'],\n",
    "        'batchsize': batchsize,\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clouds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clouds rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets.toys import load_cloud_rotated\n",
    "\n",
    "\n",
    "data_name = 'Clouds_Rotated'\n",
    "n_samples = 30  # Number of sample per class\n",
    "n_classes = 10\n",
    "batchsize = 80\n",
    "angle = 80\n",
    "\n",
    "source_data, target_data, domain_data = load_cloud_rotated(n_sample=n_samples, \n",
    "                                                           n_classes=n_classes, \n",
    "                                                           angle=angle, \n",
    "                                                           batchsize=batchsize)\n",
    "dom_corr_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clouds . Random Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.toys import load_cloud_rotated\n",
    "from datasets.utils import random_mat_dataset\n",
    "\n",
    "\n",
    "data_name = 'Clouds_RMat'\n",
    "n_samples = 30  # Number of sample per class\n",
    "n_classes = 12\n",
    "batchsize = 80\n",
    "\n",
    "source_data, target_data, domain_data = load_cloud_rotated(n_sample=n_samples, \n",
    "                                                           n_classes=n_classes, \n",
    "                                                           batchsize=batchsize)\n",
    "source_data, target_data, domain_data = random_mat_dataset(source_data)\n",
    "dom_corr_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moon rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.toys import load_moon\n",
    "\n",
    "data_name = 'Moon_Rotated'\n",
    "n_samples = 800\n",
    "batchsize = 80\n",
    "angle = 30.\n",
    "\n",
    "source_data, target_data, domain_data = load_moon(n_samples=n_samples, angle=angle, batchsize=batchsize)\n",
    "dom_corr_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moon . Random Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.toys import load_moon\n",
    "from datasets.utils import random_mat_dataset\n",
    "\n",
    "data_name = 'Moon_RMat'\n",
    "n_samples = 800\n",
    "batchsize = 80\n",
    "angle = 30.\n",
    "\n",
    "source_data, target_data, domain_data = load_moon(n_samples=n_samples, angle=angle, batchsize=batchsize)\n",
    "source_data, target_data, domain_data = random_mat_dataset(source_data)\n",
    "dom_corr_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST . Diag Dominant matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.mnist import load_mnist_src\n",
    "from datasets.utils import diag_dataset\n",
    "\n",
    "data_name = 'MNIST_Diag'\n",
    "batchsize = 500\n",
    "\n",
    "source_data = load_mnist_src(batchsize=batchsize)\n",
    "source_data, target_data, domain_data = diag_dataset(source_data, normalize=True)\n",
    "dom_corr_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Mirror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets.mnist import load_mnist_mirror\n",
    "\n",
    "data_name = 'MNIST_Mirror'\n",
    "batchsize = 500\n",
    "\n",
    "source_data, target_data, domain_data = load_mnist_mirror(batchsize=batchsize)\n",
    "dom_corr_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST . Random Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets.mnist import load_mnist_src\n",
    "from datasets.utils import random_mat_dataset\n",
    "\n",
    "data_name = 'MNIST_Rmat'\n",
    "batchsize = 500\n",
    "\n",
    "source_data = load_mnist_src(batchsize=batchsize)\n",
    "source_data, target_data, domain_data = random_mat_dataset(source_data, normalize=True)\n",
    "dom_corr_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epoch Preprocessing\n",
    "\n",
    "The preprocessing function that will run at the begining of each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/questions/25886374/pdist-for-theano-tensor\n",
    "# Tested and approved\n",
    "X = T.fmatrix('X')\n",
    "Y = T.fmatrix('Y')\n",
    "translation_vectors = X.reshape((X.shape[0], 1, -1)) - Y.reshape((1, Y.shape[0], -1))\n",
    "euclidiean_distances = (translation_vectors ** 2).sum(2)\n",
    "f_euclidean = theano.function([X, Y], euclidiean_distances, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kclosest(X, Y, k, batchsize=None):\n",
    "    \"\"\"\n",
    "    Computes for each sample from X the k-closest samples in Y and return \n",
    "    their index.\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "        X: (numpy array [n_sample, n_feature])\n",
    "        Y: (numpy array [n_sample, n_feature])\n",
    "        k: (int)\n",
    "    Return\n",
    "    ------\n",
    "        kclosest : (numpy array [n_sample, k]) the ordered index of \n",
    "            the k-closest instances from Y to X samples\n",
    "    \"\"\"\n",
    "    assert X.shape == Y.shape\n",
    "    N = X.shape[0]\n",
    "    if batchsize is None:\n",
    "        dist = f_euclidean(X.reshape(N, -1), Y.reshape(N, -1))\n",
    "    else:\n",
    "        dist = np.empty((N, N), dtype=theano.config.floatX)\n",
    "        batch = np.arange(0, N+batchsize, batchsize)\n",
    "        for excerpt_X in (slice(i0, i1) for i0, i1 in zip(batch[:-1], batch[1:])):\n",
    "            dist[excerpt_X] = f_euclidean(X[excerpt_X], Y)\n",
    "    kbest = np.argsort(dist, axis=1)[:, :k]\n",
    "    return kbest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def realign(X_out, X_trg, y, k=20, batchsize=None):\n",
    "    counter = np.zeros(X_out.shape[0], dtype=int)\n",
    "    idx = np.empty_like(y, dtype=int)\n",
    "    for label in np.unique(y):\n",
    "        # Get the examples of the right label\n",
    "        idx_label = np.where(y==label)[0]\n",
    "\n",
    "        # Get the k-closest index from the small part with the same labels\n",
    "        idx_label2 = kclosest(X_out[idx_label], X_trg[idx_label], k, batchsize=batchsize)\n",
    "        \n",
    "        for i1, i2 in zip(idx_label, idx_label2):\n",
    "            # i2 is an index array of shape (k,) with the sorted closest example index \n",
    "            # (of the sorted single class array)\n",
    "            # Then idx_label[i2] are the sorted original index of the k-closest examples\n",
    "            i = idx_label[i2[np.argmin(counter[idx_label[i2]])]]\n",
    "            # i contains the chosen one, in the (k-)clostest example, with the minimum counter\n",
    "            counter[i] = counter[i]+1\n",
    "            idx[i1] = i\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchpad(batchsize, output_shape, dtype=None):\n",
    "    \"\"\"Re-batching decorator\n",
    "    \"\"\"\n",
    "    def decoreted(func):\n",
    "        def wrapper(X, *args, **kwargs):\n",
    "            if dtype is None:\n",
    "                dtype2 = X.dtype\n",
    "            else:\n",
    "                dtype2 = dtype\n",
    "            \n",
    "            N = X.shape[0]\n",
    "            \n",
    "            if output_shape is None:\n",
    "                shape = X.shape\n",
    "            else:\n",
    "                shape = tuple( out_s if out_s is not None else X_s \n",
    "                              for out_s, X_s in zip(output_shape, X.shape))\n",
    "\n",
    "            result = np.empty(shape, dtype=dtype2)\n",
    "            batch = np.arange(0, N, batchsize)\n",
    "            for excerpt_X in (slice(i0, i1) for i0, i1 in zip(batch[:-1], batch[1:])):\n",
    "                result[excerpt_X] = func(X[excerpt_X], *args, **kwargs)\n",
    "            \n",
    "            last_excerpt = slice(batch[-1], N)\n",
    "            X = X[last_excerpt]\n",
    "            n_sample = X.shape[0]\n",
    "            X = np.vstack([X, np.zeros((batchsize-X.shape[0],)+X.shape[1:])])\n",
    "            X = func(X, *args, **kwargs)\n",
    "            result[last_excerpt] = X[:n_sample]\n",
    "            \n",
    "            return result\n",
    "        return wrapper\n",
    "    return decoreted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Méthode bourin. K-clostest on every data point\n",
    "def exhaustive_clostest(data, trainer, epoch, *args, **kwargs):\n",
    "    X = data['X_train']\n",
    "    k = data['k'] if 'k' in data else 5\n",
    "\n",
    "    @batchpad(data['batchsize'], X.shape, X.dtype)\n",
    "    def f_output(X, trainer):\n",
    "        return trainer.output(X)[0]\n",
    "    \n",
    "    X_out = f_output(X, trainer)\n",
    "    X_trg = data['y_train']\n",
    "    data['X_train'] = X[realign(X_out, X_trg, data['labels'], k=k, batchsize=None)]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classwise_shuffle(X, y):\n",
    "    \"\"\"\n",
    "    Shuffle X without changing the class positions\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "        X: the data (numpy array)\n",
    "        y: the labels \n",
    "    Return\n",
    "    ------\n",
    "        X_shuffled: Shuffled X without changing the class matching\n",
    "    \"\"\"\n",
    "    idx = np.empty_like(y, dtype=int)\n",
    "    for label in np.unique(y):\n",
    "        arr = np.where(y==label)[0]\n",
    "        arr2 = np.random.permutation(arr)\n",
    "        idx[arr] = arr2\n",
    "    return X[idx]\n",
    "\n",
    "\n",
    "def epoch_shuffle(data, trainer, epoch, *args, **kwargs):\n",
    "    data['X_train'] = classwise_shuffle(data['X_train'], data['labels'])\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster_preprocess(data, trainer, epoch, *args, **kwargs):\n",
    "    k = data['k'] if 'k' in data else 5\n",
    "    \n",
    "    @batchpad(data['batchsize'], None, X.dtype)\n",
    "    def f_output(X, trainer):\n",
    "        return trainer.output(X)[0]\n",
    "    \n",
    "    idx = realign(f_output(data['X_train_centers'], trainer), data['y_train_centers'], \n",
    "                  data['centers_labels'], k=k, batchsize=None)\n",
    "    # idx takes the indexes to relabel the closest clusters\n",
    "    # [2, 0, 1, 3]  means  0<-2, 1<-0, 2<-1, 3<-3\n",
    "    data['X_train_closest_cluster'] = idx[data['X_train_clusters'][:]]\n",
    "    # Now data['X_train_closest_cluster'] contains the nearest clusters label\n",
    "    # from each data in X_train to the clusters of y_train\n",
    "    \n",
    "    # Do a random realign from here\n",
    "    data['X_train'] = classwise_shuffle(data['X_train'], data['X_train_closest_cluster'])\n",
    "    \n",
    "    # Or a fully k-closest realignment\n",
    "    #idx = realign(f_output(data['X_train'], trainer), data['y_train'],\n",
    "    #                            data['X_train_closest_cluster'], k=k, batchsize=None)\n",
    "    #data['X_train'] = data['X_train'][idx]\n",
    "    data['preprocess'] = idx\n",
    "    return idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network building\n",
    "Start with the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hp_lambda = 0.\n",
    "\n",
    "label_rate = 1\n",
    "label_mom = 0.9\n",
    "\n",
    "domain_rate = 1\n",
    "domain_mom = 0.9\n",
    "\n",
    "# Get a logger\n",
    "logger = new_logger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare Theano variables for inputs and targets\n",
    "if data_name.startswith('MNIST'):\n",
    "    input_var = T.tensor3('inputs')\n",
    "    src_var = T.tensor3('src')\n",
    "    target_var = T.tensor3('targets')\n",
    "    shape = (batchsize, 28, 28)\n",
    "elif data_name.startswith('Moon') or data_name.startswith('Clouds'):\n",
    "    input_var = T.matrix('inputs')\n",
    "    src_var = T.matrix('src')\n",
    "    target_var = T.matrix('targets')\n",
    "    shape = (batchsize, 2)\n",
    "\n",
    "# Logs\n",
    "logger.info('Building the input and output variables for |{}|'.format(data_name))\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the layers\n",
    "input_layer = lasagne.layers.InputLayer(shape=shape, input_var=input_var)\n",
    "src_layer = lasagne.layers.InputLayer(shape=shape, input_var=src_var)\n",
    "# feature = lasagne.layers.DenseLayer(\n",
    "#                 input_layer,\n",
    "#                 num_units=np.prod(shape[1:]),\n",
    "#                 nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#                 # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "#                 )\n",
    "feature = lasagne.layers.DenseLayer(\n",
    "                input_layer,\n",
    "                num_units=np.prod(shape[1:]),\n",
    "                nonlinearity=None,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n",
    "reshaper = lasagne.layers.ReshapeLayer(feature, (-1,) + shape[1:])\n",
    "output_layer = reshaper\n",
    "\n",
    "# Logs\n",
    "logger.info('Building the neural network architecture for |{}|'.format(data_name))\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Logs\n",
    "logger.info('Compiling the neural network for |{}|'.format(data_name))\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n",
    "\n",
    "# Compilation\n",
    "corrector_trainner = Trainner(squared_error_sgd_mom(output_layer, lr=label_rate, mom=0, target_var=target_var), \n",
    "                             'corrector',)\n",
    "\n",
    "if hp_lambda != 0.0:\n",
    "    print('hp_lambda != 0 : Compliling the adversarial part of the networks')\n",
    "    domain_trainner = Trainner(adversarial([src_layer, output_layer], hp_lambda=hp_lambda,\n",
    "                                          lr=domain_rate, mom=domain_mom),\n",
    "                               'domain')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add preprocessing (for alignment)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Choose preprocessing :\n",
    "#corrector_trainner.preprocess = epoch_shuffle\n",
    "#corrector_trainner.preprocess = exhaustive_clostest\n",
    "corrector_trainner.preprocess = cluster_preprocess\n",
    "\n",
    "model_name = ''\n",
    "if corrector_trainner.preprocess is epoch_shuffle:\n",
    "    model_name = 'Classwise_Corrector'\n",
    "elif corrector_trainner.preprocess is exhaustive_clostest:\n",
    "    model_name = 'Exhaustive-closest_Corrector'\n",
    "elif corrector_trainner.preprocess is cluster_preprocess:\n",
    "    model_name = 'Cluster_Corrector'\n",
    "    \n",
    "    n_clusters = 12\n",
    "    km = KMeans(n_clusters=n_clusters)\n",
    "    km.fit(corrector_data['X_train'].reshape((corrector_data['X_train'].shape[0], -1)))\n",
    "    corrector_data['X_train_centers'] = km.cluster_centers_.reshape((n_clusters,)+shape[1:])\n",
    "    corrector_data['X_train_clusters'] = km.labels_\n",
    "    corrector_data['k'] = 12\n",
    "    km = KMeans(n_clusters=n_clusters)\n",
    "    km.fit(corrector_data['y_train'].reshape((corrector_data['y_train'].shape[0], -1)))\n",
    "    corrector_data['y_train_centers'] = km.cluster_centers_.reshape((n_clusters,)+shape[1:])\n",
    "    corrector_data['y_train_clusters'] = km.labels_\n",
    "    #corrector_data['labels'] = np.zeros(n_clusters)\n",
    "    corrector_data['centers_labels'] = np.zeros(n_clusters)\n",
    "else:\n",
    "    model_name = 'Pairwise_Corrector'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Choose preprocessing :\n",
    "#corrector_trainner.preprocess = epoch_shuffle\n",
    "#corrector_trainner.preprocess = exhaustive_clostest\n",
    "corrector_trainner.preprocess = cluster_preprocess\n",
    "\n",
    "model_name = ''\n",
    "if corrector_trainner.preprocess is epoch_shuffle:\n",
    "    model_name = 'Classwise_Corrector'\n",
    "    corrector_data['labels'] = source_data['y_train']\n",
    "elif corrector_trainner.preprocess is exhaustive_clostest:\n",
    "    model_name = 'K-closest_Corrector'\n",
    "    corrector_data['labels'] = source_data['y_train']\n",
    "elif corrector_trainner.preprocess is cluster_preprocess:\n",
    "    model_name = 'Cluster_Corrector'\n",
    "    n_clusters = 10\n",
    "    corrector_data['k'] = 10\n",
    "    y = source_data['y_train']\n",
    "    classes = np.unique(y)\n",
    "\n",
    "    centers = []\n",
    "    clusters_label = np.empty(corrector_data['X_train'].shape[0], dtype=int)\n",
    "    labels = []\n",
    "    for label in classes:\n",
    "        km = KMeans(n_clusters=n_clusters)\n",
    "        y = source_data['y_train']\n",
    "        idx = np.where(y==label)[0]\n",
    "        X = corrector_data['X_train'][idx]\n",
    "        km.fit(X.reshape((X.shape[0], -1)))\n",
    "        centers.append(km.cluster_centers_.reshape((n_clusters,)+shape[1:]))\n",
    "        clusters_label[idx] = km.labels_+label*n_clusters\n",
    "        labels.append(np.ones(n_clusters, dtype=int)*label)\n",
    "    \n",
    "    corrector_data['X_train_centers'] = np.vstack(centers)\n",
    "    corrector_data['X_train_clusters'] = clusters_label\n",
    "    corrector_data['centers_labels'] = np.hstack(labels)\n",
    "    \n",
    "    \n",
    "    centers = []\n",
    "    labels = []\n",
    "    clusters_label = np.empty(corrector_data['y_train'].shape[0], dtype=int)\n",
    "    for label in classes:\n",
    "        km = KMeans(n_clusters=n_clusters)\n",
    "        y = target_data['y_train']\n",
    "        idx = np.where(y==label)[0]\n",
    "        X = corrector_data['y_train'][idx]\n",
    "        km.fit(X.reshape((X.shape[0], -1)))\n",
    "        centers.append(km.cluster_centers_.reshape((n_clusters,)+shape[1:]))\n",
    "        clusters_label[idx] = km.labels_+label*n_clusters\n",
    "    corrector_data['y_train_centers'] = np.vstack(centers)\n",
    "    corrector_data['y_train_clusters'] = clusters_label\n",
    "\n",
    "else:\n",
    "    model_name = 'Pairwise_Corrector'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the counter and the stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.warn('Reset the epoch counter and saved statistics')\n",
    "epoch_counter = 0\n",
    "final_stats = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_n_epoch(n_epoch):\n",
    "    global epoch_counter, logger, final_stats\n",
    "    global corrector_data, domain_data, corrector_trainner, domain_trainner\n",
    "    epoch_counter += n_epoch\n",
    "    logger.info('Trainning the neural network for {} additional epochs ({} total)'.format(n_epoch, epoch_counter))\n",
    "    if hp_lambda != 0.0:\n",
    "        stats = training([corrector_trainner, domain_trainner], [corrector_data, domain_data],\n",
    "                         num_epochs=n_epoch, logger=logger)\n",
    "    else:\n",
    "        stats = training([corrector_trainner,], [corrector_data,],\n",
    "                     num_epochs=n_epoch, logger=logger)\n",
    "\n",
    "    final_stats = {k: (final_stats[k]+v if k in final_stats else v) for k, v in stats.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Learning curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve():\n",
    "    global final_stats, data_name, model_name, hp_lambda\n",
    "    title = '++'.join([data_name, model_name, 'lambda={:.3e}'.format(hp_lambda)])\n",
    "    # Plot learning accuracy curve\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(final_stats['corrector valid loss'], label='source')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.set_title(title)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, labels, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    #fig.savefig('fig/'+title+'.png', bbox_inches='tight')\n",
    "    #fig.clf() # Clear plot window\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Data plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_2D_data(save=False):\n",
    "    global data_name, model_name, hp_lambda\n",
    "    title = '++'.join([data_name, model_name, 'lambda={:.3e}'.format(hp_lambda)])\n",
    "    global source_data, target_data, corrector_data, corrector_trainner\n",
    "    \n",
    "    if data_name.startswith('MNIST'):\n",
    "        return\n",
    "    \n",
    "    from matplotlib.colors import ListedColormap\n",
    "    import matplotlib.cm as cm\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    if data_name.startswith('Moon'):\n",
    "        color = cm.ScalarMappable(cmap=cm_bright)\n",
    "    else:\n",
    "        color = cm.ScalarMappable(cmap='Paired')\n",
    "\n",
    "    if data_name.startswith('Moon') or data_name.startswith('Clouds'):\n",
    "        # Plot the test data\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))\n",
    "        X = source_data['X_test']\n",
    "        y = source_data['y_test']\n",
    "        ax1.scatter(X[:, 0], X[:, 1], label='source', marker='o', s=80, edgecolors=color.to_rgba(y), facecolors='none')\n",
    "        ax2.scatter(X[:, 0], X[:, 1], label='source', marker='o', s=80, edgecolors=color.to_rgba(y), facecolors='none')\n",
    "\n",
    "        X = np.array(corrector_trainner.output(target_data['X_test'])).reshape((-1, 2))\n",
    "        y = target_data['y_test']\n",
    "        ax1.scatter(X[:, 0], X[:, 1], label='corrected', marker='x', s=80, edgecolors=color.to_rgba(y), facecolors=color.to_rgba(y))\n",
    "    \n",
    "        if 'X_train_centers' in corrector_data:\n",
    "            X = corrector_data['X_train_centers']\n",
    "            #ax1.scatter(X[:, 0], X[:, 1], label='target centers', marker='D', s=100, edgecolors='green', facecolors='green')\n",
    "            ax2.scatter(X[:, 0], X[:, 1], label='target centers', marker='D', s=100, edgecolors='green', facecolors='green')\n",
    "        \n",
    "            X = np.array(corrector_trainner.output(corrector_data['X_train_centers'])).reshape((-1, 2))\n",
    "            ax1.scatter(X[:, 0], X[:, 1], label='corrected centers', marker='D', s=100, edgecolors='k', facecolors='k')\n",
    "\n",
    "        if 'y_train_centers' in corrector_data:\n",
    "            X = corrector_data['y_train_centers']\n",
    "            ax1.scatter(X[:, 0], X[:, 1], label='source centers', marker='D', s=100, edgecolors='purple', facecolors='purple')\n",
    "            ax2.scatter(X[:, 0], X[:, 1], label='source centers', marker='D', s=100, edgecolors='purple', facecolors='purple')\n",
    "\n",
    "        if 'preprocess' in corrector_data and 'X_train_centers' in corrector_data and 'y_train_centers' in corrector_data:\n",
    "            idx = corrector_data['X_train_closest_cluster'][::10]\n",
    "            centers_corrected = np.array(corrector_trainner.output(corrector_data['X_train'][::10])).reshape((-1, 2))\n",
    "            centers_source = corrector_data['y_train_centers']\n",
    "            centers_source = centers_source[idx]\n",
    "            ax1.quiver(centers_corrected[:,0],centers_corrected[:,1],\n",
    "                      centers_source[:,0]-centers_corrected[:,0], centers_source[:,1]-centers_corrected[:,1],\n",
    "                      scale_units='xy', angles='xy', scale=1, facecolors='b', width=0.001)\n",
    "\n",
    "            centers_corrected = np.array(corrector_trainner.output(corrector_data['X_train'][::8])).reshape((-1, 2))\n",
    "            centers_source = corrector_data['y_train'][::8]\n",
    "            ax1.quiver(centers_corrected[:,0],centers_corrected[:,1],\n",
    "                      centers_source[:,0]-centers_corrected[:,0], centers_source[:,1]-centers_corrected[:,1],\n",
    "                      scale_units='xy', angles='xy', scale=1, facecolors='r', width=0.001)\n",
    "\n",
    "            idx = corrector_data['preprocess']\n",
    "            centers_corrected = np.array(corrector_trainner.output(corrector_data['X_train_centers'])).reshape((-1, 2))\n",
    "            centers_source = corrector_data['y_train_centers']\n",
    "            centers_source = centers_source[idx]\n",
    "            ax1.quiver(centers_corrected[:,0],centers_corrected[:,1],\n",
    "                      centers_source[:,0]-centers_corrected[:,0], centers_source[:,1]-centers_corrected[:,1],\n",
    "                      scale_units='xy', angles='xy', scale=1, facecolors='g')\n",
    "\n",
    "        ax1.set_title(title)\n",
    "        handles, labels = ax1.get_legend_handles_labels()\n",
    "        ax1.legend(handles, labels, bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.)\n",
    "        \n",
    "        X = target_data['X_test']\n",
    "        y = target_data['y_test']\n",
    "        ax2.scatter(X[:, 0], X[:, 1], label='target', marker='x', s=80, edgecolors=color.to_rgba(y), facecolors=color.to_rgba(y))\n",
    "        ax2.set_title(title)\n",
    "        handles, labels = ax2.get_legend_handles_labels()\n",
    "        ax2.legend(handles, labels, bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.)\n",
    "        if save:\n",
    "            fig.savefig('../fig/'+title+'-corrected_data.png', bbox_inches='tight')\n",
    "            #plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        #logger.info('Data plot {}'.format(X.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_img_samples():\n",
    "    global data_name, model_name, hp_lambda\n",
    "    title = '++'.join([data_name, model_name, 'lambda={:.3e}'.format(hp_lambda)])\n",
    "    global source_data, target_data, corrector_trainner\n",
    "    \n",
    "    if not data_name.startswith('MNIST'):\n",
    "        return\n",
    "\n",
    "    # Plot some sample images:\n",
    "    fig = plt.figure()\n",
    "    n_sample = 4\n",
    "    rand = np.random.RandomState()\n",
    "    for n in range(n_sample):\n",
    "        i = rand.randint(source_data['X_test'].shape[0])\n",
    "        sample_src = source_data['X_test'][i]\n",
    "        sample_trg = target_data['X_test'][i]\n",
    "        sample_corrected = corrector_trainner.output(target_data['X_test'][i][np.newaxis])\n",
    "        sample_corrected = np.array(sample_corrected).reshape((28,28))\n",
    "        ax = fig.add_subplot(n_sample, 3, n*3+1)\n",
    "        ax.axis('off')\n",
    "        ax.imshow(sample_src, cmap='Greys_r')\n",
    "        ax.set_title('Source image')\n",
    "        ax = fig.add_subplot(n_sample, 3, n*3+2)\n",
    "        ax.axis('off')\n",
    "        ax.imshow(sample_trg, cmap='Greys_r')\n",
    "        ax.set_title('Target image')\n",
    "        ax = fig.add_subplot(n_sample, 3, n*3+3)\n",
    "        ax.axis('off')\n",
    "        ax.imshow(sample_corrected, cmap='Greys_r')\n",
    "        ax.set_title('Corrected image')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_W():\n",
    "    global data_name, model_name, hp_lambda\n",
    "    title = '++'.join([data_name, model_name, 'lambda={:.3e}'.format(hp_lambda)])\n",
    "    global feature\n",
    "    # Plot the weights of the corrector\n",
    "    W = feature.W.get_value()\n",
    "    plt.imshow(W, interpolation='nearest', cmap=plt.cm.coolwarm)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(model_name, data_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Play !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "do_n_epoch(1)\n",
    "plot_2D_data(save=True)\n",
    "plot_img_samples()\n",
    "plot_learning_curve()\n",
    "plot_W()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
