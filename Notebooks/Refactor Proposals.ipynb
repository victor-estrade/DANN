{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puisque l'on commence à être nombreux à travailler sur l'adaptation de \n",
    "domaine avec les réseaux de neurones, je propose que l'on se rassemble tous\n",
    "dans la même pièce pour discuter de nos besoins communs en matière \n",
    "d'implémentation. Ainsi nous ne perdrons pas de temps à coder les mêmes choses\n",
    "chacun de son coté et il sera plus facile de s'entre aider pour le débogage.\n",
    "\n",
    "Je rassemble ici les diverses choses que l'on est plusieurs à avoir besoin \n",
    "(voire tous).\n",
    "\n",
    "\n",
    "Construire le réseaux de neurones\n",
    "=================================\n",
    "1. Gérer les architectures séquentielles et en graphe\n",
    "2. Gérer les entrées multiples dans une même couche\n",
    "\t1. équivalent à partager les poids entre les couches\n",
    "\t2. équivalent aux réseaux siamois\n",
    "\t3. En Lasagne cela signifie être capable de 'cloner' les couches\n",
    "3. Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasagne\n",
    "-------\n",
    "\n",
    "Avec Lasagne on peut déjà gérer les structures en DAG (Directed Acyclic Graph) très facilement à l'exception des entrées multiples dans une même couche:\n",
    "\n",
    "~~~\n",
    " A\n",
    "  \\\n",
    "   }->C\n",
    "  /\n",
    " B  \n",
    "~~~\n",
    "\n",
    "Du coup c'est déjà bien !\n",
    "\n",
    "**Remarques:**\n",
    "Beaucoup, BEAUCOUP de structures sont possibles avec les réseaux de neurone. Mais le plus important est que cette structure **dépend des données** ce qui rend un peu plus difficile l'automatisation de leur construction.\n",
    "\n",
    "Exemples:\n",
    "- shape : suivant le nombre de descripteur de nos données mais aussi leur dimensions, (batchsize, n_features) pour les données 'classiques ou (batchsize, width, length) pour les images en nuance de gris ou encore (batchsize, chanels, width, length) pour les images en couleurs, on doit adapter les couches d'entrées voir même de sortie pour les décodeurs.\n",
    "- input variable : corrolaire du problème précédent la variable d'entrée d'un input layer doit suivre les dimensions des données, T.matrix, T.tensor3 ou T.tensor4.\n",
    "- suivant notre type de réseaux de neurone, XANN, DANN, CANN ou classique nous n'avons pas besoin des mêmes choses : partage des poids/clonage des couches, backpropagation multiple ou pas, etc\n",
    "- Enfin la structure interne du réseau va grandement changer suivant les données : convolution ou pas, nombre de couche dense et leur nombre de neurone voir même réseaux récurrents...\n",
    "\n",
    "Pour toute ces raisons je pense qu'il n'est pas très productif de créer un super code plein de paramètre pour construire la structure de nos réseau. D'ailleur les codes utilisant Lasagne sur internet ne le font jamais.Il y a probablement une bonne raison..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import lasagne\n",
    "\n",
    "batchsize = 50\n",
    "# Prepare Theano variables for inputs and targets\n",
    "input_var = T.matrix('inputs')\n",
    "src_var = T.matrix('src')\n",
    "target_var = T.matrix('targets')\n",
    "shape = (batchsize, 2)\n",
    "\n",
    "# Build the layers\n",
    "input_layer = lasagne.layers.InputLayer(shape=shape, input_var=input_var)\n",
    "dense_layer = lasagne.layers.DenseLayer(\n",
    "                input_layer,\n",
    "                num_units=np.prod(shape[1:]),\n",
    "                nonlinearity=None,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n",
    "reshaper = lasagne.layers.ReshapeLayer(dense_layer, (-1,) + shape[1:])\n",
    "output_layer = reshaper\n",
    "\n",
    "\n",
    "src_layer = lasagne.layers.InputLayer(shape=shape, input_var=src_var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrées multiples / Clonage / Partage des poids\n",
    "-----------------------------------------------\n",
    "\n",
    "On est plusieurs à avoir besoin de partager certaines parties de nos réseaux entre plusieurs rétro-propagations / entrées. La partie adversarial de façon générale doit être partagé entre plusieurs entrées/chemins dans le réseaux différents.\n",
    "\n",
    "Lasagne ne permet pas (pour des raisons de calcul de gradient ?) d'avoir plusieurs entrée possible dans une même couche. Le fonctionnement est simple. Au moment de la compilation Lasagne remonte les couches en suivant l'entrée des couches. C'est suivant ce chemin que va se passer la rétro-propagation de la fonction que l'on est en train de compiler. Au final Lasagne nous force à réfléchir en terme de rétro-propagation. Si on a plusieurs combinaisons de (entrée / sortie) alors il nous faudra une fonction compilée par combinaison (entrée /sortie).\n",
    "\n",
    "Une solution serait de pouvoir simplement cloner une couche (ie en reconstruire une partageant les poids de la couche d'origine) et de changer l'entrée et où va la sortie.\n",
    "\n",
    "Pour cela on peut, par exemple, se faire une petite usine à clonage :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clone_DenseLayer(layer, input_layer=None):\n",
    "    if not isinstance(layer, lasagne.layers.DenseLayer):\n",
    "        raise ValueError(\"The given layer should be a lasagne.layers.DenseLayer,\"\n",
    "                         \"{} given\".format(layer.__class__))\n",
    "    else:\n",
    "        if input_layer is None:\n",
    "            input_layer = layer.input_layer\n",
    "        return lasagne.layers.DenseLayer(input_layer,\n",
    "                                        num_units=layer.num_units,\n",
    "                                        nonlinearity=layer.nonlinearity,\n",
    "                                        W=layer.W, b=layer.b)\n",
    "\n",
    "\n",
    "# Clone factory\n",
    "clonable_layers = {\n",
    "    lasagne.layers.DenseLayer: clone_DenseLayer\n",
    "}\n",
    "\n",
    "\n",
    "def clone_layer(layer, input_layer=None):\n",
    "    if any([isinstance(layer, key) for key in clonable_layers.keys()]):\n",
    "        return clonable_layers[layer.__class__](layer, input_layer)\n",
    "    else:\n",
    "        raise NotImplementedError('{} is not a clonable layer (yet)'.format(layer.__class__))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilation\n",
    "-----------\n",
    "\n",
    "Concernant la compilation on doit construire :\n",
    "- l'expression symbolique du coût (loss symbolic expression)\n",
    "- la mise à jour des paramètres concerné pendant la rétro-propagation (update the paramters during backpropagation)\n",
    "- les fonctions :\n",
    "    - d'entraînement\n",
    "    - de test (sort directement la précision et le coût)\n",
    "    - de prédiction (sort les prédictions)\n",
    "    - de sortie brut (sort les probabilité ou bien les données reconstruites/décodés)\n",
    "    - d'autres ?\n",
    "    \n",
    "Bon ça pour le coup on a déjà plutôt bien travaillé dessus.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crossentropy_sgd_mom(output_layer, lr=1, mom=.9, target_var=T.ivector('target')): \n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent compiler with optionnal momentum.\n",
    "\n",
    "    info: it uses the categorical_crossentropy. Should be given to a softmax layer.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "        output_layer: the output layer from which the loss and updtaes will be computed\n",
    "        lr: (default=1) learning rate.\n",
    "        mom: (default=0.9) momentum.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "        A dictionnary with :\n",
    "            -train : function used to train the neural network\n",
    "            -predict : function used to predict the label\n",
    "            -valid : function used to get the accuracy and loss \n",
    "            -output : function used to get the output (exm: predict the label probabilities)\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> funs = compiler_sgd_mom(output_layer, lr=0.01, mom=0.1)\n",
    "    \n",
    "    \"\"\"    \n",
    "\n",
    "    input_var = lasagne.layers.get_all_layers(output_layer)[0].input_var\n",
    "    # Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "    pred = lasagne.layers.get_output(output_layer)\n",
    "    loss = T.mean(lasagne.objectives.categorical_crossentropy(pred, target_var))\n",
    "    # Create update expressions for training, i.e., how to modify the\n",
    "    # parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "    # Descent and add a momentum to it.\n",
    "    params = lasagne.layers.get_all_params(output_layer, trainable=True)\n",
    "    updates = lasagne.updates.sgd(loss, params, learning_rate=lr)\n",
    "    updates = lasagne.updates.apply_momentum(updates, params, momentum=mom)\n",
    "\n",
    "    # As a bonus, also create an expression for the classification accuracy:\n",
    "    acc = T.mean(T.eq(T.argmax(pred, axis=1), target_var))\n",
    "    # Compile a function performing a training step on a mini-batch (by giving\n",
    "    # the updates dictionary) and returning the corresponding training loss:\n",
    "    train_function = theano.function([input_var, target_var], [loss, acc], \n",
    "        updates=updates, allow_input_downcast=True)\n",
    "\n",
    "    # Create a loss expression for validation/testing. The crucial difference\n",
    "    # here is that we do a deterministic forward pass through the network,\n",
    "    # disabling dropout and noise layers.\n",
    "    pred = lasagne.layers.get_output(output_layer, deterministic=True)\n",
    "    loss = T.mean(lasagne.objectives.categorical_crossentropy(pred, target_var))\n",
    "    # As a bonus, also create an expression for the classification:\n",
    "    label = T.argmax(pred, axis=1)\n",
    "    # As a bonus, also create an expression for the classification accuracy:\n",
    "    acc = T.mean(T.eq(label, target_var))\n",
    "    # Compile a second function computing the validation loss and accuracy:\n",
    "    valid_function = theano.function([input_var, target_var], [loss, acc], allow_input_downcast=True)\n",
    "    # Compile a function computing the predicted labels:\n",
    "    predict_function = theano.function([input_var], [label], allow_input_downcast=True)\n",
    "    # Compile an output function\n",
    "    output_function = theano.function([input_var], [pred], allow_input_downcast=True)\n",
    "\n",
    "    return {\n",
    "            'train': train_function,\n",
    "            'predict': predict_function,\n",
    "            'valid': valid_function,\n",
    "            'output': output_function\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squared_error_sgd_mom(output_layer, lr=1, mom=.9, target_var=T.matrix('target')) : \n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent compiler with optionnal momentum.\n",
    "\n",
    "    info: it uses the squared_error.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "        output_layer: the output layer from which the loss and updtaes will be computed\n",
    "        lr: (default=1) learning rate.\n",
    "        mom: (default=0.9) momentum.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "        A dictionnary with :\n",
    "            -train : function used to train the neural network\n",
    "            -predict : function used to predict the label\n",
    "            -valid : function used to get the accuracy and loss \n",
    "            -output : function used to get the output (exm: predict the label probabilities)\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> funs = squared_error_sgd_mom(output_layer, lr=0.01, mom=0.1)\n",
    "    \n",
    "    \"\"\"    \n",
    "\n",
    "    input_var = lasagne.layers.get_all_layers(output_layer)[0].input_var\n",
    "    # Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "    pred = lasagne.layers.get_output(output_layer)\n",
    "    loss = T.mean(lasagne.objectives.squared_error(pred, target_var))\n",
    "    # Create update expressions for training, i.e., how to modify the\n",
    "    # parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "    # Descent and add a momentum to it.\n",
    "    params = lasagne.layers.get_all_params(output_layer, trainable=True)\n",
    "    updates = lasagne.updates.sgd(loss, params, learning_rate=lr)\n",
    "    updates = lasagne.updates.apply_momentum(updates, params, momentum=mom)\n",
    "\n",
    "    # As a bonus, also create an expression for the classification accuracy:\n",
    "    acc = T.mean((pred - target_var)**2)\n",
    "    # Compile a function performing a training step on a mini-batch (by giving\n",
    "    # the updates dictionary) and returning the corresponding training loss:\n",
    "    train_function = theano.function([input_var, target_var], [loss, acc], \n",
    "        updates=updates, allow_input_downcast=True)\n",
    "    \n",
    "    # Create a loss expression for validation/testing. The crucial difference\n",
    "    # here is that we do a deterministic forward pass through the network,\n",
    "    # disabling dropout and noise layers.\n",
    "    pred = lasagne.layers.get_output(output_layer, deterministic=True)\n",
    "    loss = T.mean(lasagne.objectives.squared_error(pred, target_var))\n",
    "    # As a bonus, also create an expression for the classification:\n",
    "    label = T.argmax(pred, axis=1)\n",
    "    # As a bonus, also create an expression for the classification accuracy:\n",
    "    acc = T.mean((pred - target_var)**2)\n",
    "    # Compile a second function computing the validation loss and accuracy:\n",
    "    valid_function = theano.function([input_var, target_var], [loss, acc], allow_input_downcast=True)\n",
    "    # Compile a function computing the predicted labels:\n",
    "    predict_function = theano.function([input_var], [label], allow_input_downcast=True)\n",
    "    # Compile an output function\n",
    "    output_function = theano.function([input_var], [pred], allow_input_downcast=True)\n",
    "    \n",
    "    return {\n",
    "            'train': train_function,\n",
    "            'predict': predict_function,\n",
    "            'valid': valid_function,\n",
    "            'output': output_function\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le processus d'entraînement\n",
    "===========================\n",
    "\n",
    "1. Gérer les données\n",
    "\t1. Les trouver/récupérer sur le net\n",
    "\t2. Les charger\n",
    "\t3. Les transformer (afin de simuler un nouveau domaine)\n",
    "\t4. Les séparer en training/validation/test set\n",
    "2. Boucle d'entraînement \"do_n_epoch()\"\n",
    "\t1. Gérer l'entraînement alternatif de divers jeux de données avec des \n",
    "\t\tfonctions d'entraînement compilés différentes (rétro-propagation multiple)\n",
    "\t2. Gérer le pré-traitement des données avant chaque époque\n",
    "\t3. Sauvegarder des statistiques sur la session d'entraînement (précision, coût, etc)\n",
    "3. Brancher la partie adversarial sur nos réseaux de neurones facilement/automatiquement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Charger les données\n",
    "-------------------\n",
    "\n",
    "En général je recommanderais l'utilisation des conventions de scikit-learn :\n",
    "- X : les données contenant les descripteurs\n",
    "- y : l'objectif (labels, valeur, etc)\n",
    "Dans le cas où on doit diviser les données en 3 sous-ensembles on ajoute simplement `_train _val _test`. Mais nous avons en plus de cela la division entre les données originelles et les données transformées voir même des données supplémentaires :\n",
    "\n",
    "`y_origin` par exemple qui indique dans un `X_melanger` où on a mélangé les données originelles et les données transformées.\n",
    "\n",
    "Bref. C'est le bordel !\n",
    "\n",
    "Il nous faut donc une méthode pour travailler avec tous ces jeux de données sans se perdre dans des noms à la java style : `X_train_transform_non_mélangé`\n",
    "\n",
    "Une possibilité :\n",
    "puisque l'on va toujours diviser les données en 3x2 (train, val, test + X, y) autant rassembler ces données sous la même variable avec un dictionnaire.\n",
    "\n",
    "Enfin je pense qu'on peut mettre tout ça dans un package python `datasets` avec des fonctions `load_smth()` qui renvoit les données préformattées comme on veut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "def load_moons(noise=0.05, n_samples=500, batchsize=32):\n",
    "    \"\"\"\n",
    "    Load the Moon dataset using sklearn.datasets.make_moons() function.\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "        noise: (default=0.05) the noise of the moon data generator\n",
    "        n_samples: (default=500) the total number of points generated\n",
    "        batchsize: (default=32) the dataset batchsize\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "        source_data: dict with the separated data\n",
    "\n",
    "    \"\"\"\n",
    "    X, y = make_moons(n_samples=n_samples, shuffle=True, noise=noise, random_state=12345)\n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    y = np.array(y, dtype=np.int32)\n",
    "    \n",
    "    #X, y = shuffle_array(X, y)  # Usefull ?\n",
    "\n",
    "    n_train = int(0.4*n_samples)\n",
    "    n_val = int(0.3*n_samples)+n_train\n",
    "\n",
    "    X_train, X_val, X_test = X[0:n_train], X[n_train:n_val], X[n_val:]\n",
    "    y_train, y_val, y_test = y[0:n_train], y[n_train:n_val], y[n_val:]\n",
    "    \n",
    "    source_data = {\n",
    "                    'X_train': X_train,\n",
    "                    'y_train': y_train,\n",
    "                    'X_val': X_val,\n",
    "                    'y_val': y_val,\n",
    "                    'X_test': X_test,\n",
    "                    'y_test': y_test,\n",
    "                    'batchsize': batchsize,\n",
    "                    }\n",
    "    return source_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Transformer les données\n",
    "-----------------------\n",
    "\n",
    "Maintenant que l'on peut gérer les données proprement, les transformer ne devrait pas être trop compliqué.\n",
    "Cependant il serait domage de faire n'importe quoi.\n",
    "\n",
    "Un \"transformer\" doit se présenter sous la forme d'une fonction:\n",
    "~~~\n",
    "data_transformed = do_smth(data)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rotate_data(X, angle=35.):\n",
    "    \"\"\"Apply a rotation on a 2D dataset.\n",
    "    \"\"\"\n",
    "    theta = (angle/180.) * np.pi\n",
    "    rotMatrix = np.array([[np.cos(theta), -np.sin(theta)], \n",
    "                             [np.sin(theta),  np.cos(theta)]])\n",
    "    X_r = np.empty_like(X)\n",
    "    X_r[:] = X[:].dot(rotMatrix)\n",
    "    return X_r\n",
    "\n",
    "def rotate_dataset(source_data, angle=35.):\n",
    "    \"\"\"\n",
    "    Transform the given dataset by applying a rotation to it.\n",
    "\n",
    "    target_data <- source_data . Rotation_Matrix\n",
    "\n",
    "    Can be used only on 2D datasets !\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "        source_data: a dataset (dict with the separated data)\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "        target_data: dict with the separated transformed data\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    X_train = source_data['X_train']\n",
    "    y_train = source_data['y_train']\n",
    "    X_val = source_data['X_val']\n",
    "    y_val = source_data['y_val']\n",
    "    X_test = source_data['X_test']\n",
    "    y_test = source_data['y_test']\n",
    "    batchsize = source_data['batchsize']\n",
    "\n",
    "    target_data = {\n",
    "                'X_train': rotate_data(X_train, angle=angle),\n",
    "                'y_train': y_train,\n",
    "                'X_val': rotate_data(X_val, angle=angle),\n",
    "                'y_val': y_val,\n",
    "                'X_test': rotate_data(X_test, angle=angle),\n",
    "                'y_test': y_test,\n",
    "                'batchsize': batchsize,\n",
    "                }\n",
    "\n",
    "    return target_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source_data = load_moons()\n",
    "rotate_data = rotate_dataset(source_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entraînement\n",
    "------------\n",
    "\n",
    "1. Gérer l'entraînement alternatif de divers jeux de données avec des \n",
    "    fonctions d'entraînement compilés différentes (rétro-propagation multiple)\n",
    "2. Gérer le pré-traitement des données avant chaque époque\n",
    "3. Sauvegarder des statistiques sur la session d'entraînement (précision, coût, etc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, funs, name='trainer'):\n",
    "        super(Trainner, self).__init__()\n",
    "        self.name = name\n",
    "        # Add the compiled functions to the object\n",
    "        # by adding dynamic property to this object\n",
    "        self.__dict__.update(funs)\n",
    "    \n",
    "    def preprocess(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "\n",
    "def training(trainers, train_data, testers=[], test_data=[], num_epochs=20, logger=None):\n",
    "    \"\"\"\n",
    "    TODO : Explain the whole function\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "        trainers: list of Trainer\n",
    "        train_data: list of dataset\n",
    "        testers: (default=[]) list of Trainer\n",
    "        test_data: (default=[]) list of datasets\n",
    "        num_epochs: (default=20)\n",
    "        logger: (default=None)\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "        stats: dict with stats\n",
    "    \"\"\"\n",
    "    if logger is None:\n",
    "        logger = new_logger()\n",
    "\n",
    "    logger.info(\"Starting training...\")\n",
    "    final_stats = {}\n",
    "    final_stats.update({trainer.name+' training loss': [] for trainer in trainers})\n",
    "    final_stats.update({trainer.name+' training acc': [] for trainer in trainers})\n",
    "    final_stats.update({trainer.name+' valid loss': [] for trainer in trainers})\n",
    "    final_stats.update({trainer.name+' valid acc': [] for trainer in trainers})\n",
    "    final_stats.update({tester.name+' valid loss': [] for tester in testers})\n",
    "    final_stats.update({tester.name+' valid acc': [] for tester in testers})\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Prepare the statistics\n",
    "        start_time = time.time()\n",
    "        stats = { key:[] for key in final_stats.keys()}\n",
    "\n",
    "        # Do some trainning preparations :\n",
    "        for data, trainer in zip(train_data+test_data, trainers+testers):\n",
    "            trainer.preprocess(data, trainer, epoch)\n",
    "\n",
    "        # Training : (forward and backward propagation)\n",
    "        # done with the iterative functions\n",
    "        batches = tuple(iterate_minibatches(data['X_train'], data['y_train'], data['batchsize'], shuffle=True) \n",
    "                        for data in train_data)\n",
    "        for minibatches in zip(*batches):\n",
    "            for batch, trainer in zip(minibatches, trainers):\n",
    "                # X, y = batch\n",
    "                loss, acc = trainer.train(*batch)\n",
    "                stats[trainer.name+' training loss'].append(loss)\n",
    "                stats[trainer.name+' training acc'].append(acc*100)\n",
    "        \n",
    "        # Validation (forward propagation)\n",
    "        # done with the iterative functions\n",
    "        batches = tuple(iterate_minibatches(data['X_val'], data['y_val'], data['batchsize']) \n",
    "                        for data in train_data+test_data)\n",
    "        for minibatches in zip(*batches):\n",
    "            for batch, valider in zip(minibatches, trainers+testers):\n",
    "                # X, y = batch\n",
    "                loss, acc = valider.valid(*batch)\n",
    "                stats[valider.name+' valid loss'].append(loss)\n",
    "                stats[valider.name+' valid acc'].append(acc*100)\n",
    "        \n",
    "        logger.info(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        for stat_name, stat_value in sorted(stats.items()):\n",
    "            if stat_value:\n",
    "                mean_value = np.mean(stat_value)\n",
    "                logger.info('   {:30} : {:.6f}'.format(\n",
    "                    stat_name, mean_value))\n",
    "                final_stats[stat_name].append(mean_value)\n",
    "\n",
    "    return final_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corrector_trainner = Trainer(squared_error_sgd_mom(output_layer, lr=label_rate, mom=0, target_var=target_var), \n",
    "                             'corrector',)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brancher l'adversarial\n",
    "----------------------\n",
    "\n",
    "Cette partie est toujours présente/possible dans lnos réseaux donc il serait plutôt bien d'avoir un méthode automatique et facile.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def adversarial(layers, hp_lambda=1, lr=1, mom=.9):\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent adversarial block compiler with optionnal momentum.\n",
    "\n",
    "    info: it uses the categorical_crossentropy.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "        lr: (default=1) learning rate.\n",
    "        mom: (default=0.9) momentum.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "        compiler_function: a function that takes an output layer and return\n",
    "            a dictionnary with :\n",
    "            -train : function used to train the neural network\n",
    "            -predict : function used to predict the label\n",
    "            -valid : function used to get the accuracy and loss \n",
    "            -output : function used to get the output (exm: predict the label probabilities)\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    TODO\n",
    "    \"\"\"    \n",
    "\n",
    "    concat = lasagne.layers.ConcatLayer(layers, axis=0)\n",
    "    rgl = ReverseGradientLayer(concat, hp_lambda=hp_lambda)\n",
    "    output_layer = lasagne.layers.DenseLayer(\n",
    "                    rgl,\n",
    "                    num_units=len(layers),\n",
    "                    nonlinearity=lasagne.nonlinearities.softmax,\n",
    "                    )\n",
    "\n",
    "    input_vars = [lasagne.layers.get_all_layers(layer)[0].input_var for layer in layers]\n",
    "    true_domains = [np.ones(lasagne.layers.get_all_layers(layer)[0].shape[0], dtype=np.int32)*i \n",
    "                        for i, layer in enumerate(layers)]\n",
    "    true_domains = np.hstack(true_domains)\n",
    "    \n",
    "    # Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "    pred = lasagne.layers.get_output(output_layer)\n",
    "    loss = T.mean(lasagne.objectives.categorical_crossentropy(pred, true_domains))\n",
    "    # Create update expressions for training, i.e., how to modify the\n",
    "    # parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "    # Descent and add a momentum to it.\n",
    "    params = lasagne.layers.get_all_params(output_layer, trainable=True)\n",
    "    updates = lasagne.updates.sgd(loss, params, learning_rate=lr)\n",
    "    updates = lasagne.updates.apply_momentum(updates, params, momentum=mom)\n",
    "\n",
    "    # As a bonus, also create an expression for the classification accuracy:\n",
    "    acc = T.mean(T.eq(T.argmax(pred, axis=1), true_domains))\n",
    "    # Compile a function performing a training step on a mini-batch (by giving\n",
    "    # the updates dictionary) and returning the corresponding training loss:\n",
    "    train_function = theano.function(input_vars, [loss, acc], \n",
    "        updates=updates, allow_input_downcast=True)\n",
    "\n",
    "    # Create a loss expression for validation/testing. The crucial difference\n",
    "    # here is that we do a deterministic forward pass through the network,\n",
    "    # disabling dropout and noise layers.\n",
    "    pred = lasagne.layers.get_output(output_layer, deterministic=True)\n",
    "    loss = T.mean(lasagne.objectives.categorical_crossentropy(pred, true_domains))\n",
    "    # As a bonus, also create an expression for the classification:\n",
    "    label = T.argmax(pred, axis=1)\n",
    "    # As a bonus, also create an expression for the classification accuracy:\n",
    "    acc = T.mean(T.eq(label, true_domains))\n",
    "    # Compile a second function computing the validation loss and accuracy:\n",
    "    valid_function = theano.function(input_vars, [loss, acc], allow_input_downcast=True)\n",
    "    # Compile a function computing the predicted labels:\n",
    "    predict_function = theano.function(input_vars, [label], allow_input_downcast=True)\n",
    "    # Compile an output function\n",
    "    output_function = theano.function(input_vars, [pred], allow_input_downcast=True)\n",
    "\n",
    "    funs = {\n",
    "            'train': train_function,\n",
    "            'predict': predict_function,\n",
    "            'valid': valid_function,\n",
    "            'output': output_function\n",
    "           }\n",
    "\n",
    "    return funs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation / Validation / Diagnostiques\n",
    "==========================================\n",
    "\n",
    "1. Courbe d'apprentissage\n",
    "\t1. Précision\n",
    "\t2. Coût\n",
    "\t3. Erreur de reconstruction\n",
    "\t4. etc\n",
    "2. Matrices de confusion\n",
    "3. Réseaux de neurones\n",
    "\t1. Poids\n",
    "\t2. Détecter la saturation ?\n",
    "\t3. Architecture\n",
    "\t4. etc\n",
    "4. Données\n",
    "\t1. 2D scatter plots\n",
    "\t2. Exemples d'image  (avant / après transformation / correction)\n",
    "\t3. TSNE\n",
    "\t4. etc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Conventions\n",
    "----------\n",
    "\n",
    "On va tracer des graphiques. Pyplot peut être assez lourds puisqu'on peut tout contrôler. L'objectif c'est d'avoir des petites foncitons d'aide pour tracer des information qu'on veux très souvent.\n",
    "\n",
    "Pyplot ne peut pas merge les figures mais on peut placer des axes dans une figure.\n",
    "Donc le mieux est de prendre `ax=None` comme argument optionnel pour pouvoir éventuellement placer le graph dans une sous parti d'une figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_curve(stats, ax=None, label=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    else:\n",
    "        fig = ax.get_figure()\n",
    "    # Plot learning accuracy curve\n",
    "    ax.plot(stats, label=label)\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def add_legend(ax, xlabel='', ylabel='', title=''):\n",
    "    \"\"\"Add legend to the given axes\n",
    "    \"\"\"\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, labels, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats = np.random.randn(100)+5*np.cos(np.linspace(0,10, num=100))\n",
    "fig, ax = plot_curve(stats, label='Curve')\n",
    "add_legend(ax, xlabel='X', ylabel='Y', title='Some Test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_img_samples(datasets, n_sample=4, cmap='Greys_r'):\n",
    "    n_datasets = len(datasets)\n",
    "    # Plot some sample images:\n",
    "    fig = plt.figure()\n",
    "    rand = np.random.RandomState()\n",
    "    for n in range(n_sample):\n",
    "        i = rand.randint(source_data['X_test'].shape[0])\n",
    "        for j, data in enumerate(datasets):\n",
    "            sample = data['X_test'][i]\n",
    "            ax = fig.add_subplot(n_sample, n_datasets, n*n_datasets+1+j)\n",
    "            ax.axis('off')\n",
    "            ax.imshow(sample, cmap=cmap)\n",
    "            if 'name' in data:\n",
    "                ax.set_title(data['name'])\n",
    "    return fig, fig.get_axes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('..')\n",
    "from datasets.mnist import load_mnist\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist_data = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plot_img_samples([mnist_data, mnist_data, mnist_data])\n",
    "fig.suptitle('Image samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_mat(mat, ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    else:\n",
    "        fig = ax.get_figure()\n",
    "    sns.heatmap(mat, cmap=plt.cm.coolwarm, ax=ax)\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_mat(np.arange(5*5).reshape(5,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_mat(mnist_data['X_train'][0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plot_mat(dense_layer.W.get_value())\n",
    "add_legend(ax, xlabel='XXX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def plot_learning_curve(stats, regex='acc', title=''):\n",
    "    keys = [k for k in stats.keys() if re.search(regex, k)]\n",
    "    print(keys)\n",
    "    fig, ax = plt.subplots()\n",
    "    for k in keys:\n",
    "        # Plot learning accuracy curve\n",
    "        ax.plot(final_stats[k], label=k)\n",
    "    add_legend(ax, xlabel='epoch', ylabel='loss')\n",
    "    fig.suptitle(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_stats = {}\n",
    "final_stats['corrector valid loss'] = np.random.randn(100)+5*np.cos(np.linspace(0,10, num=100))\n",
    "final_stats['blabla valid loss'] = np.random.randn(100)+3*np.cos(np.linspace(0,10, num=100))\n",
    "final_stats['bloublou valid loss'] = np.random.randn(100)+1*np.cos(np.linspace(0,10, num=100))\n",
    "\n",
    "plot_learning_curve(final_stats, regex='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
