{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import sys\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The objective here is to do some unit testing on every function and blocks of the EMANN method.\n",
    "Moreover this we serve as example code of how to do things and use the functions + reminder of why I have coded this way.\n",
    "\n",
    "The main blocks :\n",
    "- Loading the data\n",
    "- Transform the data to produce adaptaition toys\n",
    "- Use Kmeans is easy with sci-kit learn\n",
    "- Get the *mass* of the clusters\n",
    "- Solve the optimal transport problem\n",
    "- Produce cost-matrices with pairwise distance\n",
    "- Use the results of the optimal transport matrix\n",
    "    - Produce the coupling dataset\n",
    "    - Check things\n",
    "- Build a neural networks with the helper class CNN\n",
    "- Compile the neural network to get what we want\n",
    "\n",
    "- Train the CNN with a single path (no alternative training of sub-parts) \n",
    "- Train the CNN with minibatches parts' alternative training (I am quite proud of this overcomplex code)\n",
    "    - DANN example\n",
    "- A one-loop first and simple example of EMANN process\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import visual\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets\n",
    "\n",
    "How to load/generate and build Datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Data](#Load-datasets)\n",
    "    - [1.1 Loading of datasets](#Load-datasets)\n",
    "    - [1.2 Transformation of datasets](#Transform-datasets)\n",
    "- [2. Clusters](#Clusters)\n",
    "- [3. Optimal transport](#Optimal-Transport)\n",
    "    - [3.1 Optimal transport solver](#Optimal-Transport)\n",
    "    - [3.2 Align](#Align)\n",
    "    - [3.3 Cost matrix](#Cost-matrix)\n",
    "- [4. Neural Networks](#Neural-Networks)\n",
    "    - [4.1 Special blocks](#Special-blocks)\n",
    "    - [4.2 Compiler](#Compiler)\n",
    "    - [4.3 CNN class](#Neural-Network-class)\n",
    "- [5. Examples](#Examples)\n",
    "    - [5.1 Classification](#Classification)\n",
    "    - [5.2 Regression](#Regression)\n",
    "    - [5.3 MNIST](#MNIST)\n",
    "    - [5.4 DANN](#DANN)\n",
    "    - [5.5 Dual proba classification](#Dual-proba-classification)\n",
    "    - [5.6 One loop EMANN](#One-loop-EMANN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.toys import make_clouds, make_circles, make_X, make_moons\n",
    "from datasets.mnist import load_mnist\n",
    "from datasets.utils import make_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Data](#Load-datasets)\n",
    "    - [1.1 Loading of datasets](#Load-datasets)\n",
    "    - [1.2 Transformation of datasets](#Transform-datasets)\n",
    "- [2. Clusters](#Clusters)\n",
    "- [3. Optimal transport](#Optimal-Transport)\n",
    "    - [3.1 Optimal transport solver](#Optimal-Transport)\n",
    "    - [3.2 Align](#Align)\n",
    "    - [3.3 Cost matrix](#Cost-matrix)\n",
    "- [4. Neural Networks](#Neural-Networks)\n",
    "    - [4.1 Special blocks](#Special-blocks)\n",
    "    - [4.2 Compiler](#Compiler)\n",
    "    - [4.3 CNN class](#Neural-Network-class)\n",
    "- [5. Examples](#Examples)\n",
    "    - [5.1 Classification](#Classification)\n",
    "    - [5.2 Regression](#Regression)\n",
    "    - [5.3 MNIST](#MNIST)\n",
    "    - [5.4 DANN](#DANN)\n",
    "    - [5.5 Dual proba classification](#Dual-proba-classification)\n",
    "    - [5.6 One loop EMANN](#One-loop-EMANN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.utils import make_domain_dataset, make_corrector_dataset\n",
    "from datasets import transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusters\n",
    "\n",
    "Here we test and illustrate the use of sci-kit learn's KMeans in our problem (Optimal transport)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Data](#Load-datasets)\n",
    "    - [1.1 Loading of datasets](#Load-datasets)\n",
    "    - [1.2 Transformation of datasets](#Transform-datasets)\n",
    "- [2. Clusters](#Clusters)\n",
    "- [3. Optimal transport](#Optimal-Transport)\n",
    "    - [3.1 Optimal transport solver](#Optimal-Transport)\n",
    "    - [3.2 Align](#Align)\n",
    "    - [3.3 Cost matrix](#Cost-matrix)\n",
    "- [4. Neural Networks](#Neural-Networks)\n",
    "    - [4.1 Special blocks](#Special-blocks)\n",
    "    - [4.2 Compiler](#Compiler)\n",
    "    - [4.3 CNN class](#Neural-Network-class)\n",
    "- [5. Examples](#Examples)\n",
    "    - [5.1 Classification](#Classification)\n",
    "    - [5.2 Regression](#Regression)\n",
    "    - [5.3 MNIST](#MNIST)\n",
    "    - [5.4 DANN](#DANN)\n",
    "    - [5.5 Dual proba classification](#Dual-proba-classification)\n",
    "    - [5.6 One loop EMANN](#One-loop-EMANN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_src, y_src = make_clouds(n_samples=50, n_classes=6)\n",
    "X_tgt, y_tgt = make_moons(n_samples=500)\n",
    "data_name = \"Clouds-to-moons\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k_src = 10\n",
    "k_tgt = 11\n",
    "# We do not need to have the same number of cluster in the source and target data.\n",
    "k_means_src = KMeans(n_clusters=k_src).fit(X_src)\n",
    "k_means_tgt = KMeans(n_clusters=k_tgt).fit(X_tgt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mass(k_means):\n",
    "    \"\"\"\n",
    "    Params\n",
    "    ------\n",
    "        k_means: (sklearn.cluster.KMeans instance) should be trained\n",
    "    Return\n",
    "    ------\n",
    "        w: (numpy.array [n_clusters]) the mass of each clusters \n",
    "    \"\"\"\n",
    "    w = np.unique(k_means.labels_, return_counts=True)[1]\n",
    "    w = w/np.sum(w)\n",
    "    return w\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now the *mass computation* is very simple. Maybe we can improve the results by taking labels or other things into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_src = mass(k_means_src)\n",
    "# w_src = np.abs(np.sin(-w_src-np.arange(w_src.shape[0])))\n",
    "# w_src /= np.sum(w_src)\n",
    "\n",
    "w_tgt = mass(k_means_tgt)\n",
    "# I modify the mass in order to test a custom final distrib of the data in the coupling dataset\n",
    "w_tgt = np.abs(np.sin(-w_tgt-0.6*np.arange(w_tgt.shape[0])))\n",
    "# w_tgt = np.exp(-w_tgt-np.arange(w_tgt.shape[0]))\n",
    "w_tgt /= np.sum(w_tgt)\n",
    "cost_mat = np.random.uniform(0,1, size=(w_src.shape[0], w_tgt.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visual.mat(cost_mat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now every thing is set up(masses and cost matrix). We can solve the optimal transport problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Transport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Data](#Load-datasets)\n",
    "    - [1.1 Loading of datasets](#Load-datasets)\n",
    "    - [1.2 Transformation of datasets](#Transform-datasets)\n",
    "- [2. Clusters](#Clusters)\n",
    "- [3. Optimal transport](#Optimal-Transport)\n",
    "    - [3.1 Optimal transport solver](#Optimal-Transport)\n",
    "    - [3.2 Align](#Align)\n",
    "    - [3.3 Cost matrix](#Cost-matrix)\n",
    "- [4. Neural Networks](#Neural-Networks)\n",
    "    - [4.1 Special blocks](#Special-blocks)\n",
    "    - [4.2 Compiler](#Compiler)\n",
    "    - [4.3 CNN class](#Neural-Network-class)\n",
    "- [5. Examples](#Examples)\n",
    "    - [5.1 Classification](#Classification)\n",
    "    - [5.2 Regression](#Regression)\n",
    "    - [5.3 MNIST](#MNIST)\n",
    "    - [5.4 DANN](#DANN)\n",
    "    - [5.5 Dual proba classification](#Dual-proba-classification)\n",
    "    - [5.6 One loop EMANN](#One-loop-EMANN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the solvers functions\n",
    "from opt_transport import opt_transp_sup, computeTransportSinkhorn, computeTransportSinkhornLabelsLpL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transp = opt_transp_sup(k_means_src.cluster_centers_, k_means_tgt.cluster_centers_)\n",
    "transp = computeTransportSinkhorn(w_src, w_tgt, cost_mat, reg=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visual.mat(transp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is very easy.\n",
    "\n",
    "TODO : Add more test and visualisation of the different transport solvers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Align"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Data](#Load-datasets)\n",
    "    - [1.1 Loading of datasets](#Load-datasets)\n",
    "    - [1.2 Transformation of datasets](#Transform-datasets)\n",
    "- [2. Clusters](#Clusters)\n",
    "- [3. Optimal transport](#Optimal-Transport)\n",
    "    - [3.1 Optimal transport solver](#Optimal-Transport)\n",
    "    - [3.2 Align](#Align)\n",
    "    - [3.3 Cost matrix](#Cost-matrix)\n",
    "- [4. Neural Networks](#Neural-Networks)\n",
    "    - [4.1 Special blocks](#Special-blocks)\n",
    "    - [4.2 Compiler](#Compiler)\n",
    "    - [4.3 CNN class](#Neural-Network-class)\n",
    "- [5. Examples](#Examples)\n",
    "    - [5.1 Classification](#Classification)\n",
    "    - [5.2 Regression](#Regression)\n",
    "    - [5.3 MNIST](#MNIST)\n",
    "    - [5.4 DANN](#DANN)\n",
    "    - [5.5 Dual proba classification](#Dual-proba-classification)\n",
    "    - [5.6 One loop EMANN](#One-loop-EMANN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we build the datasets that will be fed to the NN thanks to the transport matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(transp.shape, X_src.shape, k_means_src.labels_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from align_learn.preprocess import align"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check. The cluster distribution of the **aligned data** should be the same as the **target distrib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "align_idx, cluster_T = align(transp, k_means_src.labels_, k_means_tgt.labels_)\n",
    "# print(np.unique(res).shape, X_src.shape, X_tgt.shape)\n",
    "uniq, count = np.unique(cluster_T, return_counts=True)\n",
    "# plt.plot(np.sum(transp,0), label='transp.sum(0)')\n",
    "plt.plot(w_tgt, label='w_tgt')\n",
    "# plt.plot(np.sum(transp,1), label='transp.sum(1)')\n",
    "plt.plot(w_src, label='w_src')\n",
    "plt.plot(count/cluster_T.shape[0], label='mapping')\n",
    "plt.legend(bbox_to_anchor=(1.25, 1.))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_S, y_S = X_src, y_src\n",
    "X_T, y_T = X_tgt[align_idx], y_tgt[align_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the next cell that there is a big hole in the coupling dataset. This is due to the target weights set near zero for one of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visual.target_2D(X_T, y_T)\n",
    "visual.target_2D(X_tgt, y_tgt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_T.shape, X_S.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the probabilities to be predict by the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_dataset(X_src, X_tgt, k_means_src, k_means_tgt, transp):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    align_idx, cluster_T = align(transp, k_means_src.labels_, k_means_tgt.labels_)\n",
    "    X_S, y_S = X_src, y_src\n",
    "    X_T, y_T = X_tgt[align_idx], y_tgt[align_idx]\n",
    "    # Build the probabilities to be predict\n",
    "    # For the source data\n",
    "    proba_src = np.zeros((X_S.shape[0], k_means_src.n_clusters))\n",
    "    proba_src[np.arange(X_S.shape[0]), k_means_src.labels_] = 1.\n",
    "    proba_tgt = transp[k_means_src.labels_]\n",
    "    Y_S = np.hstack([proba_src, proba_tgt])\n",
    "    \n",
    "    # Build the probabilities to be predict\n",
    "    # For the aligned target data\n",
    "    proba_tgt = np.zeros((X_T.shape[0], k_means_tgt.n_clusters))\n",
    "    proba_tgt[np.arange(X_T.shape[0]), cluster_T] = 1.\n",
    "    proba_src = transp[:, cluster_T].T\n",
    "    Y_T = np.hstack([proba_src, proba_tgt])\n",
    "    \n",
    "    Y = np.vstack([Y_S, Y_T])\n",
    "    X = np.vstack([X_S, X_T])\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, Y = train_dataset(X_src, X_tgt, k_means_src, k_means_tgt, transp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(Y[0])\n",
    "print(Y[350])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Data](#Load-datasets)\n",
    "    - [1.1 Loading of datasets](#Load-datasets)\n",
    "    - [1.2 Transformation of datasets](#Transform-datasets)\n",
    "- [2. Clusters](#Clusters)\n",
    "- [3. Optimal transport](#Optimal-Transport)\n",
    "    - [3.1 Optimal transport solver](#Optimal-Transport)\n",
    "    - [3.2 Align](#Align)\n",
    "    - [3.3 Cost matrix](#Cost-matrix)\n",
    "- [4. Neural Networks](#Neural-Networks)\n",
    "    - [4.1 Special blocks](#Special-blocks)\n",
    "    - [4.2 Compiler](#Compiler)\n",
    "    - [4.3 CNN class](#Neural-Network-class)\n",
    "- [5. Examples](#Examples)\n",
    "    - [5.1 Classification](#Classification)\n",
    "    - [5.2 Regression](#Regression)\n",
    "    - [5.3 MNIST](#MNIST)\n",
    "    - [5.4 DANN](#DANN)\n",
    "    - [5.5 Dual proba classification](#Dual-proba-classification)\n",
    "    - [5.6 One loop EMANN](#One-loop-EMANN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "Here will be tested and illustrated the neural network building, training and monitoring from the simple classification to an alternative training on a multiple output neural network.\n",
    "\n",
    "4 steps :\n",
    "1. Build the architecture (symbolic computation graph) with lasagne\n",
    "2. Compile the NN functions (train, valid, output, prediction, etc)\n",
    "3. Do forward backward training loop\n",
    "4. Plot the statistics and test the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "import nn.compilers as nnc\n",
    "import nn.block as nnb\n",
    "\n",
    "from nn.helper import CNN, NN\n",
    "from nn.rgl import ReverseGradientLayer\n",
    "from logs import log_fname, new_logger, empty_logger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special blocks\n",
    "\n",
    "Here we build the layers and introduce some function to build some special/frequently used structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Data](#Load-datasets)\n",
    "    - [1.1 Loading of datasets](#Load-datasets)\n",
    "    - [1.2 Transformation of datasets](#Transform-datasets)\n",
    "- [2. Clusters](#Clusters)\n",
    "- [3. Optimal transport](#Optimal-Transport)\n",
    "    - [3.1 Optimal transport solver](#Optimal-Transport)\n",
    "    - [3.2 Align](#Align)\n",
    "    - [3.3 Cost matrix](#Cost-matrix)\n",
    "- [4. Neural Networks](#Neural-Networks)\n",
    "    - [4.1 Special blocks](#Special-blocks)\n",
    "    - [4.2 Compiler](#Compiler)\n",
    "    - [4.3 CNN class](#Neural-Network-class)\n",
    "- [5. Examples](#Examples)\n",
    "    - [5.1 Classification](#Classification)\n",
    "    - [5.2 Regression](#Regression)\n",
    "    - [5.3 MNIST](#MNIST)\n",
    "    - [5.4 DANN](#DANN)\n",
    "    - [5.5 Dual proba classification](#Dual-proba-classification)\n",
    "    - [5.6 One loop EMANN](#One-loop-EMANN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the hyper parameters\n",
    "# ========================\n",
    "hp_lambda = 0.\n",
    "batchsize = 20\n",
    "\n",
    "# Learning rates and momentums\n",
    "label_rate = 0.1\n",
    "label_mom = 0.9\n",
    "\n",
    "domain_rate = 0.1\n",
    "domain_mom = 0.9\n",
    "\n",
    "# Get a logger\n",
    "logger = new_logger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute some data dependant params\n",
    "# ==================================\n",
    "_shape = np.shape(X)\n",
    "n_dim = len(_shape)\n",
    "n_features = np.prod(_shape[1:])\n",
    "\n",
    "shape = (batchsize,) + _shape[1:]\n",
    "target_var = T.matrix('targets')\n",
    "\n",
    "# Logs\n",
    "logger.info('Building the input and output variables for : {}'.format(data_name))\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the layers\n",
    "# ================\n",
    "input_layer = lasagne.layers.InputLayer(shape=shape)\n",
    "\n",
    "dense_1 = lasagne.layers.DenseLayer(\n",
    "                input_layer,\n",
    "                num_units=5,\n",
    "                nonlinearity=lasagne.nonlinearities.rectify,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n",
    "dense_2 = lasagne.layers.DenseLayer(\n",
    "                dense_1,\n",
    "                num_units=5,\n",
    "                nonlinearity=lasagne.nonlinearities.rectify,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n",
    "dense_3 = lasagne.layers.DenseLayer(\n",
    "                dense_2,\n",
    "                num_units=5,\n",
    "                nonlinearity=lasagne.nonlinearities.rectify,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Or we could use the dense block function\n",
    "dense_3 = nnb.dense(input_layer, [5,5,5], activation='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here is the way to get the inputs variables. Even in a complex structure the input must be a InputLayer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "concat_test = lasagne.layers.ConcatLayer([dense_1, dense_2, dense_3], axis=1)\n",
    "input_2 = lasagne.layers.InputLayer((20,2))\n",
    "concat_test_2 = lasagne.layers.ConcatLayer([concat_test, input_2], axis=1)\n",
    "dense_final = lasagne.layers.DenseLayer(concat_test_2, 5)\n",
    "lasagne.layers.get_all_layers(dense_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nn.compilers import get_input_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_variables = get_input_vars(dense_final)\n",
    "print(input_variables)\n",
    "print(input_variables[0] is input_variables[1])\n",
    "print(input_variables[0] == input_variables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiler\n",
    "Append the last part and compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Data](#Load-datasets)\n",
    "    - [1.1 Loading of datasets](#Load-datasets)\n",
    "    - [1.2 Transformation of datasets](#Transform-datasets)\n",
    "- [2. Clusters](#Clusters)\n",
    "- [3. Optimal transport](#Optimal-Transport)\n",
    "    - [3.1 Optimal transport solver](#Optimal-Transport)\n",
    "    - [3.2 Align](#Align)\n",
    "    - [3.3 Cost matrix](#Cost-matrix)\n",
    "- [4. Neural Networks](#Neural-Networks)\n",
    "    - [4.1 Special blocks](#Special-blocks)\n",
    "    - [4.2 Compiler](#Compiler)\n",
    "    - [4.3 CNN class](#Neural-Network-class)\n",
    "- [5. Examples](#Examples)\n",
    "    - [5.1 Classification](#Classification)\n",
    "    - [5.2 Regression](#Regression)\n",
    "    - [5.3 MNIST](#MNIST)\n",
    "    - [5.4 DANN](#DANN)\n",
    "    - [5.5 Dual proba classification](#Dual-proba-classification)\n",
    "    - [5.6 One loop EMANN](#One-loop-EMANN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the output layer :\n",
    "# ======================\n",
    "predict_layer = nnb.multi_proba(dense_1, [k_src, k_tgt])\n",
    "\n",
    "# Instanciate the NN :\n",
    "# ====================\n",
    "nn = NN(predict_layer, name='Special proba test')\n",
    "\n",
    "# Compile :\n",
    "# =========\n",
    "# nn.compile(compiler, lr=label_rate)\n",
    "nn.compile(nnc.crossentropy_sgd_mom, lr=label_rate, mom=label_mom, target_var=target_var,\n",
    "           regularization=None, reg_param=.1)\n",
    "nn.compile(nnc.crossentropy_validation, target_var=target_var)\n",
    "nn.compile(nnc.output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = make_dataset(X, Y, batchsize=batchsize) # Those X and Y are the coupling dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn.train(data, num_epochs=100)\n",
    "final_stats = nn.global_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of the learning procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ================\n",
    "# Learning curve\n",
    "# ================\n",
    "fig, ax = visual.learning_curve(final_stats, regex='loss')\n",
    "#     SAVE\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(fig_title+'-Learning_curve.png',bbox_inches='tight')\n",
    "fig.show()\n",
    "# visual.learning_curve(final_stats, regex='domain.* acc');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Neural Network class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Data](#Load-datasets)\n",
    "    - [1.1 Loading of datasets](#Load-datasets)\n",
    "    - [1.2 Transformation of datasets](#Transform-datasets)\n",
    "- [2. Clusters](#Clusters)\n",
    "- [3. Optimal transport](#Optimal-Transport)\n",
    "    - [3.1 Optimal transport solver](#Optimal-Transport)\n",
    "    - [3.2 Align](#Align)\n",
    "    - [3.3 Cost matrix](#Cost-matrix)\n",
    "- [4. Neural Networks](#Neural-Networks)\n",
    "    - [4.1 Special blocks](#Special-blocks)\n",
    "    - [4.2 Compiler](#Compiler)\n",
    "    - [4.3 CNN class](#Neural-Network-class)\n",
    "- [5. Examples](#Examples)\n",
    "    - [5.1 Classification](#Classification)\n",
    "    - [5.2 Regression](#Regression)\n",
    "    - [5.3 MNIST](#MNIST)\n",
    "    - [5.4 DANN](#DANN)\n",
    "    - [5.5 Dual proba classification](#Dual-proba-classification)\n",
    "    - [5.6 One loop EMANN](#One-loop-EMANN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Some sanity check on the working of AttributeDict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = CNN()\n",
    "nn['out'] = 'layer'\n",
    "def foo(string, **kwargs):\n",
    "    def bar():\n",
    "        print('compile('+', '.join([string]+kwargs.keys())+')')\n",
    "        print('OK')\n",
    "    return {'bar': bar}\n",
    "nn.compile('out', foo, kwargs1=8, kwargs2='bla')\n",
    "# TEST\n",
    "nn['out'].bar()\n",
    "nn['out']['bar']()\n",
    "nn.parts.out.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "\n",
    "Let's have some example code with the complete workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Data](#Load-datasets)\n",
    "    - [1.1 Loading of datasets](#Load-datasets)\n",
    "    - [1.2 Transformation of datasets](#Transform-datasets)\n",
    "- [2. Clusters](#Clusters)\n",
    "- [3. Optimal transport](#Optimal-Transport)\n",
    "    - [3.1 Optimal transport solver](#Optimal-Transport)\n",
    "    - [3.2 Align](#Align)\n",
    "    - [3.3 Cost matrix](#Cost-matrix)\n",
    "- [4. Neural Networks](#Neural-Networks)\n",
    "    - [4.1 Special blocks](#Special-blocks)\n",
    "    - [4.2 Compiler](#Compiler)\n",
    "    - [4.3 CNN class](#Neural-Network-class)\n",
    "- [5. Examples](#Examples)\n",
    "    - [5.1 Classification](#Classification)\n",
    "    - [5.2 Regression](#Regression)\n",
    "    - [5.3 MNIST](#MNIST)\n",
    "    - [5.4 DANN](#DANN)\n",
    "    - [5.5 Dual proba classification](#Dual-proba-classification)\n",
    "    - [5.6 One loop EMANN](#One-loop-EMANN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_classes = 5\n",
    "n_samples = 1000\n",
    "test_dataset = make_dataset(*make_clouds(n_samples=n_samples, n_classes=n_classes), batchsize=60)\n",
    "# test_dataset = make_dataset(*make_moons(n_samples=n_samples), batchsize=60)\n",
    "print('Dataset contains :', test_dataset.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get general information :\n",
    "# =========================\n",
    "X = test_dataset.X_train\n",
    "_shape = np.shape(X)\n",
    "n_dim = len(_shape)\n",
    "n_features = np.prod(_shape[1:])\n",
    "\n",
    "shape = (batchsize,) + _shape[1:]\n",
    "target_var = T.ivector('targets')\n",
    "\n",
    "# Logs\n",
    "logger.info('Building the input and output variables for : {}'.format(data_name))\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n",
    "\n",
    "# Build the layers :\n",
    "# ==================\n",
    "# Build the layers\n",
    "input_layer = lasagne.layers.InputLayer(shape=shape)\n",
    "\n",
    "dense_1 = lasagne.layers.DenseLayer(\n",
    "                input_layer,\n",
    "                num_units=3,\n",
    "                nonlinearity=lasagne.nonlinearities.tanh,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n",
    "softmax_layer = lasagne.layers.DenseLayer(\n",
    "                dense_1,\n",
    "                num_units=n_classes,\n",
    "                nonlinearity=lasagne.nonlinearities.softmax,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n",
    "\n",
    "# Instanciate the NN :\n",
    "# ====================\n",
    "\n",
    "# nn = CNN(name='Clouds test')\n",
    "nn = NN(softmax_layer, name='Clouds test')\n",
    "# nn.add_output('main', softmax_layer)\n",
    "\n",
    "# Compile :\n",
    "# =========\n",
    "nn.compile(nnc.crossentropy_sgd_mom, lr=label_rate, mom=label_mom, target_var=target_var,\n",
    "           regularization=None, reg_param=.1)\n",
    "nn.compile(nnc.crossentropy_validation, target_var=target_var)\n",
    "nn.compile(nnc.output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the nn :\n",
    "# ==============\n",
    "# nn.train([test_dataset, ], ['main', ]);\n",
    "nn.train(test_dataset);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ================\n",
    "# Learning curve\n",
    "# ================\n",
    "fig, ax = visual.learning_curve(nn.global_stats, regex='loss')\n",
    "#     SAVE\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(fig_title+'-Learning_curve.png',bbox_inches='tight')\n",
    "fig.show()\n",
    "# visual.learning_curve(final_stats, regex='domain.* acc');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in range(len(np.unique(test_dataset.y_test))):\n",
    "    visual.bound(test_dataset.X_test, test_dataset.y_test, nn.funs.output, class_idx=c);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Data](#Load-datasets)\n",
    "    - [1.1 Loading of datasets](#Load-datasets)\n",
    "    - [1.2 Transformation of datasets](#Transform-datasets)\n",
    "- [2. Clusters](#Clusters)\n",
    "- [3. Optimal transport](#Optimal-Transport)\n",
    "    - [3.1 Optimal transport solver](#Optimal-Transport)\n",
    "    - [3.2 Align](#Align)\n",
    "    - [3.3 Cost matrix](#Cost-matrix)\n",
    "- [4. Neural Networks](#Neural-Networks)\n",
    "    - [4.1 Special blocks](#Special-blocks)\n",
    "    - [4.2 Compiler](#Compiler)\n",
    "    - [4.3 CNN class](#Neural-Network-class)\n",
    "- [5. Examples](#Examples)\n",
    "    - [5.1 Classification](#Classification)\n",
    "    - [5.2 Regression](#Regression)\n",
    "    - [5.3 MNIST](#MNIST)\n",
    "    - [5.4 DANN](#DANN)\n",
    "    - [5.5 Dual proba classification](#Dual-proba-classification)\n",
    "    - [5.6 One loop EMANN](#One-loop-EMANN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST\n",
    "\n",
    "Here is illustrated the use of convolution layers for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Data](#Load-datasets)\n",
    "    - [1.1 Loading of datasets](#Load-datasets)\n",
    "    - [1.2 Transformation of datasets](#Transform-datasets)\n",
    "- [2. Clusters](#Clusters)\n",
    "- [3. Optimal transport](#Optimal-Transport)\n",
    "    - [3.1 Optimal transport solver](#Optimal-Transport)\n",
    "    - [3.2 Align](#Align)\n",
    "    - [3.3 Cost matrix](#Cost-matrix)\n",
    "- [4. Neural Networks](#Neural-Networks)\n",
    "    - [4.1 Special blocks](#Special-blocks)\n",
    "    - [4.2 Compiler](#Compiler)\n",
    "    - [4.3 CNN class](#Neural-Network-class)\n",
    "- [5. Examples](#Examples)\n",
    "    - [5.1 Classification](#Classification)\n",
    "    - [5.2 Regression](#Regression)\n",
    "    - [5.3 MNIST](#MNIST)\n",
    "    - [5.4 DANN](#DANN)\n",
    "    - [5.5 Dual proba classification](#Dual-proba-classification)\n",
    "    - [5.6 One loop EMANN](#One-loop-EMANN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batchsize = 1000\n",
    "n_classes = 10\n",
    "mnist_dataset = make_dataset(*load_mnist(), batchsize=batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(mnist_dataset['X_test'][60].reshape(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get general information :\n",
    "# =========================\n",
    "_shape = np.shape(mnist_dataset['X_train'])\n",
    "n_dim = len(_shape)\n",
    "n_features = np.prod(_shape[1:])\n",
    "\n",
    "shape = (batchsize,) + _shape[1:]\n",
    "target_var = T.ivector('targets')\n",
    "\n",
    "# Logs\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n",
    "\n",
    "# Build the layers :\n",
    "# ==================\n",
    "# Build the layers\n",
    "input_layer = lasagne.layers.InputLayer(shape=shape)\n",
    "\n",
    "dropout_1 = lasagne.layers.DropoutLayer(input_layer)\n",
    "conv_1 = lasagne.layers.Conv2DLayer(\n",
    "                dropout_1, 5, (5,5),\n",
    "                nonlinearity=lasagne.nonlinearities.rectify,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n",
    "pool_1 = lasagne.layers.MaxPool2DLayer(conv_1, (2,2))\n",
    "dropout_2 = lasagne.layers.DropoutLayer(pool_1)\n",
    "conv_2 = lasagne.layers.Conv2DLayer(\n",
    "                dropout_2, 5, (3,3),\n",
    "                nonlinearity=lasagne.nonlinearities.rectify,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n",
    "pool_2 = lasagne.layers.MaxPool2DLayer(conv_2, (2,2))\n",
    "dense_1 = lasagne.layers.DenseLayer(pool_2, 30)\n",
    "softmax_layer = lasagne.layers.DenseLayer(\n",
    "                dense_1,\n",
    "                num_units=n_classes,\n",
    "                nonlinearity=lasagne.nonlinearities.softmax,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n",
    "\n",
    "# Instanciate the NN :\n",
    "# ====================\n",
    "nn = NN(softmax_layer, name='MNIST test')\n",
    "\n",
    "# Compile :\n",
    "# =========\n",
    "nn.compile(nnc.classification_sgd_mom, lr=0.01, mom=0.9, regularization=None, reg_param=.1)\n",
    "nn.compile(nnc.classification_validation)\n",
    "nn.compile(nnc.output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the nn :\n",
    "# ==============\n",
    "nn.train(mnist_dataset, num_epochs=40);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ================\n",
    "# Learning curve\n",
    "# ================\n",
    "fig, ax = visual.learning_curve(nn.global_stats, regex='acc')\n",
    "#     SAVE\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(fig_title+'-Learning_curve.png',bbox_inches='tight')\n",
    "fig.show()\n",
    "# visual.learning_curve(final_stats, regex='domain.* acc');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Data](#Load-datasets)\n",
    "    - [1.1 Loading of datasets](#Load-datasets)\n",
    "    - [1.2 Transformation of datasets](#Transform-datasets)\n",
    "- [2. Clusters](#Clusters)\n",
    "- [3. Optimal transport](#Optimal-Transport)\n",
    "    - [3.1 Optimal transport solver](#Optimal-Transport)\n",
    "    - [3.2 Align](#Align)\n",
    "    - [3.3 Cost matrix](#Cost-matrix)\n",
    "- [4. Neural Networks](#Neural-Networks)\n",
    "    - [4.1 Special blocks](#Special-blocks)\n",
    "    - [4.2 Compiler](#Compiler)\n",
    "    - [4.3 CNN class](#Neural-Network-class)\n",
    "- [5. Examples](#Examples)\n",
    "    - [5.1 Classification](#Classification)\n",
    "    - [5.2 Regression](#Regression)\n",
    "    - [5.3 MNIST](#MNIST)\n",
    "    - [5.4 DANN](#DANN)\n",
    "    - [5.5 Dual proba classification](#Dual-proba-classification)\n",
    "    - [5.6 One loop EMANN](#One-loop-EMANN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_classes = 3\n",
    "n_samples = 1000\n",
    "# X_src, y_src = make_clouds(n_samples=n_samples, n_classes=n_classes)\n",
    "X_src, y_src = make_moons(n_samples=n_samples)\n",
    "src_dataset = make_dataset(X_src, y_src, batchsize=60)\n",
    "\n",
    "X_tgt, y_tgt = transform.rotate(X_src, y_src, angle=-35.)\n",
    "tgt_dataset =  make_dataset(X_tgt, y_tgt, batchsize=60)\n",
    "\n",
    "domain_dataset = make_domain_dataset([src_dataset, tgt_dataset])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nn.clone import clone_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get general information :\n",
    "# =========================\n",
    "X = src_dataset.X_train\n",
    "batchsize = src_dataset.batchsize\n",
    "_shape = np.shape(X)\n",
    "n_dim = len(_shape)\n",
    "n_features = np.prod(_shape[1:])\n",
    "\n",
    "shape = (batchsize,) + _shape[1:]\n",
    "target_var = T.ivector('targets')\n",
    "\n",
    "# Logs\n",
    "logger.info('Building the input and output variables for : {}'.format(data_name))\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n",
    "\n",
    "# Build the layers :\n",
    "# ==================\n",
    "# Build the layers\n",
    "input_layer = lasagne.layers.InputLayer(shape=shape)\n",
    "\n",
    "dense_1 = lasagne.layers.DenseLayer(\n",
    "                input_layer,\n",
    "                num_units=3,\n",
    "                nonlinearity=lasagne.nonlinearities.sigmoid,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n",
    "softmax_layer = lasagne.layers.DenseLayer(\n",
    "                dense_1,\n",
    "                num_units=n_classes,\n",
    "                nonlinearity=lasagne.nonlinearities.softmax,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n",
    "\n",
    "# Instanciate the NN :\n",
    "# ====================\n",
    "\n",
    "nn = CNN(name='Moons test')\n",
    "nn.add_output('main', softmax_layer)\n",
    "nn.add_output('adversarial', [dense_1, \n",
    "                              clone_layer(dense_1, input_layer=lasagne.layers.InputLayer(shape=shape))])\n",
    "\n",
    "# Compile :\n",
    "# =========\n",
    "nn.compile('main', nnc.classification_sgd_mom)\n",
    "nn.compile('main', nnc.classification_validation)\n",
    "nn.compile('main', nnc.output)\n",
    "nn.compile('adversarial', nnc.adversarial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the nn :\n",
    "# ==============\n",
    "nn.train([src_dataset, domain_dataset], ['main', 'adversarial'], num_epochs=60);\n",
    "# nn.train([src_dataset,], ['main',], num_epochs=60);\n",
    "# nn.train([domain_dataset,], ['adversarial',], num_epochs=60);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ================\n",
    "# Learning curve\n",
    "# ================\n",
    "fig, ax = visual.learning_curve(nn.global_stats, regex='loss')\n",
    "#     SAVE\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(fig_title+'-Learning_curve.png',bbox_inches='tight')\n",
    "fig.show()\n",
    "# visual.learning_curve(final_stats, regex='domain.* acc');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in range(len(np.unique(src_dataset.y_test))):\n",
    "    visual.bound(src_dataset.X_test, src_dataset.y_test, nn['main'].output, class_idx=c);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in range(len(np.unique(src_dataset.y_test))):\n",
    "    visual.bound(src_dataset.X_test, src_dataset.y_test, nn['main'].output, class_idx=c);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in range(len(np.unique(tgt_dataset.y_test))):\n",
    "    visual.bound(tgt_dataset.X_test, tgt_dataset.y_test, nn['main'].output, class_idx=c);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in range(len(np.unique(tgt_dataset.y_test))):\n",
    "    visual.bound(tgt_dataset.X_test, tgt_dataset.y_test, nn['main'].output, class_idx=c);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual proba classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Data](#Load-datasets)\n",
    "    - [1.1 Loading of datasets](#Load-datasets)\n",
    "    - [1.2 Transformation of datasets](#Transform-datasets)\n",
    "- [2. Clusters](#Clusters)\n",
    "- [3. Optimal transport](#Optimal-Transport)\n",
    "    - [3.1 Optimal transport solver](#Optimal-Transport)\n",
    "    - [3.2 Align](#Align)\n",
    "    - [3.3 Cost matrix](#Cost-matrix)\n",
    "- [4. Neural Networks](#Neural-Networks)\n",
    "    - [4.1 Special blocks](#Special-blocks)\n",
    "    - [4.2 Compiler](#Compiler)\n",
    "    - [4.3 CNN class](#Neural-Network-class)\n",
    "- [5. Examples](#Examples)\n",
    "    - [5.1 Classification](#Classification)\n",
    "    - [5.2 Regression](#Regression)\n",
    "    - [5.3 MNIST](#MNIST)\n",
    "    - [5.4 DANN](#DANN)\n",
    "    - [5.5 Dual proba classification](#Dual-proba-classification)\n",
    "    - [5.6 One loop EMANN](#One-loop-EMANN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One loop EMANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Data](#Load-datasets)\n",
    "    - [1.1 Loading of datasets](#Load-datasets)\n",
    "    - [1.2 Transformation of datasets](#Transform-datasets)\n",
    "- [2. Clusters](#Clusters)\n",
    "- [3. Optimal transport](#Optimal-Transport)\n",
    "    - [3.1 Optimal transport solver](#Optimal-Transport)\n",
    "    - [3.2 Align](#Align)\n",
    "    - [3.3 Cost matrix](#Cost-matrix)\n",
    "- [4. Neural Networks](#Neural-Networks)\n",
    "    - [4.1 Special blocks](#Special-blocks)\n",
    "    - [4.2 Compiler](#Compiler)\n",
    "    - [4.3 CNN class](#Neural-Network-class)\n",
    "- [5. Examples](#Examples)\n",
    "    - [5.1 Classification](#Classification)\n",
    "    - [5.2 Regression](#Regression)\n",
    "    - [5.3 MNIST](#MNIST)\n",
    "    - [5.4 DANN](#DANN)\n",
    "    - [5.5 Dual proba classification](#Dual-proba-classification)\n",
    "    - [5.6 One loop EMANN](#One-loop-EMANN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
