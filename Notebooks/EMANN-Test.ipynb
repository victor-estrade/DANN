{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import sys\n",
    "if '..' not in sys.path:\n",
    "    sys.path.insert(0, '..')\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The objective here is to do some unit testing on every function and blocks of the EMANN method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets\n",
    "\n",
    "- the datasets are loaded/built.\n",
    "- The batchsize is defined\n",
    "- half of the data name (the source part) is defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Loading of datasets](#Load-datasets)\n",
    "- [2. Transformation of datasets](#Transform-datasets)\n",
    "- [3. Clusters init](#Clusters-Init)\n",
    "- [4. Optimal transport init](#Optimal-Transport-Init)\n",
    "- [5. Align](#Align)\n",
    "- [6. Neural Network](#Neural-Network)\n",
    "- [7. Compiler](#Compiler)\n",
    "- [8. NN class](#Neural-Network-class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.toys import make_clouds, make_circles, make_X, make_moons\n",
    "from datasets.utils import make_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform datasets\n",
    "\n",
    "- the transformed datasets are built.\n",
    "- last part of the data name (the target part) is defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- [1. Loading of datasets](#Load-datasets)\n",
    "- [2. Transformation of datasets](#Transform-datasets)\n",
    "- [3. Clusters init](#Clusters-Init)\n",
    "- [4. Optimal transport init](#Optimal-Transport-Init)\n",
    "- [5. Align](#Align)\n",
    "- [6. Neural Network](#Neural-Network)\n",
    "- [7. Compiler](#Compiler)\n",
    "- [8. NN class](#Neural-Network-class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.utils import make_domain_dataset, make_corrector_dataset\n",
    "import datasets.transform as transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusters Init\n",
    "Here we initialize the clusters on the *source* and on the *target* domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Loading of datasets](#Load-datasets)\n",
    "- [2. Transformation of datasets](#Transform-datasets)\n",
    "- [3. Clusters init](#Clusters-Init)\n",
    "- [4. Optimal transport init](#Optimal-Transport-Init)\n",
    "- [5. Align](#Align)\n",
    "- [6. Neural Network](#Neural-Network)\n",
    "- [7. Compiler](#Compiler)\n",
    "- [8. NN class](#Neural-Network-class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_src, y_src = make_clouds(n_samples=50, n_classes=6)\n",
    "X_tgt, y_tgt = make_moons(n_samples=500)\n",
    "data_name = \"Clouds-to-moons\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k_src = 10\n",
    "k_tgt = 11\n",
    "k_means_src = KMeans(n_clusters=k_src).fit(X_src)\n",
    "k_means_tgt = KMeans(n_clusters=k_tgt).fit(X_tgt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mass(k_means):\n",
    "    \"\"\"\n",
    "    Params\n",
    "    ------\n",
    "        k_means: (sklearn.cluster.KMeans instance)\n",
    "    Return\n",
    "    ------\n",
    "        w: (numpy.array [n_clusters]) the mass of each clusters \n",
    "    \"\"\"\n",
    "    w = np.unique(k_means.labels_, return_counts=True)[1]\n",
    "    w = w/np.sum(w)\n",
    "    return w\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_src = mass(k_means_src)\n",
    "# w_src = np.abs(np.sin(-w_src-np.arange(w_src.shape[0])))\n",
    "# w_src /= np.sum(w_src)\n",
    "\n",
    "w_tgt = mass(k_means_tgt)\n",
    "w_tgt = np.abs(np.sin(-w_tgt-0.6*np.arange(w_tgt.shape[0])))\n",
    "# w_tgt = np.exp(-w_tgt-np.arange(w_tgt.shape[0]))\n",
    "w_tgt /= np.sum(w_tgt)\n",
    "cost_mat = np.random.uniform(0,1, size=(w_src.shape[0], w_tgt.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import visual\n",
    "visual.mat(cost_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Transport Init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Loading of datasets](#Load-datasets)\n",
    "- [2. Transformation of datasets](#Transform-datasets)\n",
    "- [3. Clusters init](#Clusters-Init)\n",
    "- [4. Optimal transport init](#Optimal-Transport-Init)\n",
    "- [5. Align](#Align)\n",
    "- [6. Neural Network](#Neural-Network)\n",
    "- [7. Compiler](#Compiler)\n",
    "- [8. NN class](#Neural-Network-class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from opt_transport import opt_transp_sup, computeTransportSinkhorn, computeTransportSinkhornLabelsLpL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transp = opt_transp_sup(k_means_src.cluster_centers_, k_means_tgt.cluster_centers_)\n",
    "transp = computeTransportSinkhorn(w_src, w_tgt, cost_mat, reg=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visual.mat(transp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Align"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Loading of datasets](#Load-datasets)\n",
    "- [2. Transformation of datasets](#Transform-datasets)\n",
    "- [3. Clusters init](#Clusters-Init)\n",
    "- [4. Optimal transport init](#Optimal-Transport-Init)\n",
    "- [5. Align](#Align)\n",
    "- [6. Neural Network](#Neural-Network)\n",
    "- [7. Compiler](#Compiler)\n",
    "- [8. NN class](#Neural-Network-class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transp.shape, X_src.shape, k_means_src.labels_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from align_learn.preprocess import align"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check. The cluster distribution of the aligned data should be the same as the target distrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "align_idx, cluster_T = align(transp, k_means_src.labels_, k_means_tgt.labels_)\n",
    "# print(np.unique(res).shape, X_src.shape, X_tgt.shape)\n",
    "uniq, count = np.unique(cluster_T, return_counts=True)\n",
    "# plt.plot(np.sum(transp,0), label='transp.sum(0)')\n",
    "plt.plot(w_tgt, label='w_tgt')\n",
    "# plt.plot(np.sum(transp,1), label='transp.sum(1)')\n",
    "plt.plot(w_src, label='w_src')\n",
    "plt.plot(count/cluster_T.shape[0], label='mapping')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_S, y_S = X_src, y_src\n",
    "X_T, y_T = X_tgt[align_idx], y_tgt[align_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visual.target_2D(X_T, y_T)\n",
    "visual.target_2D(X_tgt, y_tgt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_T.shape, X_S.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the probabilities to be predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_dataset(X_S, X_T, k_means_src, k_means_tgt, transp, align_idx, cluster_T):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Build the probabilities to be predict\n",
    "    # For the source data\n",
    "    proba_src = np.zeros((X_S.shape[0], k_means_src.n_clusters))\n",
    "    proba_src[np.arange(X_S.shape[0]), k_means_src.labels_] = 1.\n",
    "    proba_tgt = transp[k_means_src.labels_]\n",
    "    Y_S = np.hstack([proba_src, proba_tgt])\n",
    "    \n",
    "    # Build the probabilities to be predict\n",
    "    # For the aligned target data\n",
    "    proba_tgt = np.zeros((X_T.shape[0], k_means_tgt.n_clusters))\n",
    "    proba_tgt[np.arange(X_T.shape[0]), cluster_T] = 1.\n",
    "    proba_src = transp[:, cluster_T].T\n",
    "    Y_T = np.hstack([proba_src, proba_tgt])\n",
    "    \n",
    "    Y = np.vstack([Y_S, Y_T])\n",
    "    X = np.vstack([X_S, X_T])\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, Y = train_dataset(X_S, X_T, k_means_src, k_means_tgt, transp, align_idx, cluster_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(Y[0])\n",
    "print(Y[350])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Loading of datasets](#Load-datasets)\n",
    "- [2. Transformation of datasets](#Transform-datasets)\n",
    "- [3. Clusters init](#Clusters-Init)\n",
    "- [4. Optimal transport init](#Optimal-Transport-Init)\n",
    "- [5. Align](#Align)\n",
    "- [6. Neural Network](#Neural-Network)\n",
    "- [7. Compiler](#Compiler)\n",
    "- [8. NN class](#Neural-Network-class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nn.rgl import ReverseGradientLayer\n",
    "from nn.compilers import crossentropy_sgd_mom, squared_error_sgd_mom, adversarial\n",
    "from nn.training import Trainner, training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from logs import log_fname, new_logger, empty_logger\n",
    "\n",
    "hp_lambda = 0.\n",
    "batchsize = 20\n",
    "\n",
    "# Learning rates and momentums\n",
    "label_rate = 0.1\n",
    "label_mom = 0.9\n",
    "\n",
    "domain_rate = 0.1\n",
    "domain_mom = 0.9\n",
    "\n",
    "# Get a logger\n",
    "logger = new_logger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_shape = np.shape(X)\n",
    "n_dim = len(_shape)\n",
    "n_features = np.prod(_shape[1:])\n",
    "\n",
    "shape = (batchsize,) + _shape[1:]\n",
    "target_var = T.matrix('targets')\n",
    "\n",
    "# Logs\n",
    "logger.info('Building the input and output variables for : {}'.format(data_name))\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the layers\n",
    "input_layer = lasagne.layers.InputLayer(shape=shape)\n",
    "\n",
    "dense_1 = lasagne.layers.DenseLayer(\n",
    "                input_layer,\n",
    "                num_units=25,\n",
    "                nonlinearity=lasagne.nonlinearities.rectify,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proba_chain(last_layer, k_src, k_tgt):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    proba_src = lasagne.layers.DenseLayer(\n",
    "                last_layer,\n",
    "                num_units=k_src,\n",
    "                nonlinearity=lasagne.nonlinearities.softmax,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n",
    "    proba_tgt = lasagne.layers.DenseLayer(\n",
    "                last_layer,\n",
    "                num_units=k_tgt,\n",
    "                nonlinearity=lasagne.nonlinearities.softmax,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n",
    "    concat_layer = lasagne.layers.ConcatLayer([proba_src, proba_tgt], axis=1)\n",
    "    return concat_layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiler\n",
    "Append the last part and compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Loading of datasets](#Load-datasets)\n",
    "- [2. Transformation of datasets](#Transform-datasets)\n",
    "- [3. Clusters init](#Clusters-Init)\n",
    "- [4. Optimal transport init](#Optimal-Transport-Init)\n",
    "- [5. Align](#Align)\n",
    "- [6. Neural Network](#Neural-Network)\n",
    "- [7. Compiler](#Compiler)\n",
    "- [8. NN class](#Neural-Network-class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compiler(output_layer, lr=1, mom=.9, target_var=T.ivector('target'),\n",
    "                        regularization=None, reg_param=0.1): \n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent compiler with optionnal momentum.\n",
    "\n",
    "    info: it uses the categorical_crossentropy. Should be given to a softmax layer.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "        output_layer: the output layer from which the loss and updtaes will be computed\n",
    "        lr: (default=1) learning rate.\n",
    "        mom: (default=0.9) momentum.\n",
    "        regularisation: (default=None) the regularization, can be 'l1' or 'l2' or None.\n",
    "        reg_param: (default=0.1) the regularization hyper parameter: \n",
    "                        loss = loss + reg_param * regularization\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "        A dictionnary with :\n",
    "            -train : function used to train the neural network\n",
    "            'train_desription': ('loss',),\n",
    "            -predict : function used to predict the label\n",
    "            'predict_desription': ('prediction',),\n",
    "            -valid : function used to get the accuracy and loss \n",
    "            'valid_desription': ('loss',),\n",
    "            -output : function used to get the output (exm: predict the label probabilities)\n",
    "            'output_desription': ('prediction',)\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> funs = compiler_sgd_mom(output_layer, lr=0.01, mom=0.1)\n",
    "    >>> loss, acc = funs.train(X, y)\n",
    "    \n",
    "    \"\"\"    \n",
    "\n",
    "    input_var = lasagne.layers.get_all_layers(output_layer)[0].input_var\n",
    "    # Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "    pred = lasagne.layers.get_output(output_layer)\n",
    "    loss = T.mean(lasagne.objectives.categorical_crossentropy(pred, target_var))\n",
    "    # Add a regularization term to the loss if needed\n",
    "    if regularization == 'l1':\n",
    "        reg = lasagne.regularization.regularize_network_params(output_layer, lasagne.regularization.l1)\n",
    "        loss += reg_param*reg\n",
    "    elif regularization == 'l2':\n",
    "        reg = lasagne.regularization.regularize_network_params(output_layer, lasagne.regularization.l2)\n",
    "        loss += reg_param*reg\n",
    "    # Create update expressions for training, i.e., how to modify the\n",
    "    # parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "    # Descent and add a momentum to it.\n",
    "    params = lasagne.layers.get_all_params(output_layer, trainable=True)\n",
    "    updates = lasagne.updates.sgd(loss, params, learning_rate=lr)\n",
    "    updates = lasagne.updates.apply_momentum(updates, params, momentum=mom)\n",
    "\n",
    "    # Compile a function performing a training step on a mini-batch (by giving\n",
    "    # the updates dictionary) and returning the corresponding training loss:\n",
    "    train_function = theano.function([input_var, target_var], [loss,], \n",
    "        updates=updates, allow_input_downcast=True)\n",
    "\n",
    "    # Create a loss expression for validation/testing. The crucial difference\n",
    "    # here is that we do a deterministic forward pass through the network,\n",
    "    # disabling dropout and noise layers.\n",
    "    pred = lasagne.layers.get_output(output_layer, deterministic=True)\n",
    "    loss = T.mean(lasagne.objectives.categorical_crossentropy(pred, target_var))\n",
    "    # Compile a second function computing the validation loss and accuracy:\n",
    "    valid_function = theano.function([input_var, target_var], [loss,], allow_input_downcast=True)\n",
    "    # Compile a function computing the predicted labels:\n",
    "    predict_function = theano.function([input_var], [pred], allow_input_downcast=True)\n",
    "    # Compile an output function\n",
    "    output_function = theano.function([input_var], [pred], allow_input_downcast=True)\n",
    "\n",
    "    return {\n",
    "            'train': train_function,\n",
    "            'train_description': ('loss',),\n",
    "            'predict': predict_function,\n",
    "            'predict_description': ('prediction',),\n",
    "            'valid': valid_function,\n",
    "            'valid_description': ('loss',),\n",
    "            'output': output_function,\n",
    "            'output_description': ('prediction',),\n",
    "           }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_layer = proba_chain(dense_1, k_src, k_tgt)\n",
    "funs = compiler(predict_layer, lr=label_rate, mom=label_mom, target_var=T.matrix('targets'))\n",
    "proba_nn = Trainner(funs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_n_epoch(trainers, datas, n_epoch=1, epoch_counter=0, final_stats={}):\n",
    "    epoch_counter += n_epoch\n",
    "    # Now do the trainning part !\n",
    "    logger.info('Trainning the neural network for {} additional epochs ({} total)'.format(n_epoch, epoch_counter))\n",
    "    stats = training(trainers, datas, num_epochs=n_epoch, logger=None)\n",
    "    final_stats = {k: (final_stats[k]+v if k in final_stats else v) for k, v in stats.items()}\n",
    "    return final_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainers = [proba_nn,]\n",
    "datas = [make_dataset(X, Y, batchsize=batchsize),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch_counter = 0\n",
    "final_stats = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_stats = do_n_epoch(trainers, datas, n_epoch=10, epoch_counter=epoch_counter, final_stats=final_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of the learning procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ================\n",
    "# Learning curve\n",
    "# ================\n",
    "fig, ax = visual.learning_curve(final_stats, regex='loss')\n",
    "#     SAVE\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(fig_title+'-Learning_curve.png',bbox_inches='tight')\n",
    "fig.show()\n",
    "# visual.learning_curve(final_stats, regex='domain.* acc');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Neural Network class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Loading of datasets](#Load-datasets)\n",
    "- [2. Transformation of datasets](#Transform-datasets)\n",
    "- [3. Clusters init](#Clusters-Init)\n",
    "- [4. Optimal transport init](#Optimal-Transport-Init)\n",
    "- [5. Align](#Align)\n",
    "- [6. Neural Network](#Neural-Network)\n",
    "- [7. Compiler](#Compiler)\n",
    "- [8. NN class](#Neural-Network-class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nn.helper import CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = CNN()\n",
    "nn['out'] = 'layer'\n",
    "def foo(string, **kwargs):\n",
    "    def bar():\n",
    "        print('compile('+', '.join([string]+kwargs.keys())+')')\n",
    "    return {'bar': bar}\n",
    "nn.compile('out', foo, kwargs1=8, kwargs2='bla')\n",
    "nn['out'].bar()\n",
    "print('OK')\n",
    "nn['out']['bar']()\n",
    "print('OK')\n",
    "nn.parts.out.bar()\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_classes = 3\n",
    "n_samples = 1000\n",
    "test_dataset = make_dataset(*make_clouds(n_samples=n_samples, n_classes=n_classes), batchsize=60)\n",
    "# test_dataset = make_dataset(*make_moons(n_samples=n_samples), batchsize=60)\n",
    "test_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get general information :\n",
    "# =========================\n",
    "X = test_dataset.X_train\n",
    "_shape = np.shape(X)\n",
    "n_dim = len(_shape)\n",
    "n_features = np.prod(_shape[1:])\n",
    "\n",
    "shape = (batchsize,) + _shape[1:]\n",
    "target_var = T.ivector('targets')\n",
    "\n",
    "# Logs\n",
    "logger.info('Building the input and output variables for : {}'.format(data_name))\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n",
    "\n",
    "# Build the layers :\n",
    "# ==================\n",
    "# Build the layers\n",
    "input_layer = lasagne.layers.InputLayer(shape=shape)\n",
    "\n",
    "dense_1 = lasagne.layers.DenseLayer(\n",
    "                input_layer,\n",
    "                num_units=30,\n",
    "                nonlinearity=lasagne.nonlinearities.tanh,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n",
    "softmax_layer = lasagne.layers.DenseLayer(\n",
    "                dense_1,\n",
    "                num_units=n_classes,\n",
    "                nonlinearity=lasagne.nonlinearities.softmax,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n",
    "\n",
    "# Instanciate the NN :\n",
    "# ====================\n",
    "\n",
    "nn = CNN(name='Moons test')\n",
    "nn.add_output('main', softmax_layer)\n",
    "\n",
    "# Compile :\n",
    "# =========\n",
    "nn.compile('main', compiler)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the nn :\n",
    "# ==============\n",
    "nn.train([test_dataset, ], ['main', ]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ================\n",
    "# Learning curve\n",
    "# ================\n",
    "fig, ax = visual.learning_curve(nn.global_stats, regex='loss')\n",
    "#     SAVE\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(fig_title+'-Learning_curve.png',bbox_inches='tight')\n",
    "fig.show()\n",
    "# visual.learning_curve(final_stats, regex='domain.* acc');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in range(len(np.unique(test_dataset.y_test))):\n",
    "    visual.bound(test_dataset.X_test, test_dataset.y_test, nn['main'].output, class_idx=c);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
