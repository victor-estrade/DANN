{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The objective here is to do some unit testing on every function and blocks of the EMANN method.\n",
    "Moreover this we serve as example code of how to do things and use the functions + reminder of why I have coded this way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Data](#Load-datasets)\n",
    "    - [1.1 Loading of datasets](#Load-datasets)\n",
    "    - [1.2 Transformation of datasets](#Transform-datasets)\n",
    "- [2. Clusters](#Clusters)\n",
    "- [3. Optimal transport](#Optimal-Transport)\n",
    "    - [3.1 Optimal transport solver](#Optimal-Transport)\n",
    "    - [3.2 Align](#Align)\n",
    "    - [3.3 Cost matrix](#Cost-matrix)\n",
    "- [4. Neural Networks](#Neural-Networks)\n",
    "    - [4.1 Special blocks](#Special-blocks)\n",
    "    - [4.2 Compiler](#Compiler)\n",
    "    - [4.3 CNN class](#Neural-Network-class)\n",
    "- [5. Examples](#Examples)\n",
    "    - [5.1 Classification](#Classification)\n",
    "    - [5.2 Regression](#Regression)\n",
    "    - [5.3 MNIST](#MNIST)\n",
    "    - [5.4 DANN](#DANN)\n",
    "    - [5.5 Dual proba classification](#Dual-proba-classification)\n",
    "    - [5.6 Bi classification](#Bi-classification)\n",
    "    - [5.7 One loop EMANN](#One-loop-EMANN)\n",
    "- [6. ReGenerate Data](#ReGenerate-Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import sys\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import visual\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets\n",
    "\n",
    "How to load/generate and build Datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.toys import make_clouds, make_circles, make_X, make_moons\n",
    "from datasets.mnist import load_mnist\n",
    "from datasets.utils import make_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.utils import make_domain_dataset, make_corrector_dataset\n",
    "from datasets import transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusters\n",
    "\n",
    "Here we test and illustrate the use of sci-kit learn's KMeans in our problem (Optimal transport)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_src, y_src = make_clouds(n_samples=50, n_classes=6)\n",
    "X_tgt, y_tgt = make_moons(n_samples=500)\n",
    "data_name = \"Clouds-to-moons\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k_src = 10\n",
    "k_tgt = 11\n",
    "# We do not need to have the same number of cluster in the source and target data.\n",
    "k_means_src = KMeans(n_clusters=k_src).fit(X_src)\n",
    "k_means_tgt = KMeans(n_clusters=k_tgt).fit(X_tgt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mass(k_means):\n",
    "    \"\"\"\n",
    "    Params\n",
    "    ------\n",
    "        k_means: (sklearn.cluster.KMeans instance) should be trained\n",
    "    Return\n",
    "    ------\n",
    "        w: (numpy.array [n_clusters]) the mass of each clusters \n",
    "    \"\"\"\n",
    "    w = np.unique(k_means.labels_, return_counts=True)[1]\n",
    "    w = w/np.sum(w)\n",
    "    return w\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now the *mass computation* is very simple. Maybe we can improve the results by taking labels or other things into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_src = mass(k_means_src)\n",
    "# w_src = np.abs(np.sin(-w_src-np.arange(w_src.shape[0])))\n",
    "# w_src /= np.sum(w_src)\n",
    "\n",
    "w_tgt = mass(k_means_tgt)\n",
    "# I modify the mass in order to test a custom final distrib of the data in the coupling dataset\n",
    "w_tgt = np.abs(np.sin(-w_tgt-0.6*np.arange(w_tgt.shape[0])))\n",
    "# w_tgt = np.exp(-w_tgt-np.arange(w_tgt.shape[0]))\n",
    "w_tgt /= np.sum(w_tgt)\n",
    "cost_mat = np.random.uniform(0,1, size=(w_src.shape[0], w_tgt.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visual.mat(cost_mat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now every thing is set up(masses and cost matrix). We can solve the optimal transport problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Transport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the solvers functions\n",
    "from opt_transport import opt_transp_sup, computeTransportSinkhorn, computeTransportSinkhornLabelsLpL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transp = opt_transp_sup(k_means_src.cluster_centers_, k_means_tgt.cluster_centers_)\n",
    "transp = computeTransportSinkhorn(w_src, w_tgt, cost_mat, reg=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visual.mat(transp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is very easy.\n",
    "\n",
    "TODO : Add more test and visualisation of the different transport solvers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Align"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we build the datasets that will be fed to the NN thanks to the transport matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(transp.shape, X_src.shape, k_means_src.labels_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from align_learn.preprocess import align"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check. The cluster distribution of the **aligned data** should be the same as the **target distrib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "align_idx, cluster_T = align(transp, k_means_src.labels_, k_means_tgt.labels_)\n",
    "# print(np.unique(res).shape, X_src.shape, X_tgt.shape)\n",
    "uniq, count = np.unique(cluster_T, return_counts=True)\n",
    "# plt.plot(np.sum(transp,0), label='transp.sum(0)')\n",
    "plt.plot(w_tgt, label='w_tgt')\n",
    "# plt.plot(np.sum(transp,1), label='transp.sum(1)')\n",
    "plt.plot(w_src, label='w_src')\n",
    "plt.plot(count/cluster_T.shape[0], label='mapping')\n",
    "plt.legend(bbox_to_anchor=(1.25, 1.))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_S, y_S = X_src, y_src\n",
    "X_T, y_T = X_tgt[align_idx], y_tgt[align_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the next cell that there is a big hole in the coupling dataset. This is due to the target weights set near zero for one of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visual.target_2D(X_T, y_T)\n",
    "visual.target_2D(X_tgt, y_tgt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_T.shape, X_S.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the probabilities to be predict by the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_dataset(X_src, X_tgt, k_means_src, k_means_tgt, transp):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    align_idx, cluster_T = align(transp, k_means_src.labels_, k_means_tgt.labels_)\n",
    "    X_S, y_S = X_src, y_src\n",
    "    X_T, y_T = X_tgt[align_idx], y_tgt[align_idx]\n",
    "    # Build the probabilities to be predict\n",
    "    # For the source data\n",
    "    proba_src = np.zeros((X_S.shape[0], k_means_src.n_clusters))\n",
    "    proba_src[np.arange(X_S.shape[0]), k_means_src.labels_] = 1.\n",
    "    proba_tgt = transp[k_means_src.labels_]\n",
    "    proba_tgt = proba_tgt / np.sum(proba_tgt, 1).reshape(-1, 1)\n",
    "    Y_S = np.hstack([proba_src, proba_tgt])\n",
    "    \n",
    "    # Build the probabilities to be predict\n",
    "    # For the aligned target data\n",
    "    proba_tgt = np.zeros((X_T.shape[0], k_means_tgt.n_clusters))\n",
    "    proba_tgt[np.arange(X_T.shape[0]), cluster_T] = 1.\n",
    "    proba_src = transp[:, cluster_T].T\n",
    "    proba_src = proba_src / np.sum(proba_src, 1).reshape(-1, 1)\n",
    "    Y_T = np.hstack([proba_src, proba_tgt])\n",
    "    \n",
    "    Y = np.vstack([Y_S, Y_T])\n",
    "    X = np.vstack([X_S, X_T])\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, Y = train_dataset(X_src, X_tgt, k_means_src, k_means_tgt, transp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(Y[0])\n",
    "print(Y[350])\n",
    "print(np.sum(Y[350]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "Here will be tested and illustrated the neural network building, training and monitoring from the simple classification to an alternative training on a multiple output neural network.\n",
    "\n",
    "4 steps :\n",
    "1. Build the architecture (symbolic computation graph) with lasagne\n",
    "2. Compile the NN functions (train, valid, output, prediction, etc)\n",
    "3. Do forward backward training loop\n",
    "4. Plot the statistics and test the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "import nn.compilers as nnc\n",
    "import nn.block as nnb\n",
    "\n",
    "from nn.helper import CNN, NN\n",
    "from nn.rgl import ReverseGradientLayer\n",
    "from logs import log_fname, new_logger, empty_logger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special blocks\n",
    "\n",
    "Here we build the layers and introduce some function to build some special/frequently used structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the hyper parameters\n",
    "# ========================\n",
    "hp_lambda = 0.\n",
    "batchsize = 20\n",
    "\n",
    "# Learning rates and momentums\n",
    "label_rate = 0.1\n",
    "label_mom = 0.9\n",
    "\n",
    "domain_rate = 0.1\n",
    "domain_mom = 0.9\n",
    "\n",
    "# Get a logger\n",
    "logger = new_logger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute some data dependant params\n",
    "# ==================================\n",
    "_shape = np.shape(X)\n",
    "n_dim = len(_shape)\n",
    "n_features = np.prod(_shape[1:])\n",
    "\n",
    "shape = (batchsize,) + _shape[1:]\n",
    "target_var = T.matrix('targets')\n",
    "\n",
    "# Logs\n",
    "logger.info('Building the input and output variables for : {}'.format(data_name))\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the layers\n",
    "# ================\n",
    "input_layer = lasagne.layers.InputLayer(shape=shape)\n",
    "\n",
    "dense_1 = lasagne.layers.DenseLayer(\n",
    "                input_layer,\n",
    "                num_units=5,\n",
    "                nonlinearity=lasagne.nonlinearities.rectify,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n",
    "dense_2 = lasagne.layers.DenseLayer(\n",
    "                dense_1,\n",
    "                num_units=5,\n",
    "                nonlinearity=lasagne.nonlinearities.rectify,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n",
    "dense_3 = lasagne.layers.DenseLayer(\n",
    "                dense_2,\n",
    "                num_units=5,\n",
    "                nonlinearity=lasagne.nonlinearities.rectify,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Or we could use the dense block function\n",
    "dense_3 = nnb.dense(input_layer, [5,5,5], activation='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here is the way to get the inputs variables. Even in a complex structure the input must be a InputLayer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "concat_test = lasagne.layers.ConcatLayer([dense_1, dense_2, dense_3], axis=1)\n",
    "input_2 = lasagne.layers.InputLayer((20,2))\n",
    "concat_test_2 = lasagne.layers.ConcatLayer([concat_test, input_2], axis=1)\n",
    "dense_final = lasagne.layers.DenseLayer(concat_test_2, 5)\n",
    "lasagne.layers.get_all_layers(dense_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nn.compilers import get_input_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_variables = get_input_vars(dense_final)\n",
    "print(input_variables)\n",
    "print(input_variables[0] is input_variables[1])\n",
    "print(input_variables[0] == input_variables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiler\n",
    "Append the last part and compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the output layer :\n",
    "# ======================\n",
    "predict_layer = nnb.multi_proba(dense_1, [k_src, k_tgt])\n",
    "\n",
    "# Instanciate the NN :\n",
    "# ====================\n",
    "nn = NN(predict_layer, name='Special proba test')\n",
    "\n",
    "# Compile :\n",
    "# =========\n",
    "# nn.compile(compiler, lr=label_rate)\n",
    "nn.compile(nnc.crossentropy_sgd_mom, lr=label_rate, mom=label_mom, target_var=target_var,\n",
    "           regularization=None, reg_param=.1)\n",
    "nn.compile(nnc.crossentropy_validation, target_var=target_var)\n",
    "nn.compile(nnc.output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = make_dataset(X, Y, batchsize=batchsize) # Those X and Y are the coupling dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn.train(data, num_epochs=100)\n",
    "final_stats = nn.global_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of the learning procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ================\n",
    "# Learning curve\n",
    "# ================\n",
    "fig, ax = visual.learning_curve(final_stats, regex='loss')\n",
    "#     SAVE\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(fig_title+'-Learning_curve.png',bbox_inches='tight')\n",
    "fig.show()\n",
    "# visual.learning_curve(final_stats, regex='domain.* acc');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Neural Network class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Some sanity check on the working of AttributeDict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = CNN()\n",
    "nn['out'] = 'layer'\n",
    "def foo(string, **kwargs):\n",
    "    def bar():\n",
    "        print('compile('+', '.join([string]+kwargs.keys())+')')\n",
    "        print('OK')\n",
    "    return {'bar': bar}\n",
    "nn.compile('out', foo, kwargs1=8, kwargs2='bla')\n",
    "# TEST\n",
    "nn['out'].bar()\n",
    "nn['out']['bar']()\n",
    "nn.parts.out.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "\n",
    "Let's have some example code with the complete workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_classes = 5\n",
    "n_samples = 1000\n",
    "test_dataset = make_dataset(*make_clouds(n_samples=n_samples, n_classes=n_classes), batchsize=60)\n",
    "# test_dataset = make_dataset(*make_moons(n_samples=n_samples), batchsize=60)\n",
    "print('Dataset contains :', test_dataset.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get general information :\n",
    "# =========================\n",
    "X = test_dataset.X_train\n",
    "_shape = np.shape(X)\n",
    "n_dim = len(_shape)\n",
    "n_features = np.prod(_shape[1:])\n",
    "\n",
    "shape = (batchsize,) + _shape[1:]\n",
    "target_var = T.ivector('targets')\n",
    "\n",
    "# Logs\n",
    "logger.info('Building the input and output variables for : {}'.format(data_name))\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n",
    "\n",
    "# Build the layers :\n",
    "# ==================\n",
    "# Build the layers\n",
    "input_layer = lasagne.layers.InputLayer(shape=shape)\n",
    "\n",
    "dense_1 = lasagne.layers.DenseLayer(\n",
    "                input_layer,\n",
    "                num_units=3,\n",
    "                nonlinearity=lasagne.nonlinearities.tanh,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n",
    "softmax_layer = lasagne.layers.DenseLayer(\n",
    "                dense_1,\n",
    "                num_units=n_classes,\n",
    "                nonlinearity=lasagne.nonlinearities.softmax,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n",
    "\n",
    "# Instanciate the NN :\n",
    "# ====================\n",
    "\n",
    "# nn = CNN(name='Clouds test')\n",
    "nn = NN(softmax_layer, name='Clouds test')\n",
    "# nn.add_output('main', softmax_layer)\n",
    "\n",
    "# Compile :\n",
    "# =========\n",
    "nn.compile(nnc.crossentropy_sgd_mom, lr=label_rate, mom=label_mom, target_var=target_var,\n",
    "           regularization=None, reg_param=.1)\n",
    "nn.compile(nnc.crossentropy_validation, target_var=target_var)\n",
    "nn.compile(nnc.output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the nn :\n",
    "# ==============\n",
    "# nn.train([test_dataset, ], ['main', ]);\n",
    "nn.train(test_dataset);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ================\n",
    "# Learning curve\n",
    "# ================\n",
    "fig, ax = visual.learning_curve(nn.global_stats, regex='loss')\n",
    "#     SAVE\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(fig_title+'-Learning_curve.png',bbox_inches='tight')\n",
    "fig.show()\n",
    "# visual.learning_curve(final_stats, regex='domain.* acc');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in range(len(np.unique(test_dataset.y_test))):\n",
    "    visual.bound(test_dataset.X_test, test_dataset.y_test, nn.funs.output, class_idx=c);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST\n",
    "\n",
    "Here is illustrated the use of convolution layers for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batchsize = 1000\n",
    "n_classes = 10\n",
    "mnist_dataset = make_dataset(*load_mnist(), batchsize=batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(mnist_dataset['X_test'][60].reshape(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get general information :\n",
    "# =========================\n",
    "_shape = np.shape(mnist_dataset['X_train'])\n",
    "n_dim = len(_shape)\n",
    "n_features = np.prod(_shape[1:])\n",
    "\n",
    "shape = (batchsize,) + _shape[1:]\n",
    "target_var = T.ivector('targets')\n",
    "\n",
    "# Logs\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n",
    "\n",
    "# Build the layers :\n",
    "# ==================\n",
    "# Build the layers\n",
    "input_layer = lasagne.layers.InputLayer(shape=shape)\n",
    "\n",
    "dropout_1 = lasagne.layers.DropoutLayer(input_layer)\n",
    "conv_1 = lasagne.layers.Conv2DLayer(\n",
    "                dropout_1, 5, (5,5),\n",
    "                nonlinearity=lasagne.nonlinearities.rectify,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n",
    "pool_1 = lasagne.layers.MaxPool2DLayer(conv_1, (2,2))\n",
    "dropout_2 = lasagne.layers.DropoutLayer(pool_1)\n",
    "conv_2 = lasagne.layers.Conv2DLayer(\n",
    "                dropout_2, 5, (3,3),\n",
    "                nonlinearity=lasagne.nonlinearities.rectify,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n",
    "pool_2 = lasagne.layers.MaxPool2DLayer(conv_2, (2,2))\n",
    "dense_1 = lasagne.layers.DenseLayer(pool_2, 30)\n",
    "softmax_layer = lasagne.layers.DenseLayer(\n",
    "                dense_1,\n",
    "                num_units=n_classes,\n",
    "                nonlinearity=lasagne.nonlinearities.softmax,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n",
    "\n",
    "# Instanciate the NN :\n",
    "# ====================\n",
    "nn = NN(softmax_layer, name='MNIST test')\n",
    "\n",
    "# Compile :\n",
    "# =========\n",
    "nn.compile(nnc.classification_sgd_mom, lr=0.01, mom=0.9, regularization=None, reg_param=.1)\n",
    "nn.compile(nnc.classification_validation)\n",
    "nn.compile(nnc.output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the nn :\n",
    "# ==============\n",
    "nn.train(mnist_dataset, num_epochs=40);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ================\n",
    "# Learning curve\n",
    "# ================\n",
    "fig, ax = visual.learning_curve(nn.global_stats, regex='acc')\n",
    "#     SAVE\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(fig_title+'-Learning_curve.png',bbox_inches='tight')\n",
    "fig.show()\n",
    "# visual.learning_curve(final_stats, regex='domain.* acc');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_classes = 2\n",
    "n_samples = 1000\n",
    "# X_src, y_src = make_clouds(n_samples=n_samples, n_classes=n_classes)\n",
    "X_src, y_src = make_moons(n_samples=n_samples)\n",
    "src_dataset = make_dataset(X_src, y_src, batchsize=60)\n",
    "\n",
    "X_tgt, y_tgt = transform.rotate(X_src, y_src, angle=-45.)\n",
    "tgt_dataset =  make_dataset(X_tgt, y_tgt, batchsize=60)\n",
    "\n",
    "domain_dataset = make_domain_dataset([src_dataset, tgt_dataset])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nn.clone import clone_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get general information :\n",
    "# =========================\n",
    "X = src_dataset.X_train\n",
    "batchsize = src_dataset.batchsize\n",
    "_shape = np.shape(X)\n",
    "n_dim = len(_shape)\n",
    "n_features = np.prod(_shape[1:])\n",
    "\n",
    "shape = (batchsize,) + _shape[1:]\n",
    "target_var = T.ivector('targets')\n",
    "\n",
    "# Logs\n",
    "logger.info('Building the input and output variables for : {}'.format(data_name))\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n",
    "\n",
    "# Build the layers :\n",
    "# ==================\n",
    "# Build the layers\n",
    "input_layer = lasagne.layers.InputLayer(shape=shape)\n",
    "\n",
    "dense_1 = lasagne.layers.DenseLayer(\n",
    "                input_layer,\n",
    "                num_units=3,\n",
    "                nonlinearity=lasagne.nonlinearities.tanh,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n",
    "softmax_layer = lasagne.layers.DenseLayer(\n",
    "                dense_1,\n",
    "                num_units=n_classes,\n",
    "                nonlinearity=lasagne.nonlinearities.softmax,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instanciate the NN :\n",
    "# ====================\n",
    "\n",
    "nn = CNN(name='Moons test')\n",
    "nn.add_output('main', softmax_layer)\n",
    "nn.add_output('adversarial', [dense_1, \n",
    "                              clone_layer(dense_1, input_layer=lasagne.layers.InputLayer(shape=shape))])\n",
    "\n",
    "# Compile :\n",
    "# =========\n",
    "nn.compile('main', nnc.classification_sgd_mom, lr=0.1)\n",
    "nn.compile('main', nnc.classification_validation)\n",
    "nn.compile('main', nnc.output)\n",
    "nn.compile('adversarial', nnc.adversarial, lr=0.1, hp_lambda=0.8)\n",
    "\n",
    "# Train the nn :\n",
    "# ==============\n",
    "nn.train([src_dataset, domain_dataset], ['main', 'adversarial'], num_epochs=100);\n",
    "# nn.train([src_dataset,], ['main',], num_epochs=60);\n",
    "# nn.train([domain_dataset,], ['adversarial',], num_epochs=60);\n",
    "\n",
    "# ================\n",
    "# Learning curve\n",
    "# ================\n",
    "fig, ax = visual.learning_curve(nn.global_stats, regex='loss')\n",
    "#     SAVE\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(fig_title+'-Learning_curve.png',bbox_inches='tight')\n",
    "fig.show()\n",
    "# visual.learning_curve(final_stats, regex='domain.* acc');\n",
    "fig, ax = visual.learning_curve(nn.global_stats, regex='acc')\n",
    "#     SAVE\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(fig_title+'-Learning_curve.png',bbox_inches='tight')\n",
    "fig.show()\n",
    "# visual.learning_curve(final_stats, regex='domain.* acc');\n",
    "\n",
    "\n",
    "# Boundary\n",
    "# ========\n",
    "visual.bound(src_dataset.X_test, src_dataset.y_test, nn['main'].output);\n",
    "plt.show()\n",
    "visual.bound(tgt_dataset.X_test, tgt_dataset.y_test, nn['main'].output);\n",
    "plt.show()\n",
    "\n",
    "loss, acc = nn.parts['main'].valid(src_dataset.X_test, src_dataset.y_test)\n",
    "print('Source : loss = {}, acc = {}'.format(loss, acc))\n",
    "loss, acc = nn.parts['main'].valid(tgt_dataset.X_test, tgt_dataset.y_test)\n",
    "print('Target : loss = {}, acc = {}'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instanciate the NN :\n",
    "# ====================\n",
    "\n",
    "nn = CNN(name='Moons test')\n",
    "nn.add_output('main', softmax_layer)\n",
    "nn.add_output('adversarial', [dense_1, \n",
    "                              clone_layer(dense_1, input_layer=lasagne.layers.InputLayer(shape=shape))])\n",
    "\n",
    "# Compile :\n",
    "# =========\n",
    "nn.compile('main', nnc.classification_sgd_mom, lr=0.1)\n",
    "nn.compile('main', nnc.classification_validation)\n",
    "nn.compile('main', nnc.output)\n",
    "\n",
    "# Train the nn :\n",
    "# ==============\n",
    "nn.train([src_dataset], ['main'], num_epochs=100);\n",
    "\n",
    "# ================\n",
    "# Learning curve\n",
    "# ================\n",
    "fig, ax = visual.learning_curve(nn.global_stats, regex='loss')\n",
    "#     SAVE\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(fig_title+'-Learning_curve.png',bbox_inches='tight')\n",
    "fig.show()\n",
    "# visual.learning_curve(final_stats, regex='domain.* acc');\n",
    "fig, ax = visual.learning_curve(nn.global_stats, regex='acc')\n",
    "#     SAVE\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(fig_title+'-Learning_curve.png',bbox_inches='tight')\n",
    "fig.show()\n",
    "# visual.learning_curve(final_stats, regex='domain.* acc');\n",
    "\n",
    "# Boundary\n",
    "# ========\n",
    "visual.bound(src_dataset.X_test, src_dataset.y_test, nn['main'].output);\n",
    "plt.show()\n",
    "visual.bound(tgt_dataset.X_test, tgt_dataset.y_test, nn['main'].output);\n",
    "plt.show()\n",
    "\n",
    "loss, acc = nn.parts['main'].valid(src_dataset.X_test, src_dataset.y_test)\n",
    "print('Source : loss = {}, acc = {}'.format(loss, acc))\n",
    "loss, acc = nn.parts['main'].valid(tgt_dataset.X_test, tgt_dataset.y_test)\n",
    "print('Target : loss = {}, acc = {}'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual proba classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "n_classes = 3\n",
    "n_samples = 1000\n",
    "X, y = make_clouds(n_samples=n_samples, n_classes=n_classes)\n",
    "# X_src, y_src = make_moons(n_samples=n_samples)\n",
    "lb = LabelBinarizer()\n",
    "y_lb = lb.fit_transform(y)\n",
    "y_c = np.concatenate([y_lb, y_lb], axis=1)\n",
    "dataset = make_dataset(X, np.array(y_c, dtype=np.float32), batchsize=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(y.shape, X.shape, y_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get general information :\n",
    "# =========================\n",
    "batchsize = dataset.batchsize\n",
    "_shape = np.shape(dataset.X_train)\n",
    "n_dim = len(_shape)\n",
    "n_features = np.prod(_shape[1:])\n",
    "\n",
    "shape = (batchsize,) + _shape[1:]\n",
    "target_var = T.ivector('targets')\n",
    "\n",
    "# Logs\n",
    "logger.info('Building the input and output variables for : {}'.format(data_name))\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n",
    "\n",
    "# Build the layers :\n",
    "# ==================\n",
    "# Build the layers\n",
    "input_layer = lasagne.layers.InputLayer(shape=shape)\n",
    "\n",
    "dense_1 = lasagne.layers.DenseLayer(\n",
    "                input_layer,\n",
    "                num_units=3,\n",
    "                nonlinearity=lasagne.nonlinearities.sigmoid,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n",
    "\n",
    "\n",
    "end_layer = nnb.multi_proba(dense_1, [n_classes, n_classes])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instanciate the NN :\n",
    "# ====================\n",
    "nn = NN(end_layer, name='Dual test')\n",
    "\n",
    "# Compile :\n",
    "# =========\n",
    "nn.compile(nnc.crossentropy_sgd_mom, lr=0.1, mom=0.9)\n",
    "nn.compile(nnc.crossentropy_validation)\n",
    "nn.compile(nnc.output)\n",
    "\n",
    "# Train the nn :\n",
    "# ==============\n",
    "nn.train(dataset, num_epochs=100);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ================\n",
    "# Learning curve\n",
    "# ================\n",
    "fig, ax = visual.learning_curve(nn.global_stats, regex='loss')\n",
    "#     SAVE\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(fig_title+'-Learning_curve.png',bbox_inches='tight')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = nn.funs.output(dataset.X_test)[0]\n",
    "i = np.random.randint(0, dataset.X_test.shape[0])\n",
    "# print('\\n'.join('{:1.5f}--{:1.5f}'.format(pred, truth) for pred, truth in zip(y_pred[i], data.y_test[i])))\n",
    "width=0.4\n",
    "plt.bar(np.arange(n_classes+n_classes), y_pred[i], width, color='r')\n",
    "plt.bar(np.arange(n_classes+n_classes)+width, dataset.y_test[i], width, color='b')\n",
    "plt.title(\"One point distrib\")\n",
    "# plt.yscale('log')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset2 = make_dataset(X, y, batchsize=60)\n",
    "# Instanciate the NN :\n",
    "# ====================\n",
    "\n",
    "nn = CNN(name='Dual test')\n",
    "nn.add_output('01', lasagne.layers.DenseLayer(dense_1, n_classes, nonlinearity=lasagne.nonlinearities.softmax))\n",
    "nn.add_output('02', lasagne.layers.DenseLayer(dense_1, n_classes, nonlinearity=lasagne.nonlinearities.softmax))\n",
    "\n",
    "# Compile :\n",
    "# =========\n",
    "nn.compile('01', nnc.classification_sgd_mom, lr=0.1)\n",
    "nn.compile('02', nnc.classification_sgd_mom, lr=0.1)\n",
    "nn.compile('01', nnc.classification_validation)\n",
    "nn.compile('02', nnc.classification_validation)\n",
    "nn.compile('01', nnc.output)\n",
    "nn.compile('02', nnc.output)\n",
    "\n",
    "# Train the nn :\n",
    "# ==============\n",
    "nn.train([dataset2, dataset2], ['01', '02'], num_epochs=100);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ================\n",
    "# Learning curve\n",
    "# ================\n",
    "fig, ax = visual.learning_curve(nn.global_stats, regex='loss')\n",
    "#     SAVE\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(fig_title+'-Learning_curve.png',bbox_inches='tight')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_classes_1 = 4\n",
    "n_classes_2 = 4\n",
    "n_samples = 1000\n",
    "X_1, y_1 = make_clouds(n_samples=n_samples, n_classes=n_classes_1)\n",
    "X_2, y_2 = make_circles(n_samples=n_samples,  n_classes=n_classes_2)\n",
    "dataset_1 = make_dataset(X_1, y_1, batchsize=60)\n",
    "dataset_2 = make_dataset(X_2, y_2, batchsize=60)\n",
    "data_name='Clouds + Circles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get general information :\n",
    "# =========================\n",
    "batchsize = dataset.batchsize\n",
    "_shape = np.shape(dataset.X_train)\n",
    "n_dim = len(_shape)\n",
    "n_features = np.prod(_shape[1:])\n",
    "\n",
    "shape = (batchsize,) + _shape[1:]\n",
    "target_var = T.ivector('targets')\n",
    "\n",
    "# Logs\n",
    "logger.info('Building the input and output variables for : {}'.format(data_name))\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n",
    "\n",
    "# Build the layers :\n",
    "# ==================\n",
    "# Build the layers\n",
    "input_layer = lasagne.layers.InputLayer(shape=shape)\n",
    "\n",
    "dense_1 = lasagne.layers.DenseLayer(\n",
    "                input_layer,\n",
    "                num_units=3,\n",
    "                nonlinearity=lasagne.nonlinearities.sigmoid,\n",
    "                # W=lasagne.init.Uniform(range=0.01, std=None, mean=0.0),\n",
    "                )\n",
    "\n",
    "# Instanciate the NN :\n",
    "# ====================\n",
    "\n",
    "nn = CNN(name='Dual test')\n",
    "nn.add_output('01', lasagne.layers.DenseLayer(dense_1, n_classes_1, nonlinearity=lasagne.nonlinearities.softmax))\n",
    "nn.add_output('02', lasagne.layers.DenseLayer(dense_1, n_classes_2, nonlinearity=lasagne.nonlinearities.softmax))\n",
    "\n",
    "# Compile :\n",
    "# =========\n",
    "nn.compile('01', nnc.classification_sgd_mom, lr=0.1)\n",
    "nn.compile('02', nnc.classification_sgd_mom, lr=0.1)\n",
    "nn.compile('01', nnc.classification_validation)\n",
    "nn.compile('02', nnc.classification_validation)\n",
    "nn.compile('01', nnc.output)\n",
    "nn.compile('02', nnc.output)\n",
    "\n",
    "# Train the nn :\n",
    "# ==============\n",
    "nn.train([dataset_1, dataset_2], ['01', '02'], num_epochs=100);\n",
    "\n",
    "# ================\n",
    "# Learning curve\n",
    "# ================\n",
    "fig, ax = visual.learning_curve(nn.global_stats, regex='loss')\n",
    "#     SAVE\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(fig_title+'-Learning_curve.png',bbox_inches='tight')\n",
    "fig.show()\n",
    "fig, ax = visual.learning_curve(nn.global_stats, regex='acc')\n",
    "#     SAVE\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(fig_title+'-Learning_curve.png',bbox_inches='tight')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in range(len(np.unique(dataset_1.y_test))):\n",
    "    visual.bound(dataset_1.X_test, dataset_1.y_test, nn.parts['01'].output, class_idx=c);\n",
    "plt.show()\n",
    "for c in range(len(np.unique(dataset_2.y_test))):\n",
    "    visual.bound(dataset_2.X_test, dataset_2.y_test, nn.parts['02'].output, class_idx=c);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One loop EMANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EM_ITER = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_classes_1 = 4\n",
    "n_classes_2 = 4\n",
    "n_samples = 1000\n",
    "X_src, y_src = make_clouds(n_samples=n_samples, n_classes=n_classes_1)\n",
    "\n",
    "X_tgt, y_tgt = X_src, y_src\n",
    "# X_tgt, y_tgt = make_circles(n_samples=n_samples,  n_classes=n_classes_2)\n",
    "\n",
    "data_name='Clouds -> Same'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k_src = 10\n",
    "k_tgt = 10\n",
    "# We do not need to have the same number of cluster in the source and target data.\n",
    "k_means_src = KMeans(n_clusters=k_src).fit(X_src)\n",
    "k_means_tgt = KMeans(n_clusters=k_tgt).fit(X_tgt)\n",
    "# Mass and cost matrix\n",
    "w_src = mass(k_means_src)\n",
    "w_tgt = mass(k_means_tgt)\n",
    "cost_mat = np.random.uniform(0,1, size=(w_src.shape[0], w_tgt.shape[0]))\n",
    "visual.mat(cost_mat)\n",
    "plt.title(\"Cost matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Optimal Transport**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transp = computeTransportSinkhorn(w_src, w_tgt, cost_mat, reg=10)\n",
    "visual.mat(transp)\n",
    "plt.title(\"Transport matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Dual Proba dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y = train_dataset(X_src, X_tgt, k_means_src, k_means_tgt, transp)\n",
    "data = make_dataset(X, Y, batchsize=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Network Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get general information :\n",
    "# =========================\n",
    "batchsize = data.batchsize\n",
    "_shape = np.shape(data.X_train)\n",
    "n_dim = len(_shape)\n",
    "n_features = np.prod(_shape[1:])\n",
    "\n",
    "shape = (batchsize,) + _shape[1:]\n",
    "target_var = T.ivector('targets')\n",
    "\n",
    "# Logs\n",
    "logger.info('Building the input and output variables for : {}'.format(data_name))\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n",
    "\n",
    "# Build the layers :\n",
    "# ==================\n",
    "def build():\n",
    "    input_layer = lasagne.layers.InputLayer(shape=shape)\n",
    "\n",
    "    dense_1 = lasagne.layers.DenseLayer(input_layer, 3, nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "    dense_2 = lasagne.layers.DenseLayer(dense_1, 3, nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "\n",
    "    # With concat :\n",
    "    cluster_src = lasagne.layers.DenseLayer(dense_2, k_src, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    cluster_tgt = lasagne.layers.DenseLayer(dense_2, k_tgt, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    concat_layer = lasagne.layers.ConcatLayer([cluster_src, cluster_tgt], axis=1)\n",
    "\n",
    "    end_layer = concat_layer\n",
    "    return end_layer, dense_2\n",
    "end_layer, repr_layer = build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile the NN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instanciate the NN :\n",
    "# ====================\n",
    "# nn = NN(end_layer, name='EMANN test')\n",
    "nn = CNN(name='EMANN test')\n",
    "nn.add_output('proba', end_layer)\n",
    "nn.add_output('repr', repr_layer)\n",
    "\n",
    "# Compile :\n",
    "# =========\n",
    "# nn.compile(nnc.crossentropy_sgd_mom, lr=0.1, mom=0.9)\n",
    "# nn.compile(nnc.crossentropy_validation)\n",
    "# nn.compile(nnc.output)\n",
    "\n",
    "nn.compile('proba', nnc.crossentropy_sgd_mom, lr=0.1, mom=0.9)\n",
    "nn.compile('proba', nnc.crossentropy_validation)\n",
    "nn.compile('proba', nnc.output)\n",
    "nn.compile('repr', nnc.output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the NN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the nn :\n",
    "# ==============\n",
    "# nn.train(data, num_epochs=100);\n",
    "nn.train([data], ['proba'], num_epochs=100);\n",
    "# ================\n",
    "# Learning curve\n",
    "# ================\n",
    "fig, ax = visual.learning_curve(nn.global_stats, regex='loss')\n",
    "#     SAVE\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(fig_title+'-Learning_curve.png',bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = nn.parts['proba'].output(data.X_test)[0]\n",
    "i = np.random.randint(0, data.X_test.shape[0])\n",
    "# print('\\n'.join('{:1.5f}--{:1.5f}'.format(pred, truth) for pred, truth in zip(y_pred[i], data.y_test[i])))\n",
    "width=0.4\n",
    "plt.bar(np.arange(k_src+k_tgt), y_pred[i], width, color='r')\n",
    "plt.bar(np.arange(k_src+k_tgt)+width, data.y_test[i], width, color='b')\n",
    "plt.title(\"One point distrib\")\n",
    "plt.legend(bbox_to_anchor=(1.2,1.))\n",
    "# plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **EM Starts Here !**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "# X_src_repr = nn.parts['repr'].output(X_src)\n",
    "# X_tgt_repr = nn.parts['repr'].output(X_tgt)\n",
    "centers_src = nn.parts['repr'].output(k_means_src.cluster_centers_)[0]\n",
    "centers_tgt = nn.parts['repr'].output(k_means_tgt.cluster_centers_)[0]\n",
    "cost_mat = pairwise_distances(centers_src, centers_tgt)\n",
    "cost_mat = cost_mat**2\n",
    "visual.mat(cost_mat)\n",
    "plt.title(\"Cost matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Optimal Transport**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transp = computeTransportSinkhorn(w_src, w_tgt, cost_mat, reg=5)\n",
    "visual.mat(transp)\n",
    "plt.title(\"Transport matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Dual Proba dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y = train_dataset(X_src, X_tgt, k_means_src, k_means_tgt, transp)\n",
    "data = make_dataset(X, Y, batchsize=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural network** (re-initialization)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Build the layers :\n",
    "# ==================\n",
    "end_layer, repr_layer = build()\n",
    "\n",
    "# Instanciate the NN :\n",
    "# ====================\n",
    "# nn = NN(end_layer, name='EMANN test')\n",
    "nn = CNN(name='EMANN test')\n",
    "nn.add_output('proba', end_layer)\n",
    "nn.add_output('repr', repr_layer)\n",
    "\n",
    "# Compile :\n",
    "# =========\n",
    "# nn.compile(nnc.crossentropy_sgd_mom, lr=0.1, mom=0.9)\n",
    "# nn.compile(nnc.crossentropy_validation)\n",
    "# nn.compile(nnc.output)\n",
    "\n",
    "nn.compile('proba', nnc.crossentropy_sgd_mom, lr=0.01, mom=0.9)\n",
    "nn.compile('proba', nnc.crossentropy_validation)\n",
    "nn.compile('proba', nnc.output)\n",
    "nn.compile('repr', nnc.output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the NN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the nn :\n",
    "# ==============\n",
    "# nn.train(data, num_epochs=100);\n",
    "nn.train([data], ['proba'], num_epochs=100);\n",
    "# ================\n",
    "# Learning curve\n",
    "# ================\n",
    "fig, ax = visual.learning_curve(nn.global_stats, regex='loss')\n",
    "#     SAVE\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(fig_title+'-Learning_curve.png',bbox_inches='tight')\n",
    "fig.show()\n",
    "\n",
    "EM_ITER += 1\n",
    "print('Iteration n*', EM_ITER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = nn.parts['proba'].output(data.X_test)[0]\n",
    "i = np.random.randint(0, data.X_test.shape[0])\n",
    "# print('\\n'.join('{:1.5f}--{:1.5f}'.format(pred, truth) for pred, truth in zip(y_pred[i], data.y_test[i])))\n",
    "width = 0.4\n",
    "plt.bar(np.arange(k_src+k_tgt), y_pred[i], width, color='r', label='pred')\n",
    "plt.bar(np.arange(k_src+k_tgt)+width, data.y_test[i], width, color='b', label='truth')\n",
    "# plt.yscale('log')\n",
    "plt.title(\"One point distrib\")\n",
    "plt.legend(bbox_to_anchor=(1.2,1.))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**[EM LOOP]**](#EM-Starts-Here-!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReGenerate Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
