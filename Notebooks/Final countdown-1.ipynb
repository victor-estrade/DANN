{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Ici je vais écrire la version finale de la méthode supervisée pour le *cas 1 : Appariement connu*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Loading of datasets](#Load-datasets)\n",
    "2. [Transformation of datasets](#Transform-datasets)\n",
    "3. [Helper functions](#Helper-functions)\n",
    "4. [Linear](#Linear)\n",
    "    1. [Generate-data](#Generate-data)\n",
    "    2. [Prepare datasets](#Prepare-datasets)\n",
    "    5. [Neural Network Architecture](#Neural-Network-Architecture)\n",
    "    6. [Compile the NN](#Compile-the-NN)\n",
    "    7. [Train the NN](#Train-the-NN)\n",
    "5. [NN reverse](#NN-reverse)\n",
    "    1. [Source data](#Source-data)\n",
    "    2. [Target data](#Target-data)\n",
    "    5. [Aligmnent](#Aligmnent)\n",
    "    6. [NN archi](#NN-archi)\n",
    "    7. [Training](#Training)\n",
    "    8. [Visualization](#Visualization)\n",
    "6. [Remaining work](#Remaining-work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import sys\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "import time\n",
    "import visual\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, pairwise_distances\n",
    "\n",
    "from nn.helper import CNN, NN\n",
    "from nn import block as nnb\n",
    "from nn import compilers as nnc\n",
    "from nn import rgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets\n",
    "\n",
    "- the datasets are loaded/built.\n",
    "- The batchsize is defined\n",
    "- half of the data name (the source part) is defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets.toys import make_clouds, make_circles, make_X, make_moons\n",
    "from datasets.utils import make_dataset, make_domain_dataset\n",
    "from datasets.utils import shuffle_array\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def make_clouds(n_samples=50, n_classes=2):\n",
    "    \"\"\"\n",
    "    Dataset made from normal distributions localised at root of unity solution.\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "        n_samples: (default=50) the number of sample in each class.\n",
    "            Example : 50 samples and 3 classes means 150 points\n",
    "            Can also be a list of integers:\n",
    "                class_i gets n_samples[i] elements (n_samples used as circular list)\n",
    "        n_classes: (default=2) the number of classes\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "        X: The data (numpy array shape [n_samples, 2])\n",
    "        y: The labels (numpy array shape [n_samples])\n",
    "    \"\"\"\n",
    "    from itertools import cycle\n",
    "\n",
    "    # pos is the 2D positions as complex exponential numbers, root of unity solutions\n",
    "    pos = [np.exp(2j*np.pi*i/n_classes) for i in range(n_classes)]\n",
    "    try:\n",
    "        X = np.empty((sum([n for _, n in zip(range(n_classes), cycle(n_samples))]), 2))\n",
    "        print(np.hstack([np.random.normal(np.imag(p), 1/n_classes, size=n) \n",
    "                             for p, n in zip(pos, cycle(n_samples))]))\n",
    "        X[:, 0] = np.hstack([np.random.normal(np.imag(p), 1/n_classes, size=n) \n",
    "                             for p, n in zip(pos, cycle(n_samples))])\n",
    "        X[:, 1] = np.hstack([np.random.normal(np.real(p), 1/n_classes, size=n) \n",
    "                             for p, n in zip(pos, cycle(n_samples))])\n",
    "        y = np.hstack([np.ones(n)*i for i, n in zip(range(n_classes), cycle(n_samples))])\n",
    "    except TypeError as e:\n",
    "        X = np.empty((n_samples*n_classes, 2))\n",
    "        X[:, 0] = np.hstack([np.random.normal(np.imag(p), 1/n_classes, size=n_samples) for p in pos])\n",
    "        X[:, 1] = np.hstack([np.random.normal(np.real(p), 1/n_classes, size=n_samples) for p in pos])\n",
    "        y = np.hstack([np.ones(n_samples)*i for i in range(n_classes)])\n",
    "\n",
    "#     X, y = shuffle_array(X, y)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "make_clouds([5,2,6], n_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform datasets\n",
    "\n",
    "- the transformed datasets are built.\n",
    "- last part of the data name (the target part) is defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.utils import make_domain_dataset, make_corrector_dataset\n",
    "import datasets.transform as transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import loggers\n",
    "from logs import new_logger, empty_logger\n",
    "logger = new_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from align_learn.probability import mass, align_tgt, align_src\n",
    "from align_learn.preprocess import _classwise_shuffle\n",
    "\n",
    "def classwise_shuffle(data):\n",
    "    data['X_train'] = _classwise_shuffle(data['X_train'], data['labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCHSIZE = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data\n",
    "\n",
    "Première étape : générer les données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# n_classes_1 = 4\n",
    "# n_classes_2 = 4\n",
    "n_samples = 1000\n",
    "X_src, y_src = make_moons(n_samples=n_samples)\n",
    "\n",
    "X_tgt, y_tgt = transform.rotate(X_src, y_src, angle=15.)\n",
    "# X_tgt, y_tgt = make_circles(n_samples=n_samples,  n_classes=n_classes_2)\n",
    "\n",
    "data_name='Clouds -> rotated'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labels\n",
    "# l_src, l_tgt = k_means_src.labels_, k_means_tgt.labels_\n",
    "l_src, l_tgt = np.asarray(y_src, dtype=int), np.asarray(y_tgt, dtype=int),\n",
    "\n",
    "# Mass\n",
    "w_src = mass(l_src)\n",
    "w_tgt = mass(l_tgt)\n",
    "\n",
    "# Params\n",
    "n_class_tgt = len(np.unique(l_tgt))\n",
    "n_class_src = len(np.unique(l_src))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prepare datasets\n",
    "\n",
    "Build the training datasets.\n",
    "\n",
    "The data from the source and the target distribution ordered so $x_s$ should correspond to $x_t$.\n",
    "\n",
    "The target is the probability that $x$ belong to the label $y$ in the source space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Align the data : No alignment requiered\n",
    "X_S, y_S = X_src, l_src\n",
    "X_T, y_T = X_tgt, l_tgt\n",
    "# Get the probability to be predicted for each couple of data point.\n",
    "\n",
    "# Shuffle it all to prevent the index to be correlated to the labels\n",
    "X_S, y_S, X_T, y_T = shuffle_array(X_S, y_S, X_T, y_T)\n",
    "# Build split dataset (train, valid, test)\n",
    "src_data = make_dataset(X_S, y_S, batchsize=100)\n",
    "tgt_data = make_dataset(X_T, y_T, batchsize=100)\n",
    "corr_data = make_corrector_dataset(src_data, tgt_data)\n",
    "adversarial_data = make_domain_dataset([src_data, tgt_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "\n",
    "the neural network works as a supervised multiple regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get general information :\n",
    "# =========================\n",
    "batchsize = src_data.batchsize\n",
    "_shape = np.shape(src_data.X_train)\n",
    "n_dim = len(_shape)\n",
    "n_features = np.prod(_shape[1:])\n",
    "\n",
    "shape = (batchsize,) + _shape[1:]\n",
    "\n",
    "# Logs\n",
    "logger.info('Building the input and output variables for : {}'.format(data_name))\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n",
    "\n",
    "# Build the layers :\n",
    "# ==================\n",
    "# Inputs layers\n",
    "# -------------\n",
    "input_layer_src = lasagne.layers.InputLayer(shape=shape)\n",
    "input_layer_tgt = lasagne.layers.InputLayer(shape=shape)\n",
    "\n",
    "# Representaion layers for the source data\n",
    "# ----------------------------------------\n",
    "dense_1 = lasagne.layers.DenseLayer(input_layer_src, shape[1], nonlinearity=lasagne.nonlinearities.rectify)\n",
    "dense_2 = lasagne.layers.DenseLayer(dense_1, shape[1], nonlinearity=lasagne.nonlinearities.rectify)\n",
    "dense_3 = lasagne.layers.DenseLayer(dense_2, shape[1], nonlinearity=lasagne.nonlinearities.rectify)\n",
    "dense_4 = lasagne.layers.DenseLayer(dense_3, shape[1], nonlinearity=None)\n",
    "\n",
    "dense_0 = lasagne.layers.DenseLayer(input_layer_src, shape[1], nonlinearity=None)\n",
    "\n",
    "end_layer = dense_0\n",
    "\n",
    "\n",
    "# Adversarial structure\n",
    "# ---------------------\n",
    "adv_inputs = [end_layer, input_layer_tgt]\n",
    "concat = lasagne.layers.ConcatLayer(adv_inputs, axis=0)\n",
    "rg_layer = rgl.ReverseGradientLayer(concat, hp_lambda=1)\n",
    "adv_dense_1 = lasagne.layers.DenseLayer(rg_layer, 3, \n",
    "                                       nonlinearity=lasagne.nonlinearities.tanh)\n",
    "adv_dense_2 = lasagne.layers.DenseLayer(adv_dense_1, 3,\n",
    "                                       nonlinearity=lasagne.nonlinearities.tanh)\n",
    "adv_output = lasagne.layers.DenseLayer(adv_dense_2, len(adv_inputs), \n",
    "                                       nonlinearity=lasagne.nonlinearities.softmax,)\n",
    "\n",
    "# Adversarial bis for learning only\n",
    "adv_inputs2 = [end_layer, input_layer_tgt]\n",
    "concat2 = lasagne.layers.ConcatLayer(adv_inputs2, axis=0)\n",
    "rg_layer2 = rgl.ReverseGradientLayer(concat2, hp_lambda=0)\n",
    "adv_dense_12 = lasagne.layers.DenseLayer(rg_layer2, 3, W=adv_dense_1.W, b=adv_dense_1.b,\n",
    "                                        nonlinearity=lasagne.nonlinearities.tanh)\n",
    "adv_dense_22 = lasagne.layers.DenseLayer(adv_dense_12, 3, W=adv_dense_2.W, b=adv_dense_2.b,\n",
    "                                        nonlinearity=lasagne.nonlinearities.tanh)\n",
    "adv_output2 = lasagne.layers.DenseLayer(adv_dense_22, len(adv_inputs2), W=adv_output.W, b=adv_output.b,\n",
    "                                       nonlinearity=lasagne.nonlinearities.softmax,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the NN\n",
    "\n",
    "Compile the functions:\n",
    "- training, validation, proba output for the source path\n",
    "- training, validation, proba output for the target path\n",
    "- raw output for the representation\n",
    "- training, validation, proba output for the adverssarial path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instanciate the NN :\n",
    "# ====================\n",
    "nn = CNN(name='Linear NN')\n",
    "nn.add_output('correction', end_layer)\n",
    "# Ok for the adversarial the code is not intuitive. [Further work]\n",
    "nn.add_output('adversarial', adv_inputs)\n",
    "nn.add_output('adversarial2', adv_inputs2)\n",
    "\n",
    "# Compile :\n",
    "# =========\n",
    "nn.compile('correction', nnc.squared_error_sgd_mom, lr=0.01, mom=0.)\n",
    "nn.compile('correction', nnc.squared_error_validation)\n",
    "nn.compile('correction', nnc.output)\n",
    "nn.compile('adversarial', nnc.adversarial, output_layer=adv_output, lr=0.01, mom=0.)\n",
    "nn.compile('adversarial2', nnc.adversarial, output_layer=adv_output2, lr=0.1, mom=0.8)\n",
    "\n",
    "logger.info(\"Compilation Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the NN\n",
    "\n",
    "Now is the training session.\n",
    "\n",
    "It altarnatively (mini-batch after mini-batch) train (forwward-backward propagation) each part of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the nn :\n",
    "# ==============\n",
    "# nn.train(data, num_epochs=100);\n",
    "\n",
    "for _ in range(15):\n",
    "    classwise_shuffle(corr_data)\n",
    "#     nn.train([corr_data], ['correction'], num_epochs=1);\n",
    "    nn.train([corr_data, adversarial_data], ['correction', 'adversarial'], num_epochs=2);\n",
    "    nn.train([adversarial_data], ['adversarial2'], num_epochs=5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ================\n",
    "# Learning curve\n",
    "# ================\n",
    "# Usefull regex : 'proba.* loss', 'loss', 'acc'\n",
    "fig, ax = visual.learning_curve(nn.global_stats, regex='corr.* loss')\n",
    "#     SAVE\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(fig_title+'-Learning_curve.png',bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ================\n",
    "# Learning curve\n",
    "# ================\n",
    "# Usefull regex : 'proba.* loss', 'loss', 'acc'\n",
    "fig, ax = visual.learning_curve(nn.global_stats, regex='adv.* loss')\n",
    "#     SAVE\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(fig_title+'-Learning_curve.png',bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ================\n",
    "# Learning curve\n",
    "# ================\n",
    "# Usefull regex : 'proba.* loss', 'loss', 'acc'\n",
    "fig, ax = visual.learning_curve(nn.global_stats, regex='training.* acc')\n",
    "#     SAVE\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(fig_title+'-Learning_curve.png',bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "dense_1.W.get_value()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "theta = (23.5/180.) * np.pi\n",
    "rot_matrix = np.array([[np.cos(theta), -np.sin(theta)], \n",
    "                         [np.sin(theta),  np.cos(theta)]])\n",
    "print(rot_matrix.dot(dense_1.W.get_value()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check some results\n",
    "\n",
    "Check the output of the NN:\n",
    "\n",
    "The predicted probability of being in a partition **vs** the true value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = visual.source_2D(src_data.X_test, src_data.y_test)\n",
    "visual.target_2D(tgt_data.X_test, tgt_data.y_test, ax=ax);\n",
    "visual.add_legend(ax, title=\"Source vs target data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_pred = nn.parts['correction'].output(corr_data.X_test)[0]\n",
    "fig, ax = visual.target_2D(tgt_data.X_test, tgt_data.y_test)\n",
    "visual.corrected_2D(X_pred, src_data.y_test, ax=ax);\n",
    "visual.add_legend(ax, title=\"Corrected vs target Data\")\n",
    "# plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_s, X_t = adversarial_data.X_test\n",
    "y = np.hstack([np.zeros(X_s.shape[0]), np.ones(X_t.shape[0])])\n",
    "X_pred = nn.parts['correction'].output(X_s)[0]\n",
    "X = np.concatenate([X_pred, X_t])\n",
    "\n",
    "from matplotlib import cm\n",
    "CMAP = 'Paired'\n",
    "cm_heat = plt.cm.RdBu\n",
    "color = cm.ScalarMappable(cmap=CMAP)\n",
    "fig, ax = plt.subplots()\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, .02),\n",
    "                     np.arange(y_min, y_max, .02))\n",
    "c = np.c_[xx.ravel(), yy.ravel()]\n",
    "Z = nn.parts['adversarial'].output(c[0:5], c[5:])\n",
    "Z = np.array(Z)[0, :, 0]\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "ax.contourf(xx, yy, Z, cmap=cm_heat, alpha=.8)\n",
    "\n",
    "# Plot also the training points\n",
    "ax.scatter(X[:, 0], X[:, 1], c=color.to_rgba(y));\n",
    "ax.set_xlim(xx.min(), xx.max());\n",
    "ax.set_ylim(yy.min(), yy.max());\n",
    "\n",
    "# X_pred = nn.parts['correction'].output(corr_data.X_test)[0]\n",
    "# visual.corrected_2D(X_pred, src_data.y_test, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Z.shape, np.prod(xx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# n_classes_1 = 4\n",
    "# n_classes_2 = 4\n",
    "n_samples = 400\n",
    "X_src, y_src = make_moons(n_samples=n_samples)\n",
    "\n",
    "data_name='Moons -> rotated'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Get general information :\n",
    "# =========================\n",
    "batchsize = BATCHSIZE\n",
    "_shape = np.shape(X_src)\n",
    "n_dim = len(_shape)\n",
    "n_features = np.prod(_shape[1:])\n",
    "\n",
    "shape = (batchsize,) + _shape[1:]\n",
    "\n",
    "# Logs\n",
    "logger.info('Building the input and output variables for : {}'.format(data_name))\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n",
    "\n",
    "# Build the layers :\n",
    "# ==================\n",
    "# Inputs layers\n",
    "# -------------\n",
    "input_layer = lasagne.layers.InputLayer(shape=shape)\n",
    "\n",
    "# Representaion layers for the source data\n",
    "# ----------------------------------------\n",
    "dense_1T = lasagne.layers.DenseLayer(input_layer, 2, nonlinearity=lasagne.nonlinearities.tanh)\n",
    "dense_2T = lasagne.layers.DenseLayer(dense_1T, 5, nonlinearity=lasagne.nonlinearities.tanh)\n",
    "dense_3T = lasagne.layers.DenseLayer(dense_2T, 3, nonlinearity=lasagne.nonlinearities.tanh)\n",
    "dense_4T = lasagne.layers.DenseLayer(dense_3T, shape[1], nonlinearity=None)\n",
    "\n",
    "transformer = CNN('transformer')\n",
    "transformer.add_output('transformer', dense_4T)\n",
    "\n",
    "# Compile :\n",
    "# ---------\n",
    "transformer.compile('transformer', nnc.output)\n",
    "logger.info(\"Compilation Done\")\n",
    "\n",
    "X_tgt = transformer.parts['transformer'].output(X_src)[0]\n",
    "y_tgt = np.copy(y_src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# labels\n",
    "# l_src, l_tgt = k_means_src.labels_, k_means_tgt.labels_\n",
    "l_src, l_tgt = np.asarray(y_src, dtype=int), np.asarray(y_tgt, dtype=int),\n",
    "\n",
    "# Mass\n",
    "w_src = mass(l_src)\n",
    "w_tgt = mass(l_tgt)\n",
    "\n",
    "# Params\n",
    "n_class_tgt = len(np.unique(l_tgt))\n",
    "n_class_src = len(np.unique(l_src))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Align the data : No alignment requiered\n",
    "X_S, y_S = X_src, l_src\n",
    "X_T, y_T = X_tgt, l_tgt\n",
    "# Get the probability to be predicted for each couple of data point.\n",
    "\n",
    "# Shuffle it all to prevent the index to be correlated to the labels\n",
    "X_S, y_S, X_T, y_T = shuffle_array(X_S, y_S, X_T, y_T)\n",
    "# Build split dataset (train, valid, test)\n",
    "src_data = make_dataset(X_S, y_S, batchsize=BATCHSIZE)\n",
    "tgt_data = make_dataset(X_T, y_T, batchsize=BATCHSIZE)\n",
    "corr_data = make_corrector_dataset(src_data, tgt_data)\n",
    "# adversarial_data = make_domain_dataset([src_data, tgt_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN archi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Get general information :\n",
    "# =========================\n",
    "batchsize = src_data.batchsize\n",
    "_shape = np.shape(src_data.X_train)\n",
    "n_dim = len(_shape)\n",
    "n_features = np.prod(_shape[1:])\n",
    "\n",
    "shape = (batchsize,) + _shape[1:]\n",
    "\n",
    "# Logs\n",
    "logger.info('Building the input and output variables for : {}'.format(data_name))\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n",
    "\n",
    "# Build the layers :\n",
    "# ==================\n",
    "# Inputs layers\n",
    "# -------------\n",
    "input_layer = lasagne.layers.InputLayer(shape=shape)\n",
    "\n",
    "# Representaion layers for the source data\n",
    "# ----------------------------------------\n",
    "dense_1 = lasagne.layers.DenseLayer(input_layer, 5, nonlinearity=lasagne.nonlinearities.tanh)\n",
    "dense_2 = lasagne.layers.DenseLayer(dense_1, 3, nonlinearity=lasagne.nonlinearities.tanh)\n",
    "dense_3 = lasagne.layers.DenseLayer(dense_2, shape[1], nonlinearity=None)\n",
    "# dense_4 = lasagne.layers.DenseLayer(dense_3, shape[1], nonlinearity=None)\n",
    "\n",
    "# dense_0 = lasagne.layers.DenseLayer(input_layer, shape[1], nonlinearity=None)\n",
    "\n",
    "end_layer = dense_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Instanciate the NN :\n",
    "# ====================\n",
    "nn2 = CNN(name='Linear NN')\n",
    "nn2.add_output('correction', end_layer)\n",
    "\n",
    "# Compile :\n",
    "# =========\n",
    "nn2.compile('correction', nnc.squared_error_sgd_mom, lr=0.1, mom=0.9)\n",
    "nn2.compile('correction', nnc.squared_error_validation)\n",
    "nn2.compile('correction', nnc.output)\n",
    "\n",
    "logger.info(\"Compilation Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Train the nn :\n",
    "# ==============\n",
    "# nn.train(data, num_epochs=100);\n",
    "nn2.train([corr_data], ['correction'], num_epochs=100);"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ================\n",
    "# Learning curve\n",
    "# ================\n",
    "# Usefull regex : 'proba.* loss', 'loss', 'acc'\n",
    "fig, ax = visual.learning_curve(nn2.global_stats, regex='loss')\n",
    "#     SAVE\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(fig_title+'-Learning_curve.png',bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "fig, ax = visual.source_2D(src_data.X_test, src_data.y_test)\n",
    "visual.target_2D(tgt_data.X_test, tgt_data.y_test, ax=ax);\n",
    "visual.add_legend(ax, title=\"Source vs target data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "X_pred = nn2.parts['correction'].output(corr_data.X_test)[0]\n",
    "fig, ax = visual.target_2D(tgt_data.X_test, tgt_data.y_test)\n",
    "visual.corrected_2D(X_pred, src_data.y_test, ax=ax);\n",
    "visual.add_legend(ax, title=\"Corrected vs target Data\")\n",
    "# plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Remaining work\n",
    "\n",
    "- **Tout refaire**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
