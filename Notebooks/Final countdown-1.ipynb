{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Ici je vais écrire la version finale de la méthode supervisée pour le *cas 1 : Appariement connu*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Loading of datasets](#Load-datasets)\n",
    "2. [Transformation of datasets](#Transform-datasets)\n",
    "3. [Helper functions](#Helper-functions)\n",
    "4. [Settings](#Settings)\n",
    "    1. [Generate-data](#Generate-data)\n",
    "    2. [Prepare datasets](#Prepare-datasets)\n",
    "    5. [Neural Network Architecture](#Neural-Network-Architecture)\n",
    "    6. [Compile the NN](#Compile-the-NN)\n",
    "    7. [Train the NN](#Train-the-NN)\n",
    "5. [Test the result](#Test-the-result)\n",
    "6. [Remaining work](#Remaining-work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import sys\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "import time\n",
    "import visual\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, pairwise_distances\n",
    "\n",
    "from nn.helper import CNN, NN\n",
    "from nn import block as nnb\n",
    "from nn import compilers as nnc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets\n",
    "\n",
    "- the datasets are loaded/built.\n",
    "- The batchsize is defined\n",
    "- half of the data name (the source part) is defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.toys import make_clouds, make_circles, make_X, make_moons\n",
    "from datasets.utils import make_dataset, make_domain_dataset\n",
    "from datasets.utils import shuffle_array\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def make_clouds(n_samples=50, n_classes=2):\n",
    "    \"\"\"\n",
    "    Dataset made from normal distributions localised at root of unity solution.\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "        n_samples: (default=50) the number of sample in each class.\n",
    "            Example : 50 samples and 3 classes means 150 points\n",
    "            Can also be a list of integers:\n",
    "                class_i gets n_samples[i] elements (n_samples used as circular list)\n",
    "        n_classes: (default=2) the number of classes\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "        X: The data (numpy array shape [n_samples, 2])\n",
    "        y: The labels (numpy array shape [n_samples])\n",
    "    \"\"\"\n",
    "    from itertools import cycle\n",
    "\n",
    "    # pos is the 2D positions as complex exponential numbers, root of unity solutions\n",
    "    pos = [np.exp(2j*np.pi*i/n_classes) for i in range(n_classes)]\n",
    "    try:\n",
    "        X = np.empty((sum([n for _, n in zip(range(n_classes), cycle(n_samples))]), 2))\n",
    "        print(np.hstack([np.random.normal(np.imag(p), 1/n_classes, size=n) \n",
    "                             for p, n in zip(pos, cycle(n_samples))]))\n",
    "        X[:, 0] = np.hstack([np.random.normal(np.imag(p), 1/n_classes, size=n) \n",
    "                             for p, n in zip(pos, cycle(n_samples))])\n",
    "        X[:, 1] = np.hstack([np.random.normal(np.real(p), 1/n_classes, size=n) \n",
    "                             for p, n in zip(pos, cycle(n_samples))])\n",
    "        y = np.hstack([np.ones(n)*i for i, n in zip(range(n_classes), cycle(n_samples))])\n",
    "    except TypeError as e:\n",
    "        X = np.empty((n_samples*n_classes, 2))\n",
    "        X[:, 0] = np.hstack([np.random.normal(np.imag(p), 1/n_classes, size=n_samples) for p in pos])\n",
    "        X[:, 1] = np.hstack([np.random.normal(np.real(p), 1/n_classes, size=n_samples) for p in pos])\n",
    "        y = np.hstack([np.ones(n_samples)*i for i in range(n_classes)])\n",
    "\n",
    "#     X, y = shuffle_array(X, y)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "make_clouds([5,2,6], n_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform datasets\n",
    "\n",
    "- the transformed datasets are built.\n",
    "- last part of the data name (the target part) is defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets.utils import make_domain_dataset, make_corrector_dataset\n",
    "import datasets.transform as transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import loggers\n",
    "from logs import new_logger, empty_logger\n",
    "logger = new_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from align_learn.probability import mass, align"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data\n",
    "\n",
    "Première étape : générer les données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_classes_1 = 4\n",
    "n_classes_2 = 4\n",
    "n_samples = 1000\n",
    "X_src, y_src = make_clouds(n_samples=n_samples, n_classes=n_classes_1)\n",
    "\n",
    "X_tgt, y_tgt = transform.rotate(X_src, y_src, angle=80.)\n",
    "# X_tgt, y_tgt = make_circles(n_samples=n_samples,  n_classes=n_classes_2)\n",
    "\n",
    "data_name='Clouds -> rotated'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# labels\n",
    "# l_src, l_tgt = k_means_src.labels_, k_means_tgt.labels_\n",
    "l_src, l_tgt = np.asarray(y_src, dtype=int), np.asarray(y_tgt, dtype=int),\n",
    "\n",
    "# Mass\n",
    "w_src = mass(l_src)\n",
    "w_tgt = mass(l_tgt)\n",
    "\n",
    "# Params\n",
    "n_class_tgt = len(np.unique(l_tgt))\n",
    "n_class_src = len(np.unique(l_src))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prepare datasets\n",
    "\n",
    "Build the training datasets.\n",
    "\n",
    "The data from the source and the target distribution ordered so $x_s$ should correspond to $x_t$.\n",
    "\n",
    "The target is the probability that $x$ belong to the label $y$ in the source space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Align the data : No alignment requiered\n",
    "X_S, y_S = X_src, l_src\n",
    "X_T, y_T = X_tgt, l_tgt\n",
    "# Get the probability to be predicted for each couple of data point.\n",
    "\n",
    "# Shuffle it all to prevent the index to be correlated to the labels\n",
    "X_S, y_S, X_T, y_T = shuffle_array(X_S, y_S, X_T, y_T)\n",
    "# Build split dataset (train, valid, test)\n",
    "src_data = make_dataset(X_S, y_S, batchsize=100)\n",
    "tgt_data = make_dataset(X_T, y_T, batchsize=100)\n",
    "corr_data = make_corrector_dataset(src_data, tgt_data)\n",
    "# adversarial_data = make_domain_dataset([src_data, tgt_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "\n",
    "the neural network works as a supervised multiple regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get general information :\n",
    "# =========================\n",
    "batchsize = src_data.batchsize\n",
    "_shape = np.shape(src_data.X_train)\n",
    "n_dim = len(_shape)\n",
    "n_features = np.prod(_shape[1:])\n",
    "\n",
    "shape = (batchsize,) + _shape[1:]\n",
    "\n",
    "# Logs\n",
    "logger.info('Building the input and output variables for : {}'.format(data_name))\n",
    "logger.info('Input data expected shape : {}'.format(shape))\n",
    "\n",
    "# Build the layers :\n",
    "# ==================\n",
    "# Inputs layers\n",
    "# -------------\n",
    "input_layer = lasagne.layers.InputLayer(shape=shape)\n",
    "\n",
    "# Representaion layers for the source data\n",
    "# ----------------------------------------\n",
    "dense_1 = lasagne.layers.DenseLayer(input_layer, shape[1], nonlinearity=lasagne.nonlinearities.rectify)\n",
    "dense_2 = lasagne.layers.DenseLayer(dense_1, shape[1], nonlinearity=lasagne.nonlinearities.rectify)\n",
    "dense_3 = lasagne.layers.DenseLayer(dense_2, shape[1], nonlinearity=lasagne.nonlinearities.rectify)\n",
    "dense_4 = lasagne.layers.DenseLayer(dense_3, shape[1], nonlinearity=None)\n",
    "\n",
    "dense_0 = lasagne.layers.DenseLayer(input_layer, shape[1], nonlinearity=None)\n",
    "\n",
    "end_layer = dense_0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the NN\n",
    "\n",
    "Compile the functions:\n",
    "- training, validation, proba output for the source path\n",
    "- training, validation, proba output for the target path\n",
    "- raw output for the representation\n",
    "- training, validation, proba output for the adverssarial path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instanciate the NN :\n",
    "# ====================\n",
    "nn = CNN(name='Linear NN')\n",
    "nn.add_output('correction', end_layer)\n",
    "\n",
    "# Compile :\n",
    "# =========\n",
    "nn.compile('correction', nnc.squared_error_sgd_mom, lr=0.1, mom=0.9)\n",
    "nn.compile('correction', nnc.squared_error_validation)\n",
    "nn.compile('correction', nnc.output)\n",
    "\n",
    "logger.info(\"Compilation Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the NN\n",
    "\n",
    "Now is the training session.\n",
    "\n",
    "It altarnatively (mini-batch after mini-batch) train (forwward-backward propagation) each part of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the nn :\n",
    "# ==============\n",
    "# nn.train(data, num_epochs=100);\n",
    "nn.train([corr_data], ['correction'], num_epochs=5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ================\n",
    "# Learning curve\n",
    "# ================\n",
    "# Usefull regex : 'proba.* loss', 'loss', 'acc'\n",
    "fig, ax = visual.learning_curve(nn.global_stats, regex='loss')\n",
    "#     SAVE\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(fig_title+'-Learning_curve.png',bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dense_1.W.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta = (23.5/180.) * np.pi\n",
    "rot_matrix = np.array([[np.cos(theta), -np.sin(theta)], \n",
    "                         [np.sin(theta),  np.cos(theta)]])\n",
    "print(rot_matrix.dot(dense_1.W.get_value()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check some results\n",
    "\n",
    "Check the output of the NN:\n",
    "\n",
    "The predicted probability of being in a partition **vs** the true value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = visual.source_2D(src_data.X_test, src_data.y_test)\n",
    "visual.target_2D(tgt_data.X_test, tgt_data.y_test, ax=ax);\n",
    "visual.add_legend(ax, title=\"Source vs target data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_pred = nn.parts['correction'].output(corr_data.X_test)[0]\n",
    "fig, ax = visual.target_2D(tgt_data.X_test, tgt_data.y_test)\n",
    "visual.corrected_2D(X_pred, src_data.y_test, ax=ax);\n",
    "visual.add_legend(ax, title=\"Corrected vs target Data\")\n",
    "# plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Remaining work\n",
    "\n",
    "- **Tout refaire**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[**[Back to top]**](#Introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
